{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeTetsuyaR/AbePPO/blob/main/AbePPO1108_without_ohe_MAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXo2dqdAJSHs",
        "outputId": "85c4df25-7945-42d7-be62-14cee010930f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCrwylLHTYPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import cProfile\n",
        "import sys\n",
        "import copy\n",
        "from torch.distributions.categorical import Categorical\n",
        "import math\n",
        "import os\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.colors as mcolors\n",
        "from tqdm import tqdm  # tqdmをインポート\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import gamma, uniform, truncnorm\n",
        "#import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6hr_FE6TYPY"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, n_units=2):\n",
        "        self.n_units = n_units #number of unit\n",
        "        #self.n_states = 5 #number of state\n",
        "        self.inventory = 0\n",
        "        self.demand = 0\n",
        "        self.maintenance_status = [0] * self.n_units\n",
        "        self.interval = 24\n",
        "        self.remain_interval = 24\n",
        "        self.MAX_speed = 10/self.interval\n",
        "        self.MAX_inventory = 0\n",
        "        self.MAX_demand = 15\n",
        "        self.MAX_maintenance_time = 0\n",
        "\n",
        "        self.load_total=1\n",
        "\n",
        "        self.cp = 40#500\n",
        "        self.cc = 80#\n",
        "\n",
        "        self.cps = 0\n",
        "        self.co = 5\n",
        "        self.cs =10#500\n",
        "\n",
        "        self.levels = [0] * self.n_units\n",
        "        #self.flags = [0] * self.n_units\n",
        "        self.shape = 3\n",
        "        self.penalty = 1\n",
        "        self.L = 25#\n",
        "        #self.P_Cost =[[100,110,130,160,2540],\n",
        "                      #[110,120,140,170,2550],\n",
        "                      #[130,140,160,190,2570],\n",
        "                      #[160,170,190,220,2600],\n",
        "                      #[2540,2550,2570,2600,5000]]#convex化\n",
        "\n",
        "        self.P_Cost =[[0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [2500,2500,2500,2500,5000]]#平滑化\n",
        "        self.P_Cost_L = 100\n",
        "\n",
        "        self.Visit =[[0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0]]\n",
        "        self.cntCount=[0,0]\n",
        "\n",
        "        self.failure_keep1 = 0 #1つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep2 = 0 #2つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep3 = 0 #3つ故障しているのに保全を選択しなかった回数\n",
        "        self.replace_chance = 0 #保全を選択できた回数\n",
        "\n",
        "    def init_random(self):\n",
        "        levels = [0,0]\n",
        "        flags = [0,0]\n",
        "        self.load_total=1\n",
        "        return levels, flags, self.load_total\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.levels = np.zeros(self.n_units)\n",
        "\n",
        "    def complete_maintenance(self, unit_idx):\n",
        "        self.levels[unit_idx] = 0\n",
        "\n",
        "    def get_ability(self, level): #良品率\n",
        "        if level == 0:\n",
        "            return 1\n",
        "        elif level == 1:\n",
        "            return 0.8\n",
        "        elif level ==2:\n",
        "            return 0.5\n",
        "        elif level == 3:\n",
        "            return 0.1\n",
        "        return (self.n_states - 1 - level) / (self.n_states - 1)\n",
        "\n",
        "    def update_demand(self, speed, ability, time):\n",
        "        if self.demand >= self.inventory:\n",
        "            self.demand -= self.inventory\n",
        "            self.inventory = 0.0\n",
        "        else:\n",
        "            self.inventory -= self.demand\n",
        "            self.demand = 0.0\n",
        "        return max(0, self.demand-self.inventory-ability*speed*time)\n",
        "\n",
        "    def update_inventory(self, speed, ability, time):\n",
        "        if self.demand <= self.inventory + ability * speed * time:\n",
        "            return min(self.MAX_inventory, -self.demand+self.inventory+ability*speed*time), max(0, -self.MAX_inventory-self.demand+self.inventory+ability*speed*time)\n",
        "        else:\n",
        "            return 0.0, 0.0\n",
        "\n",
        "    def get_maintenance_time(self,level):\n",
        "        return 0\n",
        "\n",
        "    def update_maintenance_time(self, unit_idx):\n",
        "        return 0\n",
        "\n",
        "    def one_hot_encode(self):\n",
        "        level_ohe = []\n",
        "        #mstatus_ohe = []\n",
        "        for unit_idx in range(self.n_units):\n",
        "            l = [0] * self.n_states\n",
        "            #m = [0] * (self.MAX_maintenance_time + 1)\n",
        "            l[min(math.floor(self.levels[unit_idx]),self.n_states-1)] = 1\n",
        "            #m[self.maintenance_status[unit_idx]] = 1\n",
        "            level_ohe = level_ohe + l\n",
        "            #mstatus_ohe = mstatus_ohe + m\n",
        "        return level_ohe #mstatus_oheは削除\n",
        "\n",
        "\n",
        "\n",
        "    def operation(self, replacements, load_rate):\n",
        "        #print(self.levels,\"levels_before\")\n",
        "        reward = 0\n",
        "        #print(load_rate)\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        #保全の意思決定\n",
        "        #print(replacements)\n",
        "\n",
        "        if replacements==[1,1]: #稼働継続\n",
        "\n",
        "          #reward -= self.P_Cost[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] #rewardを最初に計算\n",
        "          if self.levels[0]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          #else:\n",
        "            #reward -= 40*(3**(0.01*(self.levels[0])))\n",
        "          if self.levels[1]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          #else:\n",
        "            #reward -= 40*(3**(0.01*(self.levels[1])))\n",
        "          #print(reward)\n",
        "          # パラメータの設定\n",
        "          scales=[0,0]\n",
        "          shape0 = 0.69  # ガンマ分布のパラメータ v1 用 0.69 100倍にしてみる\n",
        "          shape1 = 0.69   # ガンマ分布のパラメータ v2 用\n",
        "          tau = 0.5  # ケンドールの順位相関係数\n",
        "\n",
        "          theta = 1 / (1 - tau)\n",
        "\n",
        "          #if load_rate<0:##\n",
        "            #load_rate=0\n",
        "          #if load_rate>1:\n",
        "            #load_rate=1\n",
        "          loads=[0,0]\n",
        "          loads[0]=self.load_total*load_rate\n",
        "          loads[1]=self.load_total*(1-load_rate)\n",
        "\n",
        "          #print(speeds, \"speeds\")\n",
        "          #尺度パラメータ計算\n",
        "          for i in range(self.n_units):\n",
        "            scales[i]=6.491*(loads[i]**2)+0.726\n",
        "            #scales[i]=6.491*(speeds[i]**0.5)+0.726\n",
        "            scales[i]=scales[i] #1/10000倍にしてみる\n",
        "\n",
        "\n",
        "          # 一様分布から独立にサンプリング\n",
        "          u = uniform.rvs(size=1)\n",
        "          v = uniform.rvs(size=1)\n",
        "          #print(\"u,v:\",u,v)\n",
        "          # ガンベルコピュラの逆関数を適用\n",
        "          x = (-np.log(u.item())) ** theta#arrayをfloatに\n",
        "          y = (-np.log(v.item())) ** theta\n",
        "          #print(\"x,y:\",x,y)\n",
        "\n",
        "          t = (x + y) ** (1/theta)\n",
        "          #print(\"t:\",t)\n",
        "\n",
        "          u_new = np.exp(-t * (x / (x + y)))\n",
        "          v_new = np.exp(-t * (y / (x + y)))\n",
        "\n",
        "          # ガンマ分布に変換\n",
        "          v1 = gamma.ppf(u_new, shape0, scale=scales[0])\n",
        "          v2 = gamma.ppf(v_new, shape1, scale=scales[1])\n",
        "          #print(v1,v2)\n",
        "          #v1 = gamma.rvs(shape0, scale=scales[0])\n",
        "          #v2 = gamma.rvs(shape1, scale=scales[1])\n",
        "\n",
        "          #print(\"稼働継続\")\n",
        "          #print(v1,v2, \"劣化増分\")\n",
        "          self.levels[0]+=v1\n",
        "          self.levels[1]+=v2\n",
        "          #print(self.levels, \"劣化\")\n",
        "\n",
        "\n",
        "\n",
        "        elif replacements==[0,1]: #1個取替(順張り) #0907\n",
        "          reward -= self.cs\n",
        "\n",
        "          if self.levels[0]>self.levels[1]:\n",
        "            unit_replaced=0\n",
        "          else:\n",
        "            unit_replaced=1\n",
        "\n",
        "          if self.levels[unit_replaced]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[unit_replaced]=0\n",
        "\n",
        "        elif replacements==[1,0]:#1個取替(逆張り)\n",
        "          reward -= self.cs\n",
        "\n",
        "          if self.levels[0]>self.levels[1]:\n",
        "            unit_replaced=1\n",
        "          else:\n",
        "            unit_replaced=0\n",
        "\n",
        "          if self.levels[unit_replaced]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[unit_replaced]=0\n",
        "\n",
        "        else: #両方取替\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          if self.levels[1]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels=[0,0]\n",
        "\n",
        "        #print(self.levels)\n",
        "        #print(reward)\n",
        "\n",
        "        #level_ohe= self.one_hot_encode()\n",
        "\n",
        "        #print(f'状態:{self.levels}, 保全状態:{self.maintenance_status}, 在庫:{self.inventory}, 需要:{self.demand}, 残り時間:{self.remain_interval}, 保全行動:{replacements}, {speeds}')\n",
        "        #print(\"#############\")\n",
        "\n",
        "        flags = [0,0]#故障フラグ\n",
        "        if self.levels[0] >= self.L:\n",
        "          flags[0] = 1\n",
        "        if self.levels[1] >= self.L:\n",
        "          flags[1] = 1\n",
        "\n",
        "        #切断正規分布\n",
        "        #dist_N = truncnorm(0, 2, loc=1, scale=1)\n",
        "        #self.load_total=float(dist_N.rvs(1))\n",
        "        #self.load_total = float(np.random.randint(0, 2))+1#0916\n",
        "        #print(self.load_total)\n",
        "\n",
        "        #self.Visit[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] += 1\n",
        "        #levels_after=[]\n",
        "        #for i in range(self.n_units):\n",
        "          #l=list([self.levels[i][0]])\n",
        "          #levels_after =levels_after+l\n",
        "        #print(self.levels,\"levels_after\")\n",
        "\n",
        "        return reward, self.levels, flags, self.load_total\n",
        "\n",
        "        #return reward, level_ohe, mstatus_ohe, \\\n",
        "        #       0, (self.demand-mean)/variance, self.remain_interval * 2 / self.interval - 1, flag\n",
        "\n",
        "\n",
        "    #劣化レベル順にすべき可能性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "z_7ruy0iTYPa",
        "outputId": "69a3cc0e-bab1-41b5-a2ad-95ba04e3ea21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    def generate_advantage(self):\\n        advantage = []\\n        gae = 0\\n        gamma = math.exp(-self.beta)\\n        lambd = 0.0\\n        for t in reversed(range(len(self.rewards))):\\n            if t == len(self.rewards) - 1:\\n                delta = self.rewards[t] - self.vals[t]\\n            else:\\n                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\\n            gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\\n            advantage.insert(0, gae)\\n        self.advantage = np.array(advantage, dtype=np.float32)\\n\\n    def generate_advantage(self):\\n\\n        advantage = []\\n        gae = 0\\n        for t in reversed(range(len(self.rewards))):\\n            gamma = 1\\n            if t == len(self.rewards) - 1:\\n                delta = self.rewards[t] - self.vals[t]\\n            else:\\n                gamma = math.exp(-self.beta*1) #修正\\n                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\\n            gae = delta + gamma * gamma * gae\\n            advantage.insert(0, gae)\\n        self.advantage = np.array(advantage,dtype=np.float32)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self,batch_size, interval, beta, GAE_lam):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.vals = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.time = []\n",
        "        self.batch_size = batch_size\n",
        "        self.interval = interval\n",
        "        self.beta = beta\n",
        "        self.advantage = []\n",
        "        self.lam = GAE_lam\n",
        "\n",
        "\n",
        "\n",
        "    def generate_advantage(self):\n",
        "        advantage = []\n",
        "        adv = 0\n",
        "        #gamma = math.exp(-self.beta)\n",
        "        #lambd = 0.0\n",
        "        beta=0.99\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            if t == len(self.rewards) - 1:\n",
        "                adv = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                adv = self.rewards[t] + beta * self.vals[t+1] - self.vals[t]\n",
        "            #gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\n",
        "            advantage.insert(0, adv)\n",
        "        self.advantage = np.array(advantage, dtype=np.float32)\n",
        "\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "               np.array(self.acts_dsc),\\\n",
        "               np.array(self.acts_cnt),\\\n",
        "               np.array(self.probs_dsc),\\\n",
        "               np.array(self.probs_cnt),\\\n",
        "               np.array(self.vals),\\\n",
        "               np.array(self.rewards),\\\n",
        "               np.array(self.advantage),\\\n",
        "               batches\n",
        "\n",
        "\n",
        "\n",
        "    def store_memory(self, state, act_dsc, act_cnt, probs_dsc, probs_cnt, vals, reward):\n",
        "        self.states.append(state)\n",
        "        self.acts_dsc.append(act_dsc)\n",
        "        self.acts_cnt.append(act_cnt)\n",
        "        #self.probs.append(probs)\n",
        "        self.probs_dsc.append(probs_dsc)\n",
        "        self.probs_cnt.append(probs_cnt)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        #self.time.append(time)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.vals = []\n",
        "        self.time = []\n",
        "\n",
        "\"\"\"\n",
        "    def generate_advantage(self):\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        gamma = math.exp(-self.beta)\n",
        "        lambd = 0.0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage, dtype=np.float32)\n",
        "\n",
        "    def generate_advantage(self):\n",
        "\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            gamma = 1\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                gamma = math.exp(-self.beta*1) #修正\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + gamma * gamma * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage,dtype=np.float32)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJC8xTojTYPa"
      },
      "outputs": [],
      "source": [
        "class MATEncoderDecoderNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, d_model, n_heads, d_ff, n_layers, action_dim_a, action_dim_b, lr, chkpt_dir=\"\"):\n",
        "        super(MATEncoderDecoderNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"EncoderDecoder_torch_mat\")\n",
        "\n",
        "        # Input embedding layer to match d_model\n",
        "        self.input_embedding = nn.Linear(state_dim, d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, batch_first=True)#エンコーダ(1層)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)#エンコーダ(n層)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, batch_first=True)#デコーダ(1層)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)#デコーダ(n層)\n",
        "\n",
        "        # Linear layers for action outputs\n",
        "        self.action_a_head = nn.Linear(d_model, action_dim_a)  # For discrete actions of Agent A\n",
        "        self.action_b_mean = nn.Linear(d_model, action_dim_b)  # Mean for continuous actions of Agent B\n",
        "        self.action_b_log_std = nn.Linear(d_model, action_dim_b)  # Log std for continuous actions of Agent B\n",
        "\n",
        "        # Value estimation head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, 1)  # Output scalar value V(s)\n",
        "        )\n",
        "\n",
        "        # Optimizer with different learning rates for each module\n",
        "        self.optimizer = optim.Adam([\n",
        "            {'params': self.input_embedding.parameters(), 'lr': lr/10},  # Input embedding\n",
        "            {'params': self.encoder.parameters(), 'lr': lr},               # Transformer Encoder\n",
        "            {'params': self.decoder.parameters(), 'lr': lr},         # Transformer Decoder\n",
        "            {'params': self.action_a_head.parameters(), 'lr': lr*10},   # Action head for Agent A\n",
        "            {'params': self.action_b_mean.parameters(), 'lr': lr*10},         # Mean for Agent B's actions\n",
        "            {'params': self.action_b_log_std.parameters(), 'lr': lr*10},      # Log std for Agent B's actions\n",
        "            {'params': self.value_head.parameters(), 'lr': lr*10000}       # Value estimation head\n",
        "        ])\n",
        "\n",
        "\n",
        "        # 以下に初期化コードを追加\n",
        "        self.init_dsc_weights()\n",
        "\n",
        "        # Activation functions\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "    def init_dsc_weights(self):\n",
        "        # ここで特定の出力確率を設定するための重みとバイアスを設定\n",
        "        with torch.no_grad():\n",
        "            # すべての出力がほぼ等しくなるように設定\n",
        "            self.action_a_head.weight.fill_(0.0)\n",
        "            # 特定の確率分布に調整\n",
        "            self.action_a_head.bias.data = torch.log(torch.tensor([0.2, 0.2, 0, 0.6]))  # logを取るのがポイント\n",
        "\n",
        "    def init_mean_weights(self):\n",
        "        # mean レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.action_b_mean.weight.fill_(0.0)\n",
        "            self.action_b_mean.bias.fill_(0.0)  # ここで固定出力0.5を設定#0に変更\n",
        "\n",
        "    def init_std_weights(self):\n",
        "        # std レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.value_head.weight.fill_(0.0)\n",
        "            self.value_head.bias.fill_(0.0)  # ここで固定出力0.7を設定\n",
        "\n",
        "    def forward(self, state):\n",
        "        state_rev = state.clone()#0907\n",
        "        #print(\"state;\",state)\n",
        "        if state[0][0]>state[0][1]:\n",
        "          state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "          state_rev[:, [2, 3]] = state_rev[:, [3, 2]] #flag\n",
        "        #print(\"state;\",state_rev)\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "        Args:\n",
        "            state (torch.Tensor): Input state tensor with shape (batch_size, seq_len, state_dim).\n",
        "\n",
        "        Returns:\n",
        "            dist_a (Categorical): Distribution over discrete actions for Agent A.\n",
        "            dist_b (Normal): Distribution over continuous actions for Agent B.\n",
        "            value (torch.Tensor): Estimated value V(s).\n",
        "        \"\"\"\n",
        "        # Add sequence dimension if missing\n",
        "        if state_rev.ndim == 2:\n",
        "            state_rev = state_rev.unsqueeze(1)\n",
        "\n",
        "        # Embed the input to match d_model\n",
        "        embedded_state = self.input_embedding(state_rev)\n",
        "\n",
        "        # Encoder pass\n",
        "        encoded_state = self.encoder(embedded_state)\n",
        "\n",
        "        # Prepare decoder input (use encoded state as both input and context)\n",
        "        decoder_input = encoded_state.clone()\n",
        "        decoder_output = self.decoder(tgt=decoder_input, memory=encoded_state)\n",
        "\n",
        "        # Agent A (Discrete action)\n",
        "        action_a_logits = self.action_a_head(decoder_output[:, 0, :])  # Take first token output\n",
        "        action_a_probs = self.softmax(action_a_logits)\n",
        "        dist_a = Categorical(action_a_probs)\n",
        "\n",
        "        # Agent B (Continuous action)\n",
        "        action_b_mean = self.tanh(self.action_b_mean(decoder_output[:, 0, :]))\n",
        "        if self.training:\n",
        "            # 学習時：Agent Aの確率分布を利用\n",
        "            # Agent B (Continuous action) depends on Agent A's probabilities\n",
        "            action_b_mean = self.tanh(self.action_b_mean(decoder_output[:, 0, :]) * action_a_probs.unsqueeze(-1))\n",
        "        else:\n",
        "            # 推論時：Agent Aの行動をサンプリング\n",
        "            action_a = dist_a.sample()  # Sample action from Agent A\n",
        "            action_b_mean = self.tanh(self.action_b_mean(decoder_output[:, 0, :]) * action_a.unsqueeze(-1))\n",
        "\n",
        "        action_b_log_std = torch.clamp(self.action_b_log_std(decoder_output[:, 0, :]), min=-20, max=2)\n",
        "        action_b_std = action_b_log_std.exp()\n",
        "        if state[0][0]<state[0][1]:\n",
        "          dist_b = torch.distributions.Normal(loc=action_b_mean, scale=action_b_std)\n",
        "        else:\n",
        "          dist_b = torch.distributions.Normal(loc=-action_b_mean, scale=action_b_std)\n",
        "\n",
        "\n",
        "        # Value estimation\n",
        "        #value = self.value_head(encoded_state[:, 0, :])  # Use the first token's representation\n",
        "        value = self.value_head(encoded_state)\n",
        "\n",
        "        return dist_a, dist_b, value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78j9p-tgTYPb"
      },
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=32, fc2_dims=32, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"critic_torch_ppo\")\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims,fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(fc2_dims,fc3_dims),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(fc2_dims,1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # 重みの初期化\n",
        "        #self.init_weights()\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        #with torch.no_grad():\n",
        "            #self.mean.weight.fill_(0.0)\n",
        "            #self.mean.bias.fill_(0.0)  # ここで固定出力0.5を設定#0に変更\n",
        "\n",
        "        for layer in self.critic:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(layer.weight)\n",
        "                layer.bias.data.fill_(0.0)  # バイアスを0で初期化\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        state_rev = state.clone()#0907\n",
        "        if state[0][0]>state[0][1]:\n",
        "          state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "          state_rev[:, [2, 3]] = state_rev[:, [3, 2]]#flag\n",
        "        value = self.critic(state_rev)#0908\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMZpQl6gTYPc"
      },
      "outputs": [],
      "source": [
        "test_batch = 0\n",
        "class Agent:\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, beta=0.0005, GAE_lam=0.00, interval=24,\n",
        "                 alpha=0.03, alpha_critic=0.01,\n",
        "                 policy_clip=0.2, batch_size=512*4, n_epochs=4):\n",
        "        self.beta = beta\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.loss_history_detail = []\n",
        "\n",
        "        self.actor_loss_history = []\n",
        "        self.actor_loss_history_detail = []\n",
        "        self.critic_loss_history = []\n",
        "        self.critic_loss_history_detail = []\n",
        "        self.entropy_history = []\n",
        "        self.kl_divergence_history = []\n",
        "\n",
        "        #self.actor_d = ActorNetwork(n_units, n_states, MAX_maintenance_time, input_dims, alpha_actor_d, Actor_Num=1)\n",
        "        #self.actor_c = ActorNetwork(n_units, n_states, MAX_maintenance_time, input_dims, alpha_actor_c, Actor_Num=2)\n",
        "        #self.critic = CriticNetwork(input_dims, alpha_critic)\n",
        "        self.encoder_decoder = MATEncoderDecoderNetwork(state_dim=input_dims, d_model=128, n_heads=4, d_ff=512, n_layers=6, action_dim_a=4, action_dim_b=1, lr=alpha)\n",
        "        self.memory = PPOMemory(batch_size, interval=interval, beta=beta, GAE_lam=GAE_lam)\n",
        "\n",
        "    def remember(self,state,action_dsc,action_cnt,probs_dsc, probs_cnt,vals,reward):#probs_dsc,probs_cnt\n",
        "        self.memory.store_memory(state,action_dsc,action_cnt,probs_dsc, probs_cnt,vals,reward)\n",
        "\n",
        "    def save_models(self):\n",
        "        print(\"... saving models ...\")\n",
        "        #self.actor_d.save_checkpoint(1)\n",
        "        #self.actor_c.save_checkpoint(2)\n",
        "        #self.critic.save_checkpoint()\n",
        "        self.encoder_decoder.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print(\"... loading models ...\")\n",
        "        #self.actor_d.load_checkpoint(1)\n",
        "        #self.actor_c.load_checkpoint(2)\n",
        "        #self.critic.load_checkpoint()\n",
        "        self.encoder_decoder.load_checkpoint()\n",
        "\n",
        "    def choose_action(self,observation):\n",
        "        #state1 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor_d.device)\n",
        "        state1 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.encoder_decoder.device)\n",
        "        #dist_dsc = self.actor_d(state1,1)\n",
        "        #state2 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor_c.device)\n",
        "        #dist_cnt = self.actor_c(state1,2)\n",
        "\n",
        "        #state_rev = state.clone()\n",
        "        #state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "        #value = (self.critic(state)+self.critic(state_rev))/2\n",
        "        #value = self.critic(state1)#1007本当？\n",
        "        dist_dsc, dist_cnt, value = self.encoder_decoder(state1)\n",
        "\n",
        "        act_dsc = dist_dsc.sample()\n",
        "\n",
        "        act_cnt = dist_cnt.sample()\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "\n",
        "        if act_dsc.item() == 3:\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "          #print(log_prob_cnt, \"log_prob_cnt\")\n",
        "        else:\n",
        "          #log_prob_cnt = 0 #dist_cntを参照しないことの補正\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "\n",
        "\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def choose_action_max_prob(self,observation):\n",
        "        #state1 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor_d.device)\n",
        "        state1 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.encoder_decoder.device)\n",
        "        #dist_dsc = self.actor_d(state1,1)\n",
        "        #state2 = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor_c.device)\n",
        "        #dist_cnt = self.actor_c(state1,2)\n",
        "        #print(\"state:\", state, \"dist_dsc.probs:\", dist_dsc.probs, \"dist_cnt.mean:\",dist_cnt.mean,\"dist_cnt.scale\", dist_cnt.scale)\n",
        "        dist_dsc, dist_cnt, value = self.encoder_decoder(state1)\n",
        "\n",
        "        act_dsc = torch.argmax(dist_dsc.probs)\n",
        "        act_cnt = dist_cnt.mean\n",
        "        #print(act_dsc, \":act_dsc\")\n",
        "        #print(act_cnt, \":act_cnt\")\n",
        "\n",
        "        #value = self.critic(state1)\n",
        "\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "        log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def learn(self,episode, entropy_weight):\n",
        "        self.memory.generate_advantage()\n",
        "        actor_loss_sum = 0\n",
        "        critic_loss_sum = 0\n",
        "        entropy_sum = 0\n",
        "        kl_divergence_sum = 0\n",
        "\n",
        "        for _ in range(self.n_epochs):\n",
        "        #for _ in tqdm(range(self.n_epochs), desc=\"Training Progress\"):  # tqdmを用いて進捗表示\n",
        "            \"\"\"\n",
        "            rewards = self.memory.rewards\n",
        "            values = self.memory.vals\n",
        "            times = self.memory.time\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * \\\n",
        "                        (reward_arr[k]+math.exp(-self.beta * (-times[k]%self.interval+self.interval))*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "            \"\"\"\n",
        "            state_arr, act_dsc_arr, act_cnt_arr, old_probs_dsc_arr, old_probs_cnt_arr, vals_arr, reward_arr, advantage, batches=self.memory.generate_batches()\n",
        "            values = vals_arr\n",
        "            \"\"\"\n",
        "            values = vals_arr\n",
        "            times = time_arr\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * (reward_arr[k]+self.gamma*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            \"\"\"\n",
        "            advantage = torch.tensor(advantage).to(self.encoder_decoder.device)\n",
        "\n",
        "\n",
        "            values = torch.tensor(values).to(self.encoder_decoder.device)\n",
        "            start = time.time()\n",
        "            for batch in batches:  # 各バッチの進捗を表示\n",
        "                advantage_batch = advantage[batch].clone()\n",
        "                states1 = torch.tensor(state_arr[batch], dtype=torch.float).to(self.encoder_decoder.device)\n",
        "                #states2 = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor_c.device)\n",
        "\n",
        "                # Forward pass\n",
        "                dist_dsc, dist_cnt, value = self.encoder_decoder(states1)\n",
        "\n",
        "                #離散エージェント\n",
        "                log_old_probs_dsc = torch.tensor(old_probs_dsc_arr[batch]).to(self.encoder_decoder.device)\n",
        "                acts_dsc = torch.tensor(act_dsc_arr[batch]).to(self.encoder_decoder.device)\n",
        "                #dist_dsc = self.encoder_decoder(states1)\n",
        "                log_new_probs_dsc = dist_dsc.log_prob(acts_dsc) #pi_new\n",
        "                prob_ratio_dsc = log_new_probs_dsc.exp()/log_old_probs_dsc.exp()\n",
        "\n",
        "                entropy_dsc = dist_dsc.entropy().sum(dim=0).mean()\n",
        "\n",
        "                weighted_probs_dsc = advantage_batch*prob_ratio_dsc\n",
        "                weighted_clipped_probs_dsc = torch.clamp(prob_ratio_dsc, 1-self.policy_clip, 1+self.policy_clip)*advantage_batch\n",
        "                actor_loss_dsc = -torch.min(weighted_probs_dsc, weighted_clipped_probs_dsc).mean()\n",
        "\n",
        "                total_loss_dsc = actor_loss_dsc #+ 0.01 * entoropy_dsc\n",
        "\n",
        "                #self.actor_d.optimizer_discrete.zero_grad()\n",
        "                #total_loss_dsc.backward(retain_graph=True)\n",
        "                #self.actor_d.optimizer_discrete.step()\n",
        "\n",
        "                #with torch.no_grad():\n",
        "                    #advantage_batch = advantage_batch*prob_ratio_dsc.to(advantage_batch.dtype)\n",
        "\n",
        "                #連続エージェント\n",
        "                log_old_probs_cnt = torch.tensor(old_probs_cnt_arr[batch]).to(self.encoder_decoder.device)\n",
        "                acts_cnt = torch.tensor(act_cnt_arr[batch]).to(self.encoder_decoder.device)\n",
        "                #dist_cnt = self.actor_c(states1,2)\n",
        "                log_new_probs_cnt = dist_cnt.log_prob(acts_cnt)\n",
        "                prob_ratio_cnt = log_new_probs_cnt.exp()/log_old_probs_cnt.exp()\n",
        "                #log_new_probs = dist_dsc.log_prob(acts_dsc) + dist_cnt.log_prob(acts_cnt)\n",
        "\n",
        "                entropy_cnt = dist_cnt.entropy().sum(dim=0).mean()\n",
        "\n",
        "                weighted_probs_cnt = advantage_batch*prob_ratio_cnt\n",
        "                weighted_clipped_probs_cnt = torch.clamp(prob_ratio_cnt, 1-self.policy_clip, 1+self.policy_clip)*advantage_batch\n",
        "                actor_loss_cnt = -torch.min(weighted_probs_cnt, weighted_clipped_probs_cnt).mean()\n",
        "\n",
        "                total_loss_cnt = actor_loss_cnt #+ 0.01 * entoropy_cnt\n",
        "\n",
        "                #self.actor_c.optimizer_continuous.zero_grad()\n",
        "                #total_loss_cnt.backward(retain_graph=True)\n",
        "                #self.actor_c.optimizer_continuous.step()\n",
        "\n",
        "\n",
        "\n",
        "                #critic_value = self.critic(states1)#0907\n",
        "                critic_value = torch.squeeze(value)\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = F.mse_loss(returns.float(),critic_value.float())\n",
        "\n",
        "\n",
        "\n",
        "                #prob_ratio = log_new_probs.exp()/log_old_probs.exp()\n",
        "                #weighted_probs = advantage[batch]*prob_ratio\n",
        "                #weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                #actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "                #print(\"log_old_probs_cnt.exp():\",log_old_probs_cnt.exp())\n",
        "                #print(\"log_new_probs_cnt.exp():\",log_new_probs_cnt.exp())\n",
        "                #print(\"log_old_probs_cnt:\",log_old_probs_cnt)\n",
        "                #print(\"log_new_probs_cnt\",log_new_probs_cnt)\n",
        "\n",
        "                #print(\"prob_ratio_dsc:\",prob_ratio_dsc)\n",
        "                #print(\"prob_ratio_cnt:\",prob_ratio_cnt)\n",
        "\n",
        "\n",
        "                #weighted_probs_dsc = advantage[batch]*prob_ratio_dsc\n",
        "                #weighted_clipped_probs_dsc = torch.clamp(prob_ratio_dsc, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                #actor_loss_dsc = -torch.min(weighted_probs_dsc, weighted_clipped_probs_dsc).mean()\n",
        "\n",
        "                #weighted_probs_cnt = advantage[batch]*prob_ratio_cnt\n",
        "                #weighted_clipped_probs_cnt = torch.clamp(prob_ratio_cnt, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                #actor_loss_cnt = -torch.min(weighted_probs_cnt, weighted_clipped_probs_cnt).mean()\n",
        "\n",
        "\n",
        "                #critic_loss = critic_loss.float()\n",
        "                #print(actor_loss)\n",
        "                #print(critic_loss)\n",
        "                #print(\"#####\")\n",
        "\n",
        "\n",
        "                #entropy = torch.clamp(entropy_dsc,min=0) + torch.clamp(entropy_cnt,min=0)\n",
        "                #entropy = torch.clamp(dist_dsc.entropy().mean(),min=0) + torch.clamp(dist_cnt.entropy().mean(), min=0.0)\n",
        "\n",
        "                total_loss = total_loss_dsc + total_loss_cnt + 0.5*critic_loss #+ 0.01*entropy\n",
        "                #total_loss = actor_loss + 0.5*critic_loss + entropy_weight*entropy\n",
        "\n",
        "                #if episode >= threshold:\n",
        "                  #print(\"FLAG\")\n",
        "\n",
        "\n",
        "                #if episode >= threshold:\n",
        "                  #print(\"FLAG\")\n",
        "\n",
        "\n",
        "                #self.actor.optimizer.zero_grad()\n",
        "                #actor_loss.backward()\n",
        "                #self.actor.optimizer.step()\n",
        "\n",
        "                #self.critic.optimizer.zero_grad()\n",
        "                #critic_loss.backward()\n",
        "                #self.critic.optimizer.step()\n",
        "\n",
        "                #self.loss_history_detail.append(total_loss.item())\n",
        "\n",
        "                # Backpropagation\n",
        "                self.encoder_decoder.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.encoder_decoder.optimizer.step()\n",
        "\n",
        "                # Logging\n",
        "                #actor_loss_sum += actor_loss_dsc.item()+actor_loss_cnt.item()\n",
        "                actor_loss_sum += actor_loss_dsc.item()+actor_loss_cnt.item()\n",
        "                critic_loss_sum += critic_loss.item()\n",
        "                entropy_sum += entropy_dsc.item()+entropy_cnt.item()\n",
        "                kl_divergence_sum += torch.distributions.kl_divergence(Categorical(logits=log_old_probs_dsc+log_old_probs_cnt), Categorical(logits=log_new_probs_dsc+log_new_probs_cnt)).mean().item()\n",
        "\n",
        "                #print(\"advantage[batch].size(),advantage[batch]:\",advantage[batch].size(),advantage[batch])\n",
        "\n",
        "        print(f'actor loss: {actor_loss_sum}, critic loss: {critic_loss_sum}, entropy: {entropy_sum}, KL divergence: {kl_divergence_sum}')\n",
        "        self.loss_history.append(np.mean(self.loss_history_detail[-self.n_epochs:]))\n",
        "        self.actor_loss_history.append(actor_loss_sum)\n",
        "        self.critic_loss_history.append(critic_loss_sum)\n",
        "        self.entropy_history.append(entropy_sum)\n",
        "        self.kl_divergence_history.append(kl_divergence_sum)\n",
        "            # Update sums\n",
        "        #self.actor.scheduler_actor.step()  # 学習率を更新\n",
        "        #self.critic.scheduler_critic.step()  # 学習率を更新\n",
        "        self.memory.clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jRw3Bjb7TYPb",
        "outputId": "e3745e36-e3d4-4a04-9803-ed30072ac0df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PW7F3zTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkbRyHFHTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JV5KgBxTYPd"
      },
      "outputs": [],
      "source": [
        "#エージェントの初期化\n",
        "n_units = 2\n",
        "n_states = 5\n",
        "MAX_maintenance_time = 0\n",
        "#input_size = n_units * n_states + n_units * (MAX_maintenance_time) + 2 #MDPのため[残り時間]と[保全意思決定時]の2つの入力は入れない\n",
        "input_size = n_units*2 + 1 #flagを追加#lを追加\n",
        "#action_size = 2**n_units  # 行動数は2^2個\n",
        "batch_size = 512*4#512-5120\n",
        "interval = 24\n",
        "#alpha_actor_d = 0.0001#ここを変更する0.0001#0917\n",
        "alpha = 0.00001\n",
        "alpha_critic = 0.2#ここを変更する0.1\n",
        "n_epochs = 4\n",
        "policy_clip = 0.1\n",
        "beta=0.0005\n",
        "\n",
        "\n",
        "agent = Agent(n_units=n_units,\n",
        "              input_dims=input_size,\n",
        "              n_states=n_states,\n",
        "              MAX_maintenance_time=MAX_maintenance_time,\n",
        "              beta=beta,\n",
        "              interval=interval,\n",
        "              alpha=alpha,\n",
        "              #alpha_actor_c=alpha_actor_c,\n",
        "              alpha_critic=alpha_critic,\n",
        "              policy_clip=policy_clip,\n",
        "              batch_size=batch_size,\n",
        "              n_epochs=n_epochs)\n",
        "env = Environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-m2zQoab8sw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV6_pfkGblJK",
        "outputId": "397aff2d-e6dd-483b-86f0-dbb357420d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.2000, 0.2000, 0.0000, 0.6000]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[[-0.0882],\n",
            "         [-0.0882],\n",
            "         [-0.0000],\n",
            "         [-0.2591]]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[[0.5083],\n",
            "         [0.5083],\n",
            "         [0.5083],\n",
            "         [0.5083]]], grad_fn=<ExpandBackward0>) value0: tensor([[[0.2502]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "observation0=[0,0,0,0,1]\n",
        "state0 = torch.tensor(np.array([observation0]),dtype=torch.float).to(agent.encoder_decoder.device)\n",
        "dist_dsc0,dist_cnt0,value0 = agent.encoder_decoder(state0)\n",
        "#state2 = torch.tensor(np.array([observation0]),dtype=torch.float).to(agent.actor_c.device)\n",
        "#dist_cnt0 = agent.encoder_decoder(state0)\n",
        "#value0 = agent.critic(state0)#1007本当？\n",
        "print(\"state:\", state0, \"dist_dsc.probs:\", dist_dsc0.probs, \"dist_cnt.mean:\",dist_cnt0.mean,\"dist_cnt.scale\", dist_cnt0.scale,\"value0:\",value0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "15IhPu1nTYPd",
        "outputId": "f6302102-c474-42da-b16b-a01a943687ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a Tensor with 4 elements cannot be converted to Scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3b6ce659099e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload_total\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mact_dsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_dsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_prob_dsc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_prob_cnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mact_dsc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'0{env.n_units}b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-80f906c0401e>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0;31m#log_prob_cnt = 0 #dist_cntを参照しないことの補正\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m           \u001b[0mlog_prob_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_cnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 4 elements cannot be converted to Scalar"
          ]
        }
      ],
      "source": [
        "num_episode=128#4で1分\n",
        "threshold = 0 #cnt学習を開始するエピソード\n",
        "best_reward = -np.inf\n",
        "episode_reward_history = []\n",
        "avg_cost = 0\n",
        "critic_value0_history=[]\n",
        "entropy_weight=0.01\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    episode_reward = 0\n",
        "    operation_time = 0\n",
        "    one_action = 0\n",
        "    two_action = 0\n",
        "    three_action = 0\n",
        "    penalty_action = 0\n",
        "    #level_ohe, load_total = env.init_random()\n",
        "    levels, flags, load_total = env.init_random()\n",
        "    if episode % 100 == 0:\n",
        "        interval_time_episode = time.time()\n",
        "    interval_time_episode = time.time()\n",
        "    for i in range(256):#1024*8\n",
        "        #state = level_ohe + list([inventory,demand])\n",
        "        #print(levels,\"levels\")\n",
        "        state = levels + flags + list([load_total])\n",
        "        #print(state)\n",
        "        act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action(state)\n",
        "        log_prob=log_prob_dsc+log_prob_cnt\n",
        "        act_dsc_list = [int(bit) for bit in format(act_dsc.item(), f'0{env.n_units}b')]\n",
        "        if sum(act_dsc_list) == 2:\n",
        "            one_action += 1\n",
        "        elif sum(act_dsc_list) == 1:\n",
        "            two_action += 1\n",
        "        elif sum(act_dsc_list) == 0:\n",
        "            three_action += 1\n",
        "        act_cnt_np = act_cnt.squeeze().cpu().numpy().copy()\n",
        "        act_cnt_np = act_cnt_np * 0.5 + 0.5 #補正\n",
        "\n",
        "        if act_cnt_np<0.5:\n",
        "          env.cntCount[0]+=1\n",
        "        else:\n",
        "          env.cntCount[1]+=1\n",
        "        #print(act_dsc_list,act_cnt_np)\n",
        "        #reward, level_ohe_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "        reward, levels_next, flags_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "\n",
        "        episode_reward = episode_reward *0.99 + reward\n",
        "        #episode_reward = episode_reward + reward *0.99\n",
        "        #penalty_action += flag\n",
        "        #if remain_interval > remain_interval_next:\n",
        "            #operation_time += (remain_interval+1)/2*interval - (remain_interval_next+1)/2*interval\n",
        "        #else:\n",
        "            #operation_time += (remain_interval_next + 1) / 2 * interval\n",
        "        agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob_dsc, log_prob_cnt, val, reward)\n",
        "        #agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob, val, reward)\n",
        "        levels = levels_next\n",
        "        flags = flags_next\n",
        "        #mstatus_ohe = mstatus_ohe_next\n",
        "        #inventory = inventory_next\n",
        "        #demand = demand_next\n",
        "        #remain_interval = remain_interval_next\n",
        "        load_total = load_total_next\n",
        "        if (i + 1) % 128 == 0:\n",
        "           print(\"episode_reward,reward:\",episode_reward,reward,\"act_dsc, act_cnt:\",act_dsc_list,act_cnt_np,\"log_prob_dsc, log_prob_cnt, levels:\",log_prob_dsc, log_prob_cnt, levels)\n",
        "    #print(f'{episode}エピソード目の時間：{time.time()-interval_time_episode}')\n",
        "    interval_time_episode = time.time()\n",
        "    if threshold<=episode:\n",
        "      entropy_weight =0.01\n",
        "\n",
        "\n",
        "    agent.learn(episode, entropy_weight)\n",
        "\n",
        "    old_agent = Agent(n_units=n_units,\n",
        "                        input_dims=input_size,\n",
        "                        n_states=n_states,\n",
        "                        MAX_maintenance_time=MAX_maintenance_time,\n",
        "                        beta=beta,\n",
        "                        interval=interval,\n",
        "                        alpha=alpha,\n",
        "                        #alpha_actor_c=alpha_actor_c,\n",
        "                        alpha_critic=alpha_critic,\n",
        "                        policy_clip=policy_clip,\n",
        "                        batch_size=batch_size,\n",
        "                        n_epochs=n_epochs)\n",
        "    if episode != 0:\n",
        "        old_agent.load_models()\n",
        "\n",
        "    #if Check_convergence(agent, old_agent, n_units, n_states, MAX_maintenance_time):\n",
        "    #    break\n",
        "\n",
        "\n",
        "\n",
        "    agent.save_models()\n",
        "    print(f'状態{state}, 離散行動：{act_dsc_list}, 連続行動：{act_cnt_np}')\n",
        "    print(f'[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [{env.replace_chance}, {env.failure_keep1}, {env.failure_keep2}, {env.failure_keep3}]')\n",
        "    env.replace_chance = 0\n",
        "    env.failure_keep1 = 0\n",
        "    env.failure_keep2 = 0\n",
        "    env.failure_keep3 = 0\n",
        "    #print(f'{episode}エピソード目の学習時間：{time.time()-interval_time_episode}')\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    print(f'{episode}エピソード目の累積報酬：{episode_reward}, 一つ保全の回数：{one_action}, 二つ保全の回数：{two_action}, 三つ保全の回数：{three_action}, 違反回数：{penalty_action},episode_reward_history：{episode_reward_history}')\n",
        "\n",
        "    #_, _, value = agent.encoder_decoder(state0.unsqueeze(0))  # バッチ次元を追加\n",
        "    #value0 = float(value[0, 0])  # 推定価値を取得\n",
        "    #critic_value0_history.append(float(value[0, 0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncPGOL29TYPd"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHLLAogeTYPe"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_curve2(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "\n",
        "    # 直近10エピソードの移動平均を計算してプロット\n",
        "    if len(rewards) >= 10:\n",
        "        moving_average = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
        "        plt.plot(np.arange(9, len(rewards)), moving_average, label='Moving Average (last 10 episodes)', linestyle='--')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# エージェントの学習\n",
        "# (agent.train()の呼び出しなど)\n",
        "\n",
        "# 学習後のエージェントの評価\n",
        "#evaluate_agent(agent, env, num_episodes=10)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "last10=episode_reward_history[-10:]\n",
        "print(last10)\n",
        "print(np.mean(last10))\n",
        "plot_learning_curve(episode_reward_history, title=\"PPO Learning Curve\", label=\"Total reward\")\n",
        "plot_learning_curve(agent.loss_history_detail, title=\"loss curve\", label=\"Total loss (actor loss +  critic loss) \")\n",
        "plot_learning_curve(agent.actor_loss_history, title=\"actor loss curve\", label=\"actor loss\")\n",
        "plot_learning_curve(agent.critic_loss_history, title=\"critic loss curve\", label=\"critic loss\")\n",
        "plot_learning_curve(agent.entropy_history, title=\"entropy curve\", label=\"entropy\")\n",
        "plot_learning_curve(agent.kl_divergence_history, title=\"kl divergence curve\", label=\"kl divergence\")\n",
        "plot_learning_curve(critic_value0_history, title=\"PPO Learning Curve\", label=\"critic_value0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3bLWs2RTYPe"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(levels, n_units=2,n_states=5, MAX_maintenance_time=0):\n",
        "    level_ohe = []\n",
        "    #mstatus_ohe = []\n",
        "    for unit_idx in range(n_units):\n",
        "        l = [0] * n_states\n",
        "        #m = [0] * (MAX_maintenance_time + 1)\n",
        "        l[levels[unit_idx]] = 1\n",
        "        #m[maintenance_status[unit_idx]] = 1\n",
        "        level_ohe = level_ohe + l\n",
        "        #mstatus_ohe = mstatus_ohe + m\n",
        "    return level_ohe #, mstatus_ohe\n",
        "\n",
        "\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        print(action)\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"lightblue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, act_dsc, act_cnt, load_total):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "    if act_dsc==0:\n",
        "        ax.text(center_x,center_y,f'M12',ha='center', va='center')\n",
        "    elif act_dsc==1:\n",
        "        if center_x<center_y:\n",
        "          ax.text(center_x,center_y,f'M2',ha='center', va='center')\n",
        "        else:\n",
        "          ax.text(center_x,center_y,f'M1',ha='center', va='center')\n",
        "    elif act_dsc==2:\n",
        "        if center_x<center_y:\n",
        "          ax.text(center_x,center_y,f'M1',ha='center', va='center')\n",
        "        else:\n",
        "          ax.text(center_x,center_y,f'M2',ha='center', va='center')\n",
        "    else: #稼働継続\n",
        "        print(act_cnt, type(act_cnt))\n",
        "        act_cnt = act_cnt * 0.5 + 0.5\n",
        "        #ax.text(center_x, center_y,f'{(round(act_cnt[0],2),round(1-act_cnt[0],2))}',ha='center', va='center',fontsize=5)\n",
        "        ax.text(center_x, center_y, f'{(round(act_cnt, 2))}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "def plot_state_value(ax, center_x, center_y, val):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "\n",
        "    ax.text(center_x, center_y, f'{round(float(val), 0)}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 10\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a\n",
        "            x2 =s2*a\n",
        "\n",
        "            flag1=0\n",
        "            flag2=0\n",
        "            if s1>=x:\n",
        "              flag1=1\n",
        "            if s2>=x:\n",
        "              flag2=1\n",
        "\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] +[flag1,flag2] +list([load_total])\n",
        "            #print(state)\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "    # 軸のラベルを設定\n",
        "    ax.set_xticks(range(x+2))  # 目盛りの位置を設定\n",
        "    ax.set_xticklabels([f\"{int(label * a)}\" for label in ax.get_xticks()])  # X倍の値でラベルを整数で更新\n",
        "    ax.set_yticks(range(x+2))\n",
        "    ax.set_yticklabels([f\"{int(label * a)}\" for label in ax.get_yticks()])\n",
        "    plt.show()\n",
        "\n",
        "def state_value(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 10\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a\n",
        "            x2 =s2*a\n",
        "\n",
        "            flag1=0\n",
        "            flag2=0\n",
        "            if s1>=x:\n",
        "              flag1=1\n",
        "            if s2>=x:\n",
        "              flag2=1\n",
        "\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] +[flag1,flag2] +list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_state_value(ax, s1, s2, val)\n",
        "    # 軸の範囲と目盛りを設定\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1.0)\n",
        "optimal_policy(load_total=2.0)\n",
        "\n",
        "\n",
        "state_value(load_total=1)\n",
        "\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=0,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=1,m2=1,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=0.5)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "s1 = 4\n",
        "s2 = 4\n",
        "s3 = 4\n",
        "m1 = 0\n",
        "m2 = 0\n",
        "m3 = 0\n",
        "inventory = 0\n",
        "#demand = 0\n",
        "remain_interval = 1\n",
        "#level_ohe, mstatus_ohe = one_hot_encoding(levels=[s1,s2,s3],maintenance_status=[m1,m2,m3])\n",
        "#state = level_ohe + mstatus_ohe + list([inventory,demand,remain_interval])\n",
        "#act_dsc, act_cnt, log_prob, val = agent.choose_action_max_prob(state)\n",
        "#act_dsc = act_dsc.item()\n",
        "#act_dsc\n",
        "print(env.Visit)\n",
        "print(env.cntCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBPaE5n89aZQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"blue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, size, action):\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        size,\n",
        "        size,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(action)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    if action >= 0:\n",
        "        r = list(map(int, bin(action)[2:].zfill(env.num_targets)))\n",
        "        #ax.text(center_x,center_y,f'({r[0]},{r[1]},{r[2]})',ha='center', va='center')\n",
        "        ax.text(center_x,center_y,f'{action}',ha='center', va='center')\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 26\n",
        "    for s1 in range(x):\n",
        "        for s2 in range(x):\n",
        "            state = [s1,s2] + list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "            plot_action(ax,cs2,cs3,size=1,action=action)\n",
        "    ax.set_xlim(-0.5,x-1+0.5)\n",
        "    ax.set_ylim(-0.5,x-1+0.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s2\")\n",
        "    ax.set_ylabel(\"s3\")\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"(s1=\"+str(cs1)+\", s2, s3, z1=\"+str(cz1)+\", z2=\"+str(cz2)+\", z3=\"+str(cz3)+\")\")\n",
        "    plt.show()\n",
        "\n",
        "optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1)\n",
        "optimal_policy(load_total=1.8)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH5e1egBJRR_"
      },
      "outputs": [],
      "source": [
        "num_episode =1000#4で1分\n",
        "threshold = 0 #cnt学習を開始するエピソード\n",
        "best_reward = -np.inf\n",
        "episode_reward_history2 = []\n",
        "avg_cost = 0\n",
        "critic_value0_history=[]\n",
        "entropy_weight=0.01\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    episode_reward = 0\n",
        "    operation_time = 0\n",
        "    one_action = 0\n",
        "    two_action = 0\n",
        "    three_action = 0\n",
        "    penalty_action = 0\n",
        "    #level_ohe, load_total = env.init_random()\n",
        "    levels, flags, load_total = env.init_random()\n",
        "    if episode % 100 == 0:\n",
        "        interval_time_episode = time.time()\n",
        "    interval_time_episode = time.time()\n",
        "    for i in range(256):#1024*8\n",
        "        #state = level_ohe + list([inventory,demand])\n",
        "        #print(levels,\"levels\")\n",
        "        state = levels + flags + list([load_total])\n",
        "        #print(state)\n",
        "        act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)#決定論的方策\n",
        "        #act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action(state)#確率的方策\n",
        "        log_prob=log_prob_dsc+log_prob_cnt\n",
        "        act_dsc_list = [int(bit) for bit in format(act_dsc.item(), f'0{env.n_units}b')]\n",
        "        if sum(act_dsc_list) == 2:\n",
        "            one_action += 1\n",
        "        elif sum(act_dsc_list) == 1:\n",
        "            two_action += 1\n",
        "        elif sum(act_dsc_list) == 0:\n",
        "            three_action += 1\n",
        "        act_cnt_np = act_cnt.squeeze().cpu().detach().numpy().copy()#決定論的方策\n",
        "        #act_cnt_np = act_cnt.squeeze().cpu().numpy().copy()#確率的方策\n",
        "        act_cnt_np = act_cnt_np * 0.5 + 0.5 #補正\n",
        "\n",
        "        if act_cnt_np<0.5:\n",
        "          env.cntCount[0]+=1\n",
        "        else:\n",
        "          env.cntCount[1]+=1\n",
        "        #print(act_dsc_list,act_cnt_np)\n",
        "        #reward, level_ohe_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "        reward, levels_next, flags_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "\n",
        "        episode_reward = episode_reward *0.99 + reward\n",
        "        #episode_reward = episode_reward + reward *0.99\n",
        "        #penalty_action += flag\n",
        "        #if remain_interval > remain_interval_next:\n",
        "            #operation_time += (remain_interval+1)/2*interval - (remain_interval_next+1)/2*interval\n",
        "        #else:\n",
        "            #operation_time += (remain_interval_next + 1) / 2 * interval        #agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob_dsc, log_prob_cnt, val, reward, operation_time)\n",
        "        #agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob, val, reward)\n",
        "        levels = levels_next\n",
        "        flags = flags_next\n",
        "        #mstatus_ohe = mstatus_ohe_next\n",
        "        #inventory = inventory_next\n",
        "        #demand = demand_next\n",
        "        #remain_interval = remain_interval_next\n",
        "        load_total = load_total_next\n",
        "        if (i + 1) % 128 == 0:\n",
        "           print(\"episode_reward,reward:\",episode_reward,reward,\"act_dsc, act_cnt:\",act_dsc_list,act_cnt_np,\"log_prob_dsc, log_prob_cnt, levels:\",log_prob_dsc, log_prob_cnt, levels)\n",
        "    #print(f'{episode}エピソード目の時間：{time.time()-interval_time_episode}')\n",
        "    interval_time_episode = time.time()\n",
        "    if threshold<=episode:\n",
        "      entropy_weight =0.01\n",
        "\n",
        "\n",
        "    #agent.learn(episode, entropy_weight)\n",
        "\n",
        "    old_agent = Agent(n_units=n_units,\n",
        "                        input_dims=input_size,\n",
        "                        n_states=n_states,\n",
        "                        MAX_maintenance_time=MAX_maintenance_time,\n",
        "                        beta=beta,\n",
        "                        interval=interval,\n",
        "                        alpha_actor_d=alpha_actor_d,\n",
        "                        alpha_actor_c=alpha_actor_c,\n",
        "                        alpha_critic=alpha_critic,\n",
        "                        policy_clip=policy_clip,\n",
        "                        batch_size=batch_size,\n",
        "                        n_epochs=n_epochs)\n",
        "    if episode != 0:\n",
        "        old_agent.load_models()\n",
        "\n",
        "    #if Check_convergence(agent, old_agent, n_units, n_states, MAX_maintenance_time):\n",
        "    #    break\n",
        "\n",
        "\n",
        "\n",
        "    #agent.save_models()\n",
        "    print(f'状態{state}, 離散行動：{act_dsc_list}, 連続行動：{act_cnt_np}')\n",
        "    print(f'[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [{env.replace_chance}, {env.failure_keep1}, {env.failure_keep2}, {env.failure_keep3}]')\n",
        "    env.replace_chance = 0\n",
        "    env.failure_keep1 = 0\n",
        "    env.failure_keep2 = 0\n",
        "    env.failure_keep3 = 0\n",
        "    #print(f'{episode}エピソード目の学習時間：{time.time()-interval_time_episode}')\n",
        "    episode_reward_history2.append(episode_reward)\n",
        "    print(f'{episode}エピソード目の累積報酬：{episode_reward}, 一つ保全の回数：{one_action}, 二つ保全の回数：{two_action}, 三つ保全の回数：{three_action}, 違反回数：{penalty_action},episode_reward_history2：{episode_reward_history2}')\n",
        "\n",
        "    critic_value0_history.append(float(agent.critic(state0)[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PUT0R4gJVNF"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_curve2(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "\n",
        "    # 直近10エピソードの移動平均を計算してプロット\n",
        "    if len(rewards) >= 10:\n",
        "        moving_average = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
        "        plt.plot(np.arange(9, len(rewards)), moving_average, label='Moving Average (last 10 episodes)', linestyle='--')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_real_number_frequency(data):\n",
        "    \"\"\"\n",
        "    実数の配列を受け取り、横軸に実数、縦軸にその実数が現れた回数をとるグラフを表示する関数。\n",
        "\n",
        "    Parameters:\n",
        "    data (list or np.array): 実数の配列\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # ヒストグラムをプロット\n",
        "    plt.hist(data, bins=30, edgecolor='black')\n",
        "    plt.xlabel('Real Numbers')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Frequency of Real Numbers')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# エージェントの学習\n",
        "# (agent.train()の呼び出しなど)\n",
        "\n",
        "# 学習後のエージェントの評価\n",
        "#evaluate_agent(agent, env, num_episodes=10)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "last100=episode_reward_history2[-100:]\n",
        "print(last100)\n",
        "print(np.mean(last100))\n",
        "plot_real_number_frequency(episode_reward_history2)\n",
        "plot_learning_curve(episode_reward_history2, title=\"PPO Learning Curve\", label=\"Total reward\")\n",
        "plot_learning_curve(agent.loss_history_detail, title=\"loss curve\", label=\"Total loss (actor loss +  critic loss) \")\n",
        "plot_learning_curve(agent.actor_loss_history, title=\"actor loss curve\", label=\"actor loss\")\n",
        "plot_learning_curve(agent.critic_loss_history, title=\"critic loss curve\", label=\"critic loss\")\n",
        "plot_learning_curve(agent.entropy_history, title=\"entropy curve\", label=\"entropy\")\n",
        "plot_learning_curve(agent.kl_divergence_history, title=\"kl divergence curve\", label=\"kl divergence\")\n",
        "plot_learning_curve(critic_value0_history, title=\"PPO Learning Curve\", label=\"critic_value0\")\n",
        "\n",
        "\n",
        "#df = pd.DataFrame(episode_reward_history, columns=['values'])\n",
        "#df.to_csv('/content/reward1000.csv', index=False)\n",
        "#np.save('/content/reward1000.npy', episode_reward_history)\n",
        "np.save('/content/drive/MyDrive/result/reward1000-HAPPO.npy', episode_reward_history2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "DRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}