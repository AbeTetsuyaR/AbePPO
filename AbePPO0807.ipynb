{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeTetsuyaR/AbePPO/blob/main/AbePPO0807.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qCrwylLHTYPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import cProfile\n",
        "import sys\n",
        "import copy\n",
        "from torch.distributions.categorical import Categorical\n",
        "import math\n",
        "import os\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.colors as mcolors\n",
        "from tqdm import tqdm  # tqdmをインポート\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import gamma, uniform\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L6hr_FE6TYPY"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, n_units=2):\n",
        "        self.n_units = n_units #number of unit\n",
        "        self.n_states = 5 #number of state\n",
        "        self.inventory = 0\n",
        "        self.demand = 0\n",
        "        self.maintenance_status = [0] * self.n_units\n",
        "        self.interval = 24\n",
        "        self.remain_interval = 24\n",
        "        self.MAX_speed = 10/self.interval\n",
        "        self.MAX_inventory = 0\n",
        "        self.MAX_demand = 15\n",
        "        self.MAX_maintenance_time = 0\n",
        "\n",
        "        self.cp = 500#\n",
        "        self.cc = 1800#\n",
        "\n",
        "        self.cps = 0\n",
        "        self.co = 5\n",
        "        self.cs = 500#\n",
        "\n",
        "        self.levels = [0] * self.n_units\n",
        "        self.scale_list = [61,46,31,16]\n",
        "        self.shape = 3\n",
        "        self.penalty = 1\n",
        "        self.L = 100#\n",
        "        self.P_Cost =[[100,120,140,160,2500],\n",
        "                      [120,140,160,180,2520],\n",
        "                      [140,160,180,200,2540],\n",
        "                      [160,180,200,220,2560],\n",
        "                      [2500,2520,2540,2560,2580]]#一旦\n",
        "\n",
        "        self.failure_keep1 = 0 #1つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep2 = 0 #2つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep3 = 0 #3つ故障しているのに保全を選択しなかった回数\n",
        "        self.replace_chance = 0 #保全を選択できた回数\n",
        "\n",
        "    def init_random(self):\n",
        "        l = range(self.n_states)\n",
        "        m = range(self.MAX_maintenance_time)\n",
        "        flag = True\n",
        "        while flag:\n",
        "            for unit_idx in range(self.n_units):\n",
        "                self.levels[unit_idx] = random.choice(l)\n",
        "                if self.levels[unit_idx] == self.n_states-1:\n",
        "                        flag = False\n",
        "                #if self.levels[unit_idx] == 0:\n",
        "                    #self.maintenance_status[unit_idx] = random.choice(m)\n",
        "\n",
        "        #需要\n",
        "        mean = 10\n",
        "        variance = 2  # 標準偏差\n",
        "        mu = np.log(mean**2 / np.sqrt(variance**2 + mean**2))\n",
        "        sigma = np.sqrt(np.log(1 + (variance**2 / mean**2)))\n",
        "        self.demand = np.random.lognormal(mu, sigma)\n",
        "        if self.demand > 15:\n",
        "            self.demand = 15\n",
        "        self.demand = random.uniform(0,15)\n",
        "        #意思決定時\n",
        "        #self.remain_interval = random.uniform(0, self.interval)\n",
        "        #在庫\n",
        "        self.inventory = random.uniform(0,self.MAX_inventory)\n",
        "\n",
        "        level_ohe, mstatus_ohe = self.one_hot_encode()\n",
        "\n",
        "        return level_ohe, mstatus_ohe, \\\n",
        "               0, self.demand * 2 /15 - 1, self.remain_interval * 2 / self.interval - 1\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.levels = np.zeros(self.n_units)\n",
        "\n",
        "    def complete_maintenance(self, unit_idx):\n",
        "        self.levels[unit_idx] = 0\n",
        "\n",
        "    def get_ability(self, level): #良品率\n",
        "        if level == 0:\n",
        "            return 1\n",
        "        elif level == 1:\n",
        "            return 0.8\n",
        "        elif level ==2:\n",
        "            return 0.5\n",
        "        elif level == 3:\n",
        "            return 0.1\n",
        "        return (self.n_states - 1 - level) / (self.n_states - 1)\n",
        "\n",
        "    def update_demand(self, speed, ability, time):\n",
        "        if self.demand >= self.inventory:\n",
        "            self.demand -= self.inventory\n",
        "            self.inventory = 0.0\n",
        "        else:\n",
        "            self.inventory -= self.demand\n",
        "            self.demand = 0.0\n",
        "        return max(0, self.demand-self.inventory-ability*speed*time)\n",
        "\n",
        "    def update_inventory(self, speed, ability, time):\n",
        "        if self.demand <= self.inventory + ability * speed * time:\n",
        "            return min(self.MAX_inventory, -self.demand+self.inventory+ability*speed*time), max(0, -self.MAX_inventory-self.demand+self.inventory+ability*speed*time)\n",
        "        else:\n",
        "            return 0.0, 0.0\n",
        "\n",
        "    def get_maintenance_time(self,level):\n",
        "        return 0\n",
        "\n",
        "    def update_maintenance_time(self, unit_idx):\n",
        "        return 0\n",
        "\n",
        "    def one_hot_encode(self):\n",
        "        level_ohe = []\n",
        "        mstatus_ohe = []\n",
        "        for unit_idx in range(self.n_units):\n",
        "            l = [0] * self.n_states\n",
        "            m = [0] * (self.MAX_maintenance_time + 1)\n",
        "            l[self.levels[unit_idx]] = 1\n",
        "            m[self.maintenance_status[unit_idx]] = 1\n",
        "            level_ohe = level_ohe + l\n",
        "            mstatus_ohe = mstatus_ohe + m\n",
        "        return level_ohe, mstatus_ohe\n",
        "\n",
        "\n",
        "\n",
        "    def operation(self, replacements, speeds):\n",
        "        reward = 0\n",
        "\n",
        "        #生産速度の調整\n",
        "        for i in range(self.n_units):\n",
        "            if speeds[i] < 0:\n",
        "                #reward -= self.penalty\n",
        "                speeds[i] = 0\n",
        "            if speeds[i] > 1:\n",
        "                speeds[i] = 1\n",
        "                #reward -= self.penalty\n",
        "            #if speeds[i] < 0.01:\n",
        "                #speeds[i] = 0\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        #保全の意思決定\n",
        "\n",
        "        if replacements==[1,1]: #稼働継続\n",
        "          # パラメータの設定\n",
        "          scales=[0.79, 0.98, 1.31, 1.76, 2.34, 3.05, 3.90, 4.87, 5.97]#1個追加\n",
        "          shape1 = 0.69  # ガンマ分布のパラメータ v1 用\n",
        "          shape2 = 0.69  # ガンマ分布のパラメータ v2 用\n",
        "          tau = 0.5  # ケンドールの順位相関係数\n",
        "\n",
        "          theta = 1 / (1 - tau)\n",
        "          # 一様乱数を生成\n",
        "          u = uniform.rvs(size=1)\n",
        "          v = uniform.rvs(size=1)\n",
        "\n",
        "          # ガンベルコピュラの変換適用\n",
        "          c = (-np.log(u)) ** theta + (-np.log(v)) ** theta\n",
        "          u_transformed = np.exp(-c**(1/theta))\n",
        "          v_transformed = np.exp(-c**(1/theta))\n",
        "\n",
        "          # 一様乱数をガンマ分布の逆関数に通す\n",
        "          x=math.floor(speeds[0]*10)\n",
        "          if x==10:\n",
        "            x=9\n",
        "          v1 = gamma.ppf(u_transformed, shape1, scale=scales[x])\n",
        "          v2 = gamma.ppf(v_transformed, shape2, scale=scales[9-x])\n",
        "\n",
        "          print(\"稼働継続\")\n",
        "          print(v1,v2)\n",
        "          self.levels[0]+=v1/25\n",
        "          self.levels[1]+=v2/25\n",
        "\n",
        "          reward -= self.P_Cost[math.floor(self.levels[0])][math.floor(self.levels[1])]\n",
        "\n",
        "        elif replacements==[0,1]: #1のみ取替\n",
        "          self.levels[0]=0\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<4:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "\n",
        "        elif replacements==[1,0]: #2のみ取替\n",
        "          self.levels[1]=0\n",
        "          reward -= self.cs\n",
        "          if self.levels[1]<4:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "\n",
        "        else: #両方取替\n",
        "          self.levels=[0,0]\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<4:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          if self.levels[0]<4:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "\n",
        "        print(self.levels)\n",
        "        print(reward)\n",
        "\n",
        "        replace_failure_flag = 0\n",
        "        if abs(self.remain_interval - self.interval) < 0.001:\n",
        "            self.replace_chance += 1\n",
        "            if sum(replacements) == self.n_units: #ここのif文はmaskingしたため通らない（はず）\n",
        "                flag = 0\n",
        "                for i in range(self.n_units):\n",
        "                    if self.levels[i] != self.n_states-1:\n",
        "                        flag = 1\n",
        "                        break\n",
        "                if flag == 0:\n",
        "                    print(\"AAAA\")\n",
        "                    reward -= self.cp * self.n_units * 2 #全部故障で稼働した時のペナルティ\n",
        "\n",
        "\n",
        "            if sum(replacements) != self.n_units:\n",
        "                reward -= self.cs\n",
        "            keep_failure = 0\n",
        "            for unit_idx in range(self.n_units):\n",
        "                if replacements[unit_idx] == 0:\n",
        "                    self.maintenance_status[unit_idx] = self.get_maintenance_time(self.levels[unit_idx])\n",
        "                    self.levels[unit_idx] = 0\n",
        "                else:\n",
        "                    if self.levels[unit_idx] == self.n_states-1:\n",
        "                        keep_failure += 1\n",
        "                if self.maintenance_status[unit_idx] != 0:\n",
        "                    reward -= self.cp\n",
        "            if keep_failure == 1:\n",
        "                self.failure_keep1 += 1\n",
        "                replace_failure_flag = 1\n",
        "            elif keep_failure == 2:\n",
        "                self.failure_keep2 += 1\n",
        "                replace_failure_flag = 1\n",
        "            elif keep_failure == 3:\n",
        "                self.failure_keep3 += 1\n",
        "                replace_failure_flag = 1\n",
        "            for unit_idx in range(self.n_units): #保全をしている場合にかかるセットアップコスト→2個以上の保全をしやすくする？\n",
        "                if self.maintenance_status[unit_idx] != 0:\n",
        "                    reward -= self.cps\n",
        "                    break\n",
        "\n",
        "\n",
        "        #状態推移の更新と生産\n",
        "        operation_flag = True\n",
        "        while operation_flag:\n",
        "            transit_time = self.remain_interval #滞在時間\n",
        "            transit_unit = 100 #推移するユニット番号\n",
        "            transit_level = self.n_states #推移したユニットの推移後の状態\n",
        "\n",
        "            for i in range(self.n_units):\n",
        "                if self.maintenance_status[i] != 0:\n",
        "                    continue\n",
        "                reward += (self.n_states-1 - self.levels[i])/(self.n_states)\n",
        "\n",
        "            for unit_idx in range(self.n_units):\n",
        "                if replacements[unit_idx] == 0 or speeds[unit_idx] == 0 or self.levels[unit_idx] == self.n_states-1 or self.maintenance_status[unit_idx] != 0:\n",
        "                    continue\n",
        "                level = self.levels[unit_idx]\n",
        "                #現在の状態から次の状態へ推移する時の滞在時間を計算し、一番小さい時間と推移する状態を計算する\n",
        "                sojurn_time_list = np.random.weibull(self.shape,self.n_states-1-level) \\\n",
        "                                * (self.scale_list[level]-1.5*self.MAX_speed*self.interval*speeds[unit_idx])\n",
        "                #print(sojurn_time_list)\n",
        "                #最も短い滞在時間とそのユニット、推移後の状態を保持\n",
        "                min_time = min(sojurn_time_list)\n",
        "                min_unit = unit_idx\n",
        "                min_level = np.argmin(sojurn_time_list) + level + 1\n",
        "\n",
        "                if transit_time > min_time:\n",
        "                    transit_time = min_time\n",
        "                    transit_unit = min_unit\n",
        "                    transit_level = min_level\n",
        "\n",
        "            #生産量を計算する\n",
        "            amount = 0\n",
        "            for unit_idx in range(self.n_units):\n",
        "                if replacements[unit_idx] == 0 or self.maintenance_status[unit_idx] != 0 or speeds[unit_idx] == 0 or self.levels[unit_idx] == self.n_states-1 :\n",
        "                    continue\n",
        "                ability = self.get_ability(self.levels[unit_idx])\n",
        "                #print(self.levels[unit_idx], ability, speeds[unit_idx] * self.MAX_speed * transit_time * ability)\n",
        "                amount += speeds[unit_idx] * self.MAX_speed * transit_time * ability\n",
        "\n",
        "            #超過した分をペナルティにする\n",
        "            excess = 0\n",
        "\n",
        "            #販売量を報酬にする\n",
        "            init_demand = self.demand\n",
        "\n",
        "            #在庫と需要の更新\n",
        "            if self.demand >= amount:\n",
        "                self.demand -= amount\n",
        "                amount = 0\n",
        "                if self.demand >= self.inventory:\n",
        "                    self.demand -= self.inventory\n",
        "                    self.inventory = 0\n",
        "                else:\n",
        "                    self.inventory -= self.demand\n",
        "                    self.demand = 0\n",
        "            else:\n",
        "                amount -= self.demand\n",
        "                self.demand = 0\n",
        "                self.inventory = min(self.MAX_inventory, self.inventory + amount)\n",
        "                #超過した分をペナルティにする\n",
        "                excess = max(0, self.inventory + amount - self.MAX_inventory)\n",
        "            #print(excess, init_demand-self.demand)\n",
        "            reward -= excess * 1 #/ self.cp\n",
        "            reward += (init_demand-self.demand) * 3 #/ self.cp\n",
        "\n",
        "            mean = 10\n",
        "            variance = 2  # 標準偏差\n",
        "            mu = np.log(mean**2 / np.sqrt(variance**2 + mean**2))\n",
        "            sigma = np.sqrt(np.log(1 + (variance**2 / mean**2)))\n",
        "\n",
        "            if transit_unit != 100: #状態が推移した時\n",
        "                self.levels[transit_unit] = transit_level\n",
        "                self.remain_interval -= transit_time\n",
        "\n",
        "            else: #状態が推移せず、定期的な意思決定が到着した時\n",
        "                self.remain_interval = self.interval\n",
        "                for unit_idx in range(self.n_units):\n",
        "                    self.maintenance_status[unit_idx] = self.update_maintenance_time(unit_idx)\n",
        "\n",
        "                reward -= self.demand * self.co\n",
        "                operation_flag = False\n",
        "\n",
        "                #次の需要を乱数で発生させる\n",
        "                #  μ と σ を計算\n",
        "                #mean(E[x]) and variance(V[x]) of lognormal\n",
        "                #mean = 10\n",
        "                #variance = 2  # 標準偏差\n",
        "                self.demand = np.random.lognormal(mu, sigma)\n",
        "\n",
        "\n",
        "        level_ohe, mstatus_ohe = self.one_hot_encode()\n",
        "        flag = 0\n",
        "\n",
        "        return reward, level_ohe, mstatus_ohe, \\\n",
        "               0, self.demand * 2 /15 - 1, self.remain_interval * 2 / self.interval - 1, flag\n",
        "\n",
        "        #状態推移の更新\n",
        "        for unit_idx in range(self.n_units):\n",
        "            if replacements[unit_idx] == 0 or speeds[unit_idx] == 0 or self.levels[unit_idx] == self.n_states-1 or self.maintenance_status[unit_idx] != 0:\n",
        "                continue\n",
        "            level = self.levels[unit_idx]\n",
        "            #現在の状態から次の状態へ推移する時の滞在時間を計算し、一番小さい時間と推移する状態を計算する\n",
        "            sojurn_time_list = np.random.weibull(self.shape,self.n_states-1-level) \\\n",
        "                               * (self.scale_list[level]-1.5*self.MAX_speed*self.interval*speeds[unit_idx])\n",
        "            #最も短い滞在時間とそのユニット、推移後の状態を保持\n",
        "            min_time = min(sojurn_time_list)\n",
        "            min_unit = unit_idx\n",
        "            min_level = np.argmin(sojurn_time_list) + level + 1\n",
        "\n",
        "            if transit_time > min_time:\n",
        "                transit_time = min_time\n",
        "                transit_unit = min_unit\n",
        "                transit_level = min_level\n",
        "\n",
        "        #生産量を計算する\n",
        "        amount = 0\n",
        "        for unit_idx in range(self.n_units):\n",
        "            if replacements[unit_idx] == 0 or self.maintenance_status[unit_idx] != 0 or speeds[unit_idx] == 0 or self.levels[unit_idx] == self.n_states-1 :\n",
        "                continue\n",
        "            ability = self.get_ability(self.levels[unit_idx])\n",
        "            #print(self.levels[unit_idx], ability, speeds[unit_idx] * self.MAX_speed * transit_time * ability)\n",
        "            amount += speeds[unit_idx] * self.MAX_speed * transit_time * ability\n",
        "\n",
        "\n",
        "        #print(amount)\n",
        "\n",
        "        #超過した分をペナルティにする\n",
        "        excess = 0\n",
        "\n",
        "        #販売量を報酬にする\n",
        "        init_demand = self.demand\n",
        "\n",
        "\n",
        "        #在庫と需要の更新\n",
        "        if self.demand >= amount:\n",
        "            self.demand -= amount\n",
        "            amount = 0\n",
        "            if self.demand >= self.inventory:\n",
        "                self.demand -= self.inventory\n",
        "                self.inventory = 0\n",
        "            else:\n",
        "                self.inventory -= self.demand\n",
        "                self.demand = 0\n",
        "        else:\n",
        "            amount -= self.demand\n",
        "            self.demand = 0\n",
        "            self.inventory = min(self.MAX_inventory, self.inventory + amount)\n",
        "            #超過した分をペナルティにする\n",
        "            excess = max(0, self.inventory + amount - self.MAX_inventory)\n",
        "        #print(excess, init_demand-self.demand)\n",
        "        reward -= excess * 1 #/ self.cp\n",
        "        reward += (init_demand-self.demand) * 3 #/ self.cp\n",
        "\n",
        "\n",
        "\n",
        "        #\"\"\"\n",
        "        if self.demand > 0.01: #全部稼働を選択するものの、生産できるタイミングで生産を全くしなくなる時のペナルティ\n",
        "            flag = 0\n",
        "            for i in range(self.n_units):\n",
        "                if speeds[i] > 0 and self.levels[i] != self.n_states-1 and self.maintenance_status[i] == 0:\n",
        "                    flag = 1\n",
        "            if flag == 0:\n",
        "                reward -= self.cp * self.n_units * 2\n",
        "\n",
        "        #在庫と残り需要の更新\n",
        "        for unit_idx in range(self.n_units):\n",
        "            ability = self.get_ability(self.levels[unit_idx])\n",
        "            self.demand = self.update_demand(speed=speeds[unit_idx]*self.MAX_speed*replacements[unit_idx],ability=ability,time=transit_time)\n",
        "            self.inventory, excess = self.update_inventory(speed=speeds[unit_idx]*self.MAX_speed*replacements[unit_idx],ability=ability,time=transit_time)\n",
        "            print(transit_time)\n",
        "            reward -= 0.1 * excess\n",
        "        #\"\"\"\n",
        "\n",
        "\n",
        "        mean = 10\n",
        "        variance = 2  # 標準偏差\n",
        "        mu = np.log(mean**2 / np.sqrt(variance**2 + mean**2))\n",
        "        sigma = np.sqrt(np.log(1 + (variance**2 / mean**2)))\n",
        "\n",
        "        if transit_unit != -1: #状態が推移した時\n",
        "            self.levels[transit_unit] = transit_level\n",
        "            self.remain_interval -= transit_time\n",
        "\n",
        "        else: #状態が推移せず、定期的な意思決定が到着した時\n",
        "            self.remain_interval = self.interval\n",
        "            for unit_idx in range(self.n_units):\n",
        "                self.maintenance_status[unit_idx] = self.update_maintenance_time(unit_idx)\n",
        "\n",
        "            reward -= self.demand * self.co\n",
        "\n",
        "            self.demand = np.random.lognormal(mu, sigma)\n",
        "\n",
        "\n",
        "        #\"\"\"\n",
        "        A = self.levels\n",
        "        B = self.maintenance_status\n",
        "        pairs = list(zip(A, B))\n",
        "        pairs.sort(key=lambda x: (-x[0], x[1]))\n",
        "        A_sorted, B_sorted = zip(*pairs)\n",
        "        self.levels = list(A_sorted)\n",
        "        self.maintenance_status = list(B_sorted)\n",
        "        #\"\"\"\n",
        "\n",
        "\n",
        "        level_ohe, mstatus_ohe = self.one_hot_encode()\n",
        "\n",
        "        #print(f'状態:{self.levels}, 保全状態:{self.maintenance_status}, 在庫:{self.inventory}, 需要:{self.demand}, 残り時間:{self.remain_interval}, 保全行動:{replacements}, {speeds}')\n",
        "        #print(\"#############\")\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        return reward, level_ohe, mstatus_ohe, \\\n",
        "               0, self.demand * 2 /15 - 1, 0, flag\n",
        "        #return reward, level_ohe, mstatus_ohe, \\\n",
        "        #       0, (self.demand-mean)/variance, self.remain_interval * 2 / self.interval - 1, flag\n",
        "\n",
        "\n",
        "    #劣化レベル順にすべき可能性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z_7ruy0iTYPa"
      },
      "outputs": [],
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self,batch_size, interval, beta, GAE_lam):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.time = []\n",
        "        self.batch_size = batch_size\n",
        "        self.interval = interval\n",
        "        self.beta = beta\n",
        "        self.advantage = []\n",
        "        self.lam = GAE_lam\n",
        "\n",
        "    def generate_advantage(self):\n",
        "\n",
        "        \"\"\"\n",
        "        advantage = np.zeros(len(self.rewards),dtype=np.float32)\n",
        "        for t in range(len(self.rewards)-1):\n",
        "            a_t = 0\n",
        "            for k in range(t, len(self.rewards)-1):\n",
        "                a_t += math.exp(-self.beta * self.time[k]) * \\\n",
        "                    (self.rewards[k]+math.exp(-self.beta * (-self.time[k]%self.interval+self.interval))*self.vals[k+1]-self.vals[k])\n",
        "            advantage[t] = a_t\n",
        "        self.advantage = advantage\n",
        "        \"\"\"\n",
        "\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            gamma = 1\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                gamma = math.exp(-self.beta*(self.time[t+1]-self.time[t]))\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + gamma * self.lam * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage,dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "               np.array(self.acts_dsc),\\\n",
        "               np.array(self.acts_cnt),\\\n",
        "               np.array(self.probs),\\\n",
        "               np.array(self.vals),\\\n",
        "               np.array(self.rewards),\\\n",
        "               np.array(self.advantage),\\\n",
        "               batches\n",
        "\n",
        "\n",
        "\n",
        "    def store_memory(self, state, act_dsc, act_cnt, probs, vals, reward, time):\n",
        "        self.states.append(state)\n",
        "        self.acts_dsc.append(act_dsc)\n",
        "        self.acts_cnt.append(act_cnt)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.time.append(time)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.vals = []\n",
        "        self.time = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MJC8xTojTYPa"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, alpha, fc1_dims=64, fc2_dims=64, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.n_units = n_units\n",
        "        self.n_states = n_states\n",
        "        self.MAX_maintenance_time = MAX_maintenance_time\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"actor_torch_ppo\")\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.fc3 = nn.Linear(fc1_dims, fc3_dims)\n",
        "\n",
        "        self.dsc = nn.Linear(fc2_dims, 2 ** n_units) #離散行動\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.Tanh = nn.Tanh()\n",
        "\n",
        "        self.mean = nn.Linear(fc3_dims, n_units)\n",
        "        self.log_std = nn.Linear(fc3_dims, n_units)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "    #離散行動空間を制限するための関数, 返り値はmaskで制限されるところは-inf,されないところは1。返り値はバッチ数*2\n",
        "    def create_dsc_mask(self, state): #state=[s,m,b,d,t], action=[P(replace), P(keep)]\n",
        "        mask = torch.zeros(state.size(0),2**self.n_units)\n",
        "        #保全を選択できる時点にて、保全中のユニットは保全を選択できない\n",
        "        for unit_idx in range(self.n_units):\n",
        "            for a in range(2 ** self.n_units):\n",
        "                action_list = [int(bit) for bit in format(a, f'0{self.n_units}b')] #action_list=[r1,r2,r3,...]\n",
        "                if action_list[unit_idx] == 0:\n",
        "                    #保全の意思決定時点のとき、保全中の場合は保全を選択できない\n",
        "                    #保全の意思決定時点でないとき、保全を選択できない\n",
        "                    mask[(state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)\\\n",
        "                        , a] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        mask[(state[:, 4] == 1) & (state[:, 9] == 1) & \\\n",
        "             (state[:,self.n_states*self.n_units-1 + 1] == 1) & (state[:,self.n_states*self.n_units-1 + (self.MAX_maintenance_time+1) * 1 + 1] == 1) & (state[:,self.n_states*self.n_units-1 + (self.MAX_maintenance_time+1) * 2 + 1] == 1)\n",
        "            ,1:] = torch.tensor(1) #2 ** self.n_units-1\n",
        "            #状態とユニット数により要変更\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "    def create_cnt_mask(self, state):\n",
        "        mask= torch.zeros(state.size(0), self.n_units)\n",
        "        for unit_idx in range(self.n_units):\n",
        "            a = 2**(self.n_units)-1 - 2**(self.n_units-1-unit_idx)\n",
        "\n",
        "            #保全の意思決定ができる時\n",
        "            #mask[(state[:,-1] == 1) & \\\n",
        "            #     ((dist_dsc[:,a] <= 0.0001) |\n",
        "            #      (state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "            #      (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "            mask[((state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "                  (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x2 = F.relu(self.fc2(x))\n",
        "        x3 = F.relu(self.fc3(x))\n",
        "\n",
        "        first_mask = self.create_dsc_mask(state)\n",
        "\n",
        "        #離散行動の分布\n",
        "        dist_dsc = self.dsc(x2)\n",
        "        dist_dsc = dist_dsc.masked_fill(first_mask,-1e5)\n",
        "\n",
        "        dist_dsc = self.softmax(dist_dsc)\n",
        "        #if (state[0,4]==1 and state[0,9]==1 and state[0,14]==1 and state[0,15]==1 and state[0,19]==1 and state[0,23]==1 and state[0,-1]==1):\n",
        "        #    print(state)\n",
        "        #    print(dist_dsc)\n",
        "        #    print(\"&&&&&&&&\")\n",
        "\n",
        "\n",
        "        second_mask = self.create_cnt_mask(state)\n",
        "        dist_dsc = Categorical(dist_dsc)\n",
        "\n",
        "        #連続行動の分布\n",
        "        mean = self.mean(x3)\n",
        "        mean = self.Tanh(mean)\n",
        "        mean = mean.masked_fill(second_mask, -1)\n",
        "        #print(dist_dsc, mean)\n",
        "        #print(state)\n",
        "        #print(dist_dsc)\n",
        "        #mean = torch.clamp(mean,min=-5,max=5)\n",
        "        log_std = self.log_std(x3)\n",
        "        log_std = torch.clamp(log_std,min=-20,max=2)\n",
        "        std = log_std.exp()\n",
        "        std = std.masked_fill(second_mask, 1e-4)\n",
        "        #print(mean)\n",
        "        #print(\"AAAA\")\n",
        "\n",
        "        #print(mean,std)\n",
        "\n",
        "        dist_cnt = torch.distributions.MultivariateNormal(loc=mean,\n",
        "                                                          covariance_matrix = torch.stack([torch.diag(x**2+1e-10) for x in std]))\n",
        "        #dist_cnt = torch.distributions.MultivariateNormal(loc=mean,\n",
        "        #                                                  covariance_matrix = torch.stack([torch.diag(x**2) for x in torch.tensor([[0.03,0.03,0.03]])]) + 1e-10)\n",
        "\n",
        "        return dist_dsc, dist_cnt\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "78j9p-tgTYPb"
      },
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=32, fc2_dims=32, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"critic_torch_ppo\")\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims,fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(fc2_dims,fc3_dims),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(fc2_dims,1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jRw3Bjb7TYPb",
        "outputId": "723446da-8d63-4fec-9a30-dc2c136835c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DMZpQl6gTYPc"
      },
      "outputs": [],
      "source": [
        "test_batch = 0\n",
        "class Agent:\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, beta=0.0005, GAE_lam=0.95, interval=24,\n",
        "                 alpha_actor=0.003, alpha_critic=0.01,\n",
        "                 policy_clip=0.2, batch_size=512*4, n_epochs=4):\n",
        "        self.beta = beta\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.loss_history_detail = []\n",
        "\n",
        "        self.actor_loss_history = []\n",
        "        self.actor_loss_history_detail = []\n",
        "        self.critic_loss_history = []\n",
        "        self.critic_loss_history_detail = []\n",
        "        self.entropy_history = []\n",
        "        self.kl_divergence_history = []\n",
        "\n",
        "        self.actor = ActorNetwork(n_units, n_states, MAX_maintenance_time, input_dims, alpha_actor)\n",
        "        self.critic = CriticNetwork(input_dims, alpha_critic)\n",
        "        self.memory = PPOMemory(batch_size, interval=interval, beta=beta, GAE_lam=GAE_lam)\n",
        "\n",
        "    def remember(self,state,action_dsc,action_cnt,probs,vals,reward, time):\n",
        "        self.memory.store_memory(state,action_dsc,action_cnt,probs,vals,reward, time)\n",
        "\n",
        "    def save_models(self):\n",
        "        print(\"... saving models ...\")\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print(\"... loading models ...\")\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        act_dsc = dist_dsc.sample()\n",
        "        act_cnt = dist_cnt.sample()\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "        log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "\n",
        "\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob, value\n",
        "\n",
        "    def choose_action_max_prob(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "        act_dsc = torch.argmax(dist_dsc.probs)\n",
        "        act_cnt = dist_cnt.mean\n",
        "        value = self.critic(state)\n",
        "\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "        log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob, value\n",
        "\n",
        "    def learn(self):\n",
        "        self.memory.generate_advantage()\n",
        "        actor_loss_sum = 0\n",
        "        critic_loss_sum = 0\n",
        "        entropy_sum = 0\n",
        "        kl_divergence_sum = 0\n",
        "        for _ in range(self.n_epochs):\n",
        "        #for _ in tqdm(range(self.n_epochs), desc=\"Training Progress\"):  # tqdmを用いて進捗表示\n",
        "            \"\"\"\n",
        "            rewards = self.memory.rewards\n",
        "            values = self.memory.vals\n",
        "            times = self.memory.time\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * \\\n",
        "                        (reward_arr[k]+math.exp(-self.beta * (-times[k]%self.interval+self.interval))*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "            \"\"\"\n",
        "            state_arr, act_dsc_arr, act_cnt_arr, old_probs_arr, vals_arr, reward_arr, advantage, batches=self.memory.generate_batches()\n",
        "            values = vals_arr\n",
        "            \"\"\"\n",
        "            values = vals_arr\n",
        "            times = time_arr\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * (reward_arr[k]+self.gamma*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            \"\"\"\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = torch.tensor(values).to(self.actor.device)\n",
        "            start = time.time()\n",
        "            for batch in batches:  # 各バッチの進捗を表示\n",
        "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
        "                log_old_probs = torch.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
        "                acts_dsc = torch.tensor(act_dsc_arr[batch]).to(self.actor.device)\n",
        "                acts_cnt = torch.tensor(act_cnt_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist_dsc, dist_cnt = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "                critic_value = torch.squeeze(critic_value)\n",
        "\n",
        "                log_new_probs = dist_dsc.log_prob(acts_dsc) + dist_cnt.log_prob(acts_cnt)\n",
        "\n",
        "                prob_ratio = log_new_probs.exp()/log_old_probs.exp()\n",
        "                weighted_probs = advantage[batch]*prob_ratio\n",
        "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "                #print(actor_loss)\n",
        "                #print(critic_loss)\n",
        "                #print(\"#####\")\n",
        "                entropy = torch.clamp(dist_dsc.entropy().mean(),min=0) + torch.clamp(dist_cnt.entropy().mean(), min=0.0)\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss + 0.01*entropy\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "                self.loss_history_detail.append(total_loss.item())\n",
        "\n",
        "                actor_loss_sum += actor_loss.item()\n",
        "                critic_loss_sum += critic_loss.item()\n",
        "                entropy_sum += entropy.item()\n",
        "                kl_divergence_sum += torch.distributions.kl_divergence(Categorical(logits=log_old_probs), Categorical(logits=log_new_probs)).mean().item()\n",
        "\n",
        "        print(f'actor loss: {actor_loss_sum}, critic loss: {critic_loss_sum}, entropy: {entropy_sum}, KL divergence: {kl_divergence_sum}')\n",
        "        self.loss_history.append(np.mean(self.loss_history_detail[-self.n_epochs:]))\n",
        "        self.actor_loss_history.append(actor_loss_sum)\n",
        "        self.critic_loss_history.append(critic_loss_sum)\n",
        "        self.entropy_history.append(entropy_sum)\n",
        "        self.kl_divergence_history.append(kl_divergence_sum)\n",
        "            # Update sums\n",
        "        self.actor.scheduler_actor.step()  # 学習率を更新\n",
        "        self.critic.scheduler_critic.step()  # 学習率を更新\n",
        "        self.memory.clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l4PW7F3zTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PkbRyHFHTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7JV5KgBxTYPd"
      },
      "outputs": [],
      "source": [
        "#エージェントの初期化\n",
        "n_units = 2\n",
        "n_states = 5\n",
        "MAX_maintenance_time = 0\n",
        "input_size = n_units * n_states + n_units * (MAX_maintenance_time+1) + 2 #MDPのため[残り時間]と[保全意思決定時]の2つの入力は入れない\n",
        "action_size = 2**n_units  # 行動数は2^3個\n",
        "batch_size = 512*4#512-5120\n",
        "interval = 24\n",
        "alpha_actor = 0.0001\n",
        "alpha_critic = 0.0002\n",
        "n_epochs = 4\n",
        "policy_clip = 0.1\n",
        "beta=0.0005\n",
        "\n",
        "\n",
        "agent = Agent(n_units=n_units,\n",
        "              input_dims=input_size,\n",
        "              n_states=n_states,\n",
        "              MAX_maintenance_time=MAX_maintenance_time,\n",
        "              beta=beta,\n",
        "              interval=interval,\n",
        "              alpha_actor=alpha_actor,\n",
        "              alpha_critic=alpha_critic,\n",
        "              policy_clip=policy_clip,\n",
        "              batch_size=batch_size,\n",
        "              n_epochs=n_epochs)\n",
        "env = Environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "15IhPu1nTYPd",
        "outputId": "9cca2364-39fa-41e0-9b4f-4f1aff11e7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0]\n",
            "-1500\n",
            "[0, 0]\n",
            "-1000\n",
            "[0, 0]\n",
            "-1500\n",
            "稼働継続\n",
            "[0.67139215] [1.48774396]\n",
            "[array([0.02685569]), array([0.05950976])]\n",
            "-100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9cb36cd3f185>:170: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  reward -= self.P_Cost[math.floor(self.levels[0])][math.floor(self.levels[1])]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.float64' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-64a88e38bc6c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mact_cnt_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_cnt_np\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print(act_dsc_list,act_cnt_np)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_ohe_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmstatus_ohe_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minventory_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremain_interval_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dsc_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_cnt_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9cb36cd3f185>\u001b[0m in \u001b[0;36moperation\u001b[0;34m(self, replacements, speeds)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munit_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;31m#現在の状態から次の状態へ推移する時の滞在時間を計算し、一番小さい時間と推移する状態を計算する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0msojurn_time_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweibull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                                 \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_speed\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mspeeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munit_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;31m#print(sojurn_time_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.weibull\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "num_episode = 2\n",
        "best_reward = -np.inf\n",
        "episode_reward_history = []\n",
        "avg_cost = 0\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    episode_reward = 0\n",
        "    operation_time = 0\n",
        "    one_action = 0\n",
        "    two_action = 0\n",
        "    three_action = 0\n",
        "    penalty_action = 0\n",
        "    level_ohe, mstatus_ohe, inventory, demand, remain_interval = env.init_random()\n",
        "    if episode % 100 == 0:\n",
        "        interval_time_episode = time.time()\n",
        "    interval_time_episode = time.time()\n",
        "    for _ in range(1024):#1024*8\n",
        "        state = level_ohe + mstatus_ohe + list([inventory,demand])\n",
        "        act_dsc, act_cnt, log_prob, val = agent.choose_action(state)\n",
        "        act_dsc_list = [int(bit) for bit in format(act_dsc.item(), f'0{env.n_units}b')]\n",
        "        if sum(act_dsc_list) == 2:\n",
        "            one_action += 1\n",
        "        elif sum(act_dsc_list) == 1:\n",
        "            two_action += 1\n",
        "        elif sum(act_dsc_list) == 0:\n",
        "            three_action += 1\n",
        "        act_cnt_np = act_cnt.squeeze().cpu().numpy().copy()\n",
        "        act_cnt_np = act_cnt_np * 0.5 + 0.5\n",
        "        #print(act_dsc_list,act_cnt_np)\n",
        "        reward, level_ohe_next, mstatus_ohe_next, inventory_next, demand_next, remain_interval_next, flag = env.operation(act_dsc_list,act_cnt_np)\n",
        "\n",
        "        episode_reward += reward\n",
        "        penalty_action += flag\n",
        "        #if remain_interval > remain_interval_next:\n",
        "            #operation_time += (remain_interval+1)/2*interval - (remain_interval_next+1)/2*interval\n",
        "        #else:\n",
        "            #operation_time += (remain_interval_next + 1) / 2 * interval\n",
        "        agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob, val, reward, operation_time)\n",
        "        level_ohe = level_ohe_next\n",
        "        mstatus_ohe = mstatus_ohe_next\n",
        "        inventory = inventory_next\n",
        "        demand = demand_next\n",
        "        remain_interval = remain_interval_next\n",
        "    #print(f'{episode}エピソード目の時間：{time.time()-interval_time_episode}')\n",
        "    interval_time_episode = time.time()\n",
        "    agent.learn()\n",
        "\n",
        "    old_agent = Agent(n_units=n_units,\n",
        "                        input_dims=input_size,\n",
        "                        n_states=n_states,\n",
        "                        MAX_maintenance_time=MAX_maintenance_time,\n",
        "                        beta=beta,\n",
        "                        interval=interval,\n",
        "                        alpha_actor=alpha_actor,\n",
        "                        alpha_critic=alpha_critic,\n",
        "                        policy_clip=policy_clip,\n",
        "                        batch_size=batch_size,\n",
        "                        n_epochs=n_epochs)\n",
        "    if episode != 0:\n",
        "        old_agent.load_models()\n",
        "\n",
        "    #if Check_convergence(agent, old_agent, n_units, n_states, MAX_maintenance_time):\n",
        "    #    break\n",
        "\n",
        "\n",
        "\n",
        "    agent.save_models()\n",
        "    print(f'状態{state}, 離散行動：{act_dsc_list}, 連続行動：{act_cnt_np}')\n",
        "    print(f'[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [{env.replace_chance}, {env.failure_keep1}, {env.failure_keep2}, {env.failure_keep3}]')\n",
        "    env.replace_chance = 0\n",
        "    env.failure_keep1 = 0\n",
        "    env.failure_keep2 = 0\n",
        "    env.failure_keep3 = 0\n",
        "    #print(f'{episode}エピソード目の学習時間：{time.time()-interval_time_episode}')\n",
        "    print(f'{episode}エピソード目の累積報酬：{episode_reward}, 一つ保全の回数：{one_action}, 二つ保全の回数：{two_action}, 三つ保全の回数：{three_action}, 違反回数：{penalty_action}')\n",
        "    episode_reward_history.append(episode_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncPGOL29TYPd"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHLLAogeTYPe"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# エージェントの学習\n",
        "# (agent.train()の呼び出しなど)\n",
        "\n",
        "# 学習後のエージェントの評価\n",
        "#evaluate_agent(agent, env, num_episodes=10)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plot_learning_curve(episode_reward_history, title=\"PPO Learning Curve\", label=\"Total reward\")\n",
        "plot_learning_curve(agent.loss_history_detail, title=\"loss curve\", label=\"Total loss (actor loss +  critic loss) \")\n",
        "plot_learning_curve(agent.actor_loss_history, title=\"actor loss curve\", label=\"actor loss\")\n",
        "plot_learning_curve(agent.critic_loss_history, title=\"critic loss curve\", label=\"critic loss\")\n",
        "plot_learning_curve(agent.entropy_history, title=\"entropy curve\", label=\"entropy\")\n",
        "plot_learning_curve(agent.kl_divergence_history, title=\"kl divergence curve\", label=\"kl divergence\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w98R-WQ4TYPe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_encoding(levels, maintenance_status, n_units=2,n_states=5, MAX_maintenance_time=0):\n",
        "    level_ohe = []\n",
        "    mstatus_ohe = []\n",
        "    for unit_idx in range(n_units):\n",
        "        l = [0] * n_states\n",
        "        m = [0] * (MAX_maintenance_time + 1)\n",
        "        l[levels[unit_idx]] = 1\n",
        "        m[maintenance_status[unit_idx]] = 1\n",
        "        level_ohe = level_ohe + l\n",
        "        mstatus_ohe = mstatus_ohe + m\n",
        "    return level_ohe, mstatus_ohe\n",
        "\n",
        "\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        print(action)\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"lightblue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, act_dsc, act_cnt):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    if act_dsc >= 0:\n",
        "        #ax.text(center_x,center_y,f'({r[0]},{r[1]},{r[2]})',ha='center', va='center')\n",
        "        ax.text(center_x, center_y,f'{(round(act_cnt[0],1),round(act_cnt[1],1))}',ha='center', va='center',fontsize=5)\n",
        "\n",
        "def optimal_policy(s1,m1,m2,m3,b,d,t):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 5\n",
        "    for s2 in range(x):\n",
        "        for s3 in range(x):\n",
        "            level_ohe, mstatus_ohe = one_hot_encoding(levels=[s2,s3],maintenance_status=[m2,m3])\n",
        "            state = level_ohe + mstatus_ohe + list([b,d])\n",
        "            act_dsc, act_cnt, log_prob, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            plot_action(ax, s2, s3, act_dsc, act_cnt)\n",
        "    ax.set_xlim(-0.5,x+0.5)\n",
        "    ax.set_ylim(-0.5,x+0.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s2\")\n",
        "    ax.set_ylabel(\"s3\")\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"(s1=\"+str(s1)+\", s2, s3, m1=\"+str(m1)+\", m2=\"+str(m2)+\", m3=\"+str(m3)+\", b=\"+str(b)+\", d=\"+str(d)+\", t=\"+str(t)+\")\")\n",
        "    plt.show()\n",
        "optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=1,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=2,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=3,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=4,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=4,m1=0,m2=0,m3=0,b=0,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=1,m2=1,m3=0,b=1,d=0,t=1)\n",
        "optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=0.5)\n",
        "optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "s1 = 4\n",
        "s2 = 4\n",
        "s3 = 4\n",
        "m1 = 0\n",
        "m2 = 0\n",
        "m3 = 0\n",
        "inventory = 0\n",
        "#demand = 0\n",
        "remain_interval = 1\n",
        "level_ohe, mstatus_ohe = one_hot_encoding(levels=[s1,s2,s3],maintenance_status=[m1,m2,m3])\n",
        "state = level_ohe + mstatus_ohe + list([inventory,demand,remain_interval])\n",
        "#act_dsc, act_cnt, log_prob, val = agent.choose_action_max_prob(state)\n",
        "#act_dsc = act_dsc.item()\n",
        "#act_dsc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3bLWs2RTYPe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}