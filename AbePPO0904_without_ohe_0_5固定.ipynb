{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeTetsuyaR/AbePPO/blob/main/AbePPO0904_without_ohe_0_5%E5%9B%BA%E5%AE%9A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCrwylLHTYPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import cProfile\n",
        "import sys\n",
        "import copy\n",
        "from torch.distributions.categorical import Categorical\n",
        "import math\n",
        "import os\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.colors as mcolors\n",
        "from tqdm import tqdm  # tqdmをインポート\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import gamma, uniform, truncnorm\n",
        "#import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6hr_FE6TYPY"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, n_units=2):\n",
        "        self.n_units = n_units #number of unit\n",
        "        #self.n_states = 5 #number of state\n",
        "        self.inventory = 0\n",
        "        self.demand = 0\n",
        "        self.maintenance_status = [0] * self.n_units\n",
        "        self.interval = 24\n",
        "        self.remain_interval = 24\n",
        "        self.MAX_speed = 10/self.interval\n",
        "        self.MAX_inventory = 0\n",
        "        self.MAX_demand = 15\n",
        "        self.MAX_maintenance_time = 0\n",
        "\n",
        "        self.load_total=1\n",
        "\n",
        "        self.cp = 500#\n",
        "        self.cc = 1800#\n",
        "\n",
        "        self.cps = 0\n",
        "        self.co = 5\n",
        "        self.cs = 500#\n",
        "\n",
        "        self.levels = [0] * self.n_units\n",
        "        #self.flags = [0] * self.n_units\n",
        "        self.shape = 3\n",
        "        self.penalty = 1\n",
        "        self.L = 100#\n",
        "        #self.P_Cost =[[100,110,130,160,2540],\n",
        "                      #[110,120,140,170,2550],\n",
        "                      #[130,140,160,190,2570],\n",
        "                      #[160,170,190,220,2600],\n",
        "                      #[2540,2550,2570,2600,5000]]#convex化\n",
        "\n",
        "        self.P_Cost =[[0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [2500,2500,2500,2500,5000]]#平滑化\n",
        "        self.P_Cost_L = 2500\n",
        "\n",
        "        self.Visit =[[0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0]]\n",
        "        self.cntCount=[0,0]\n",
        "\n",
        "        self.failure_keep1 = 0 #1つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep2 = 0 #2つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep3 = 0 #3つ故障しているのに保全を選択しなかった回数\n",
        "        self.replace_chance = 0 #保全を選択できた回数\n",
        "\n",
        "    def init_random(self):\n",
        "        levels = [0,0]\n",
        "        flags = [0,0]\n",
        "        self.load_total=1\n",
        "        return levels,  flags, self.load_total\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.levels = np.zeros(self.n_units)\n",
        "\n",
        "    def complete_maintenance(self, unit_idx):\n",
        "        self.levels[unit_idx] = 0\n",
        "\n",
        "    def get_ability(self, level): #良品率\n",
        "        if level == 0:\n",
        "            return 1\n",
        "        elif level == 1:\n",
        "            return 0.8\n",
        "        elif level ==2:\n",
        "            return 0.5\n",
        "        elif level == 3:\n",
        "            return 0.1\n",
        "        return (self.n_states - 1 - level) / (self.n_states - 1)\n",
        "\n",
        "    def update_demand(self, speed, ability, time):\n",
        "        if self.demand >= self.inventory:\n",
        "            self.demand -= self.inventory\n",
        "            self.inventory = 0.0\n",
        "        else:\n",
        "            self.inventory -= self.demand\n",
        "            self.demand = 0.0\n",
        "        return max(0, self.demand-self.inventory-ability*speed*time)\n",
        "\n",
        "    def update_inventory(self, speed, ability, time):\n",
        "        if self.demand <= self.inventory + ability * speed * time:\n",
        "            return min(self.MAX_inventory, -self.demand+self.inventory+ability*speed*time), max(0, -self.MAX_inventory-self.demand+self.inventory+ability*speed*time)\n",
        "        else:\n",
        "            return 0.0, 0.0\n",
        "\n",
        "    def get_maintenance_time(self,level):\n",
        "        return 0\n",
        "\n",
        "    def update_maintenance_time(self, unit_idx):\n",
        "        return 0\n",
        "\n",
        "    def one_hot_encode(self):\n",
        "        level_ohe = []\n",
        "        #mstatus_ohe = []\n",
        "        for unit_idx in range(self.n_units):\n",
        "            l = [0] * self.n_states\n",
        "            #m = [0] * (self.MAX_maintenance_time + 1)\n",
        "            l[min(math.floor(self.levels[unit_idx]),self.n_states-1)] = 1\n",
        "            #m[self.maintenance_status[unit_idx]] = 1\n",
        "            level_ohe = level_ohe + l\n",
        "            #mstatus_ohe = mstatus_ohe + m\n",
        "        return level_ohe #mstatus_oheは削除\n",
        "\n",
        "\n",
        "\n",
        "    def operation(self, replacements, load_rate):\n",
        "        #print(self.levels,\"levels_before\")\n",
        "        reward = 0\n",
        "        #print(load_rate)\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        #保全の意思決定\n",
        "        #print(replacements)\n",
        "\n",
        "        if replacements==[1,1]: #稼働継続\n",
        "\n",
        "          #reward -= self.P_Cost[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] #rewardを最初に計算\n",
        "          if self.levels[0]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          if self.levels[1]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          #print(reward)\n",
        "          # パラメータの設定\n",
        "          scales=[0,0]\n",
        "          shape0 = 0.69  # ガンマ分布のパラメータ v1 用 0.69 100倍にしてみる\n",
        "          shape1 = 0.69   # ガンマ分布のパラメータ v2 用\n",
        "          tau = 0.5  # ケンドールの順位相関係数\n",
        "\n",
        "          theta = 1 / (1 - tau)\n",
        "\n",
        "          #if load_rate<0:##\n",
        "            #load_rate=0\n",
        "          #if load_rate>1:\n",
        "            #load_rate=1\n",
        "          loads=[0,0]\n",
        "          loads[0]=load_total*load_rate\n",
        "          loads[1]=load_total*(1-load_rate)\n",
        "\n",
        "          #print(speeds, \"speeds\")\n",
        "          #尺度パラメータ計算\n",
        "          for i in range(self.n_units):\n",
        "            scales[i]=6.491*(loads[i]**2)+0.726\n",
        "            #scales[i]=6.491*(speeds[i]**0.5)+0.726\n",
        "            #scales[i]=scales[i] #1/10000倍にしてみる\n",
        "\n",
        "\n",
        "          # 一様分布から独立にサンプリング\n",
        "          u = uniform.rvs(size=1)\n",
        "          v = uniform.rvs(size=1)\n",
        "          #print(\"u,v:\",u,v)\n",
        "          # ガンベルコピュラの逆関数を適用\n",
        "          x = (-np.log(u.item())) ** theta#arrayをfloatに\n",
        "          y = (-np.log(v.item())) ** theta\n",
        "          #print(\"x,y:\",x,y)\n",
        "\n",
        "          t = (x + y) ** (1/theta)\n",
        "          #print(\"t:\",t)\n",
        "\n",
        "          u_new = np.exp(-t * (x / (x + y)))\n",
        "          v_new = np.exp(-t * (y / (x + y)))\n",
        "\n",
        "          # ガンマ分布に変換\n",
        "          v1 = gamma.ppf(u_new, shape0, scale=scales[0])\n",
        "          v2 = gamma.ppf(v_new, shape1, scale=scales[1])\n",
        "          #print(v1,v2)\n",
        "          #v1 = gamma.rvs(shape0, scale=scales[0])\n",
        "          #v2 = gamma.rvs(shape1, scale=scales[1])\n",
        "\n",
        "          #print(\"稼働継続\")\n",
        "          #print(v1,v2, \"劣化増分\")\n",
        "          self.levels[0]+=v1\n",
        "          self.levels[1]+=v2\n",
        "          #print(self.levels, \"劣化\")\n",
        "\n",
        "\n",
        "\n",
        "        elif replacements==[0,1]: #1個取替(順張り) #0907\n",
        "          reward -= self.cs\n",
        "\n",
        "          if self.levels[0]>self.levels[1]:\n",
        "            unit_replaced=0\n",
        "          else:\n",
        "            unit_replaced=1\n",
        "\n",
        "          if self.levels[unit_replaced]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[unit_replaced]=0\n",
        "\n",
        "        elif replacements==[1,0]:#1個取替(逆張り)\n",
        "          reward -= self.cs\n",
        "\n",
        "          if self.levels[0]>self.levels[1]:\n",
        "            unit_replaced=1\n",
        "          else:\n",
        "            unit_replaced=0\n",
        "\n",
        "          if self.levels[unit_replaced]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[unit_replaced]=0\n",
        "\n",
        "        else: #両方取替\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          if self.levels[1]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels=[0,0]\n",
        "\n",
        "        #print(self.levels)\n",
        "        #print(reward)\n",
        "\n",
        "        #level_ohe= self.one_hot_encode()\n",
        "\n",
        "        #print(f'状態:{self.levels}, 保全状態:{self.maintenance_status}, 在庫:{self.inventory}, 需要:{self.demand}, 残り時間:{self.remain_interval}, 保全行動:{replacements}, {speeds}')\n",
        "        #print(\"#############\")\n",
        "\n",
        "        flags = [0,0]#故障フラグ\n",
        "        if self.levels[0] >= self.L:\n",
        "          flags[0] = 1\n",
        "        if self.levels[1] >= self.L:\n",
        "          flags[1] = 1\n",
        "\n",
        "        #切断正規分布\n",
        "        dist_N = truncnorm(0, 2, loc=1, scale=1)\n",
        "        self.load_total=float(dist_N.rvs(1))\n",
        "        #print(self.load_total)\n",
        "\n",
        "        #self.Visit[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] += 1\n",
        "        #levels_after=[]\n",
        "        #for i in range(self.n_units):\n",
        "          #l=list([self.levels[i][0]])\n",
        "          #levels_after =levels_after+l\n",
        "        #print(self.levels,\"levels_after\")\n",
        "\n",
        "        return reward, self.levels, flags, self.load_total\n",
        "\n",
        "        #return reward, level_ohe, mstatus_ohe, \\\n",
        "        #       0, (self.demand-mean)/variance, self.remain_interval * 2 / self.interval - 1, flag\n",
        "\n",
        "\n",
        "    #劣化レベル順にすべき可能性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_7ruy0iTYPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c7e1fa2b-7f22-4c27-f4e5-cb08a0d77c6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    def generate_advantage(self):\\n        advantage = []\\n        gae = 0\\n        gamma = math.exp(-self.beta)\\n        lambd = 0.0\\n        for t in reversed(range(len(self.rewards))):\\n            if t == len(self.rewards) - 1:\\n                delta = self.rewards[t] - self.vals[t]\\n            else:\\n                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\\n            gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\\n            advantage.insert(0, gae)\\n        self.advantage = np.array(advantage, dtype=np.float32)\\n\\n    def generate_advantage(self):\\n\\n        advantage = []\\n        gae = 0\\n        for t in reversed(range(len(self.rewards))):\\n            gamma = 1\\n            if t == len(self.rewards) - 1:\\n                delta = self.rewards[t] - self.vals[t]\\n            else:\\n                gamma = math.exp(-self.beta*1) #修正\\n                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\\n            gae = delta + gamma * gamma * gae\\n            advantage.insert(0, gae)\\n        self.advantage = np.array(advantage,dtype=np.float32)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self,batch_size, interval, beta, GAE_lam):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.vals = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.time = []\n",
        "        self.batch_size = batch_size\n",
        "        self.interval = interval\n",
        "        self.beta = beta\n",
        "        self.advantage = []\n",
        "        self.lam = GAE_lam\n",
        "\n",
        "\n",
        "\n",
        "    def generate_advantage(self):\n",
        "        advantage = []\n",
        "        adv = 0\n",
        "        #gamma = math.exp(-self.beta)\n",
        "        #lambd = 0.0\n",
        "        beta=0.99\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            if t == len(self.rewards) - 1:\n",
        "                adv = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                adv = self.rewards[t] + beta * self.vals[t+1] - self.vals[t]\n",
        "            #gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\n",
        "            advantage.insert(0, adv)\n",
        "        self.advantage = np.array(advantage, dtype=np.float32)\n",
        "\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "               np.array(self.acts_dsc),\\\n",
        "               np.array(self.acts_cnt),\\\n",
        "               np.array(self.probs_dsc),\\\n",
        "               np.array(self.probs_cnt),\\\n",
        "               np.array(self.vals),\\\n",
        "               np.array(self.rewards),\\\n",
        "               np.array(self.advantage),\\\n",
        "               batches\n",
        "\n",
        "\n",
        "\n",
        "    def store_memory(self, state, act_dsc, act_cnt, probs_dsc, probs_cnt, vals, reward, time):\n",
        "        self.states.append(state)\n",
        "        self.acts_dsc.append(act_dsc)\n",
        "        self.acts_cnt.append(act_cnt)\n",
        "        self.probs_dsc.append(probs_dsc)\n",
        "        self.probs_cnt.append(probs_cnt)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.time.append(time)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.vals = []\n",
        "        self.time = []\n",
        "\n",
        "\"\"\"\n",
        "    def generate_advantage(self):\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        gamma = math.exp(-self.beta)\n",
        "        lambd = 0.0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage, dtype=np.float32)\n",
        "\n",
        "    def generate_advantage(self):\n",
        "\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            gamma = 1\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                gamma = math.exp(-self.beta*1) #修正\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + gamma * gamma * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage,dtype=np.float32)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJC8xTojTYPa"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, alpha, fc1_dims=64, fc2_dims=64, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.n_units = n_units\n",
        "        self.n_states = n_states\n",
        "        self.MAX_maintenance_time = MAX_maintenance_time\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"actor_torch_ppo\")\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.fc3 = nn.Linear(fc1_dims, fc3_dims)\n",
        "\n",
        "        self.dsc = nn.Linear(fc2_dims, 2 ** n_units) #離散行動\n",
        "        # 以下に初期化コードを追加\n",
        "        self.init_dsc_weights()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.Tanh = nn.Tanh()\n",
        "\n",
        "        self.mean = nn.Linear(fc3_dims, n_units-1)\n",
        "        self.log_std = nn.Linear(fc3_dims, n_units-1)\n",
        "\n",
        "        # mean レイヤーの初期化\n",
        "        self.init_mean_weights()\n",
        "        # std レイヤーの初期化\n",
        "        self.init_std_weights()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "\n",
        "        #0824 optimizer追加\n",
        "        self.optimizer_discrete = optim.Adam([\n",
        "            {'params':self.fc1.parameters()},\n",
        "            {'params':self.fc2.parameters()},\n",
        "            {'params':self.dsc.parameters()},\n",
        "        ], lr=alpha)\n",
        "\n",
        "        self.optimizer_continuous = optim.Adam([\n",
        "            {'params':self.fc1.parameters()},\n",
        "            {'params':self.fc3.parameters()},\n",
        "            {'params':self.mean.parameters()},\n",
        "            {'params':self.log_std.parameters()},\n",
        "        ], lr=alpha)#0908\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "    def init_dsc_weights(self):\n",
        "        # ここで特定の出力確率を設定するための重みとバイアスを設定\n",
        "        with torch.no_grad():\n",
        "            # すべての出力がほぼ等しくなるように設定\n",
        "            self.dsc.weight.fill_(0.0)\n",
        "            # 特定の確率分布に調整\n",
        "            self.dsc.bias.data = torch.log(torch.tensor([0.01, 0.01, 0.01, 0.97]))  # logを取るのがポイント\n",
        "\n",
        "    def init_mean_weights(self):\n",
        "        # mean レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.mean.weight.fill_(0.0)\n",
        "            self.mean.bias.fill_(0.0)  # ここで固定出力0.5を設定#0に変更\n",
        "\n",
        "    def init_std_weights(self):\n",
        "        # std レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.log_std.weight.fill_(0.0)\n",
        "            self.log_std.bias.fill_(0.0)  # ここで固定出力0.7を設定\n",
        "\n",
        "    #離散行動空間を制限するための関数, 返り値はmaskで制限されるところは-inf,されないところは1。返り値はバッチ数*2\n",
        "    def create_dsc_mask(self, state): #state=[s,m,b,d,t], action=[P(replace), P(keep)]\n",
        "        mask = torch.zeros(state.size(0),2**self.n_units)\n",
        "        #保全を選択できる時点にて、保全中のユニットは保全を選択できない\n",
        "        for unit_idx in range(self.n_units):\n",
        "            for a in range(2 ** self.n_units):\n",
        "                action_list = [int(bit) for bit in format(a, f'0{self.n_units}b')] #action_list=[r1,r2,r3,...]\n",
        "\n",
        "                #エラーのため省略\n",
        "                #if action_list[unit_idx] == 0:\n",
        "                    #保全の意思決定時点のとき、保全中の場合は保全を選択できない\n",
        "                    #保全の意思決定時点でないとき、保全を選択できない\n",
        "                    #mask[(state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)\\\n",
        "                        #, a] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        mask[(state[:, 4] == 1) & (state[:, 9] == 1) & \\\n",
        "             (state[:,self.n_states*self.n_units-1 + 1] == 1)\n",
        "            ,1:] = torch.tensor(1) #2 ** self.n_units-1\n",
        "            #状態とユニット数により要変更\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "    def create_cnt_mask(self, state):\n",
        "        mask= torch.zeros(state.size(0), self.n_units)\n",
        "        for unit_idx in range(self.n_units):\n",
        "            a = 2**(self.n_units)-1 - 2**(self.n_units-1-unit_idx)\n",
        "\n",
        "            #エラーのため省略\n",
        "            #保全の意思決定ができる時\n",
        "            mask[(state[:,-1] == 1) & \\\n",
        "                 ((dist_dsc[:,a] <= 0.0001) |\n",
        "                  (state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "                  (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "            mask[((state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "                  (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        state_rev = state.clone()#0907\n",
        "        if state[0][0]>state[0][1]:\n",
        "          state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "          state_rev[:, [2, 3]] = state_rev[:, [3, 2]] #flag\n",
        "\n",
        "        #print(state[0,-1].item())\n",
        "        x = F.relu(self.fc1(state_rev))\n",
        "        x2 = F.relu(self.fc2(x))\n",
        "        x3 = F.relu(self.fc3(x))\n",
        "\n",
        "        #first_mask = self.create_dsc_mask(state)\n",
        "\n",
        "        #離散行動の分布\n",
        "        dist_dsc = self.dsc(x2)\n",
        "        #dist_dsc = dist_dsc.masked_fill(first_mask,-1e5)\n",
        "\n",
        "        dist_dsc = self.softmax(dist_dsc)\n",
        "        #if (state[0,4]==1 and state[0,9]==1 and state[0,14]==1 and state[0,15]==1 and state[0,19]==1 and state[0,23]==1 and state[0,-1]==1):\n",
        "        #print(state)\n",
        "        #    print(dist_dsc)\n",
        "        #    print(\"&&&&&&&&\")\n",
        "\n",
        "\n",
        "        #second_mask = self.create_cnt_mask(state)\n",
        "        dist_dsc = Categorical(dist_dsc)\n",
        "\n",
        "        #連続行動の分布\n",
        "        mean = self.mean(x3)\n",
        "        #mean = self.softmax(mean)\n",
        "        #mean = self.Tanh(mean)/2+0.5#[0,1]に補正\n",
        "        mean = self.Tanh(mean)\n",
        "\n",
        "        #load_max=min(state[0,-1].item(),1) #operationへ移動\n",
        "        #load_min=max(state[0,-1].item() -1,0)\n",
        "        #mean=load_min + (load_max-load_min)*mean\n",
        "\n",
        "        #mean = mean.masked_fill(second_mask, -1) #セカンドマスク\n",
        "\n",
        "        #print(dist_dsc, mean)\n",
        "        #print(state)\n",
        "        #print(dist_dsc)\n",
        "        #mean = torch.clamp(mean,min=-5,max=5)\n",
        "        log_std = self.log_std(x3)\n",
        "        log_std = torch.clamp(log_std,min=-20,max=2)\n",
        "        std = log_std.exp()\n",
        "        #std = std.masked_fill(second_mask, 1e-4) #セカンドマスク\n",
        "        #print(mean)\n",
        "        #print(\"AAAA\")\n",
        "\n",
        "        #print(mean,std)\n",
        "\n",
        "        #dist_cnt = torch.distributions.MultivariateNormal(loc=mean, covariance_matrix = torch.stack([torch.diag(x**2+1e-10) for x in std]))\n",
        "        #mean=state[0,-1].item()/2#試験\n",
        "\n",
        "        variance = std ** 2\n",
        "        #print(variance)\n",
        "        #if variance > mean*(1-mean)/2:#補正\n",
        "            #variance = mean*(1-mean)/2\n",
        "        variance = torch.min(variance, mean*(1-mean)/2)\n",
        "\n",
        "        # alpha と beta の計算式を安全に実施\n",
        "        epsilon = 1e-6  # ゼロ除算を防ぐための小さな値\n",
        "        mean_clamped = mean.clamp(min=epsilon, max=1-epsilon)  # mean を [epsilon, 1-epsilon] でクランプ\n",
        "        variance_clamped = variance.clamp(min=epsilon)  # variance を epsilon 以上でクランプ\n",
        "\n",
        "        alpha = ((1 - mean_clamped) / variance_clamped - 1 / mean_clamped) * (mean_clamped ** 2)\n",
        "        beta = alpha * (1 / mean_clamped - 1)\n",
        "        #print(\"After:\", mean)\n",
        "        # これらの値が正であることを保証\n",
        "        alpha = torch.max(alpha, torch.tensor(epsilon))\n",
        "        beta = torch.max(beta, torch.tensor(epsilon))\n",
        "\n",
        "\n",
        "        #dist_cnt = torch.distributions.Beta(alpha, beta) #ベータ分布にしてみる\n",
        "        #print(state[0,-1].item(),mean)\n",
        "        #print(state[0][0],state[0][1])\n",
        "        if state[0][0]<state[0][1]:\n",
        "          dist_cnt = torch.distributions.Normal(loc=mean, scale=std) #1次元化のため\n",
        "        #elif state[0][0]>state[0][1]:\n",
        "        else:\n",
        "          #revised_probs = dist_dsc.probs.clone()\n",
        "          #revised_probs[:, [1, 2]] = revised_probs[:, [2, 1]]  #0 要素の交換\n",
        "          #dist_dsc = torch.nn.ParameterDict({'probs': torch.nn.Parameter(revised_probs)})\n",
        "          dist_cnt = torch.distributions.Normal(loc=-mean, scale=std)\n",
        "        #else:\n",
        "          #dist_cnt = torch.distributions.Normal(loc=0.0, scale=std)\n",
        "\n",
        "        return dist_dsc, dist_cnt\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78j9p-tgTYPb"
      },
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=32, fc2_dims=32, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"critic_torch_ppo\")\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims,fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(fc2_dims,fc3_dims),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(fc2_dims,1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "        # 重みの初期化\n",
        "        #self.init_weights()\n",
        "\n",
        "    #def init_weights(self):\n",
        "        #with torch.no_grad():\n",
        "            #self.mean.weight.fill_(0.0)\n",
        "            #self.mean.bias.fill_(0.0)  # ここで固定出力0.5を設定#0に変更\n",
        "\n",
        "        #for layer in self.critic:\n",
        "            #if isinstance(layer, nn.Linear):\n",
        "                #torch.nn.init.xavier_uniform_(layer.weight)  # Xavier初期化を適用\n",
        "                #torch.nn.init.constant_(layer.bias, -13000)  # バイアスを14000で初期化\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        state_rev = state.clone()#0907\n",
        "        if state[0][0]>state[0][1]:\n",
        "          state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "          state_rev[:, [2, 3]] = state_rev[:, [3, 2]]#flag\n",
        "        value = self.critic(state_rev)#0908\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jRw3Bjb7TYPb",
        "outputId": "59c40385-6f89-4731-ebec-48a03edef94c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMZpQl6gTYPc"
      },
      "outputs": [],
      "source": [
        "test_batch = 0\n",
        "class Agent:\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, beta=0.0005, GAE_lam=0.00, interval=24,\n",
        "                 alpha_actor=0.03, alpha_critic=0.01,\n",
        "                 policy_clip=0.2, batch_size=512*4, n_epochs=4):\n",
        "        self.beta = beta\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.loss_history_detail = []\n",
        "\n",
        "        self.actor_loss_history = []\n",
        "        self.actor_loss_history_detail = []\n",
        "        self.critic_loss_history = []\n",
        "        self.critic_loss_history_detail = []\n",
        "        self.entropy_history = []\n",
        "        self.kl_divergence_history = []\n",
        "\n",
        "        self.actor = ActorNetwork(n_units, n_states, MAX_maintenance_time, input_dims, alpha_actor)\n",
        "        self.critic = CriticNetwork(input_dims, alpha_critic)\n",
        "        self.memory = PPOMemory(batch_size, interval=interval, beta=beta, GAE_lam=GAE_lam)\n",
        "\n",
        "    def remember(self,state,action_dsc,action_cnt,probs_dsc,probs_cnt,vals,reward, time):\n",
        "        self.memory.store_memory(state,action_dsc,action_cnt,probs_dsc,probs_cnt,vals,reward, time)\n",
        "\n",
        "    def save_models(self):\n",
        "        print(\"... saving models ...\")\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print(\"... loading models ...\")\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "\n",
        "        #state_rev = state.clone()\n",
        "        #state_rev[:, [0, 1]] = state_rev[:, [1, 0]]\n",
        "        #value = (self.critic(state)+self.critic(state_rev))/2\n",
        "        value = self.critic(state)#0907\n",
        "\n",
        "        act_dsc = dist_dsc.sample()\n",
        "\n",
        "\n",
        "        act_cnt = dist_cnt.sample()\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "\n",
        "        if act_dsc.item() == 3:\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "          #print(log_prob_cnt, \"log_prob_cnt\")\n",
        "        else:\n",
        "          #log_prob_cnt = 0 #dist_cntを参照しないことの補正\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "\n",
        "\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def choose_action_max_prob(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "        print(\"state:\", state, \"dist_dsc.probs:\", dist_dsc.probs, \"dist_cnt.mean:\",dist_cnt.mean,\"dist_cnt.scale\", dist_cnt.scale)\n",
        "        act_dsc = torch.argmax(dist_dsc.probs)\n",
        "        act_cnt = dist_cnt.mean\n",
        "        #print(act_dsc, \":act_dsc\")\n",
        "        #print(act_cnt, \":act_cnt\")\n",
        "\n",
        "        value = self.critic(state)\n",
        "\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "        log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def learn(self,episode, threshold):\n",
        "        self.memory.generate_advantage()\n",
        "        actor_loss_sum = 0\n",
        "        critic_loss_sum = 0\n",
        "        entropy_sum = 0\n",
        "        kl_divergence_sum = 0\n",
        "        for _ in range(self.n_epochs):\n",
        "        #for _ in tqdm(range(self.n_epochs), desc=\"Training Progress\"):  # tqdmを用いて進捗表示\n",
        "            \"\"\"\n",
        "            rewards = self.memory.rewards\n",
        "            values = self.memory.vals\n",
        "            times = self.memory.time\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * \\\n",
        "                        (reward_arr[k]+math.exp(-self.beta * (-times[k]%self.interval+self.interval))*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "            \"\"\"\n",
        "            state_arr, act_dsc_arr, act_cnt_arr, old_probs_dsc_arr, old_probs_cnt_arr, vals_arr, reward_arr, advantage, batches=self.memory.generate_batches()\n",
        "            values = vals_arr\n",
        "            \"\"\"\n",
        "            values = vals_arr\n",
        "            times = time_arr\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * (reward_arr[k]+self.gamma*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            \"\"\"\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = torch.tensor(values).to(self.actor.device)\n",
        "            start = time.time()\n",
        "            for batch in batches:  # 各バッチの進捗を表示\n",
        "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
        "                log_old_probs_dsc = torch.tensor(old_probs_dsc_arr[batch]).to(self.actor.device)\n",
        "                log_old_probs_cnt = torch.tensor(old_probs_cnt_arr[batch]).to(self.actor.device)\n",
        "                acts_dsc = torch.tensor(act_dsc_arr[batch]).to(self.actor.device)\n",
        "                acts_cnt = torch.tensor(act_cnt_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist_dsc, dist_cnt = self.actor(states)\n",
        "\n",
        "                critic_value = self.critic(states)#0907\n",
        "                critic_value = torch.squeeze(critic_value)\n",
        "\n",
        "                log_new_probs_dsc = dist_dsc.log_prob(acts_dsc) #pi_new\n",
        "                log_new_probs_cnt = dist_cnt.log_prob(acts_cnt)\n",
        "\n",
        "                #log_new_probs = dist_dsc.log_prob(acts_dsc) + dist_cnt.log_prob(acts_cnt)\n",
        "                #print(\"log_old_probs_cnt.exp():\",log_old_probs_cnt.exp())\n",
        "                #print(\"log_new_probs_cnt.exp():\",log_new_probs_cnt.exp())\n",
        "                #print(\"log_old_probs_cnt:\",log_old_probs_cnt)\n",
        "                #print(\"log_new_probs_cnt\",log_new_probs_cnt)\n",
        "\n",
        "                prob_ratio_dsc = log_new_probs_dsc.exp()/log_old_probs_dsc.exp() #確率比\n",
        "                prob_ratio_cnt = log_new_probs_cnt.exp()/log_old_probs_cnt.exp()\n",
        "                #print(\"prob_ratio_dsc:\",prob_ratio_dsc)\n",
        "                #print(\"prob_ratio_cnt:\",prob_ratio_cnt)\n",
        "\n",
        "\n",
        "                weighted_probs_dsc = advantage[batch]*prob_ratio_dsc\n",
        "                weighted_clipped_probs_dsc = torch.clamp(prob_ratio_dsc, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss_dsc = -torch.min(weighted_probs_dsc, weighted_clipped_probs_dsc).mean()\n",
        "\n",
        "                weighted_probs_cnt = advantage[batch]*prob_ratio_cnt\n",
        "                weighted_clipped_probs_cnt = torch.clamp(prob_ratio_cnt, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss_cnt = -torch.min(weighted_probs_cnt, weighted_clipped_probs_cnt).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = F.mse_loss(returns.float(),critic_value.float())\n",
        "                #critic_loss = critic_loss.float()\n",
        "                #print(actor_loss)\n",
        "                #print(critic_loss)\n",
        "                #print(\"#####\")\n",
        "                entropy_dsc = dist_dsc.entropy().sum(dim=0).mean()\n",
        "                entropy_cnt = dist_cnt.entropy().sum(dim=0).mean()\n",
        "                entropy = torch.clamp(entropy_dsc,min=0) + torch.clamp(entropy_cnt,min=0)\n",
        "                #entropy = torch.clamp(dist_dsc.entropy().mean(),min=0) + torch.clamp(dist_cnt.entropy().mean(), min=0.0)\n",
        "\n",
        "                total_loss_dsc = actor_loss_dsc #+ 0.01 * entoropy_dsc\n",
        "                total_loss_cnt = actor_loss_cnt #+ 0.01 * entoropy_cnt\n",
        "                total_loss = total_loss_dsc + total_loss_cnt + 0.5*critic_loss #+ 0.01*entropy\n",
        "\n",
        "                if episode >= threshold:\n",
        "                  print(\"FLAG\")\n",
        "                  self.actor.optimizer_discrete.zero_grad()\n",
        "                  total_loss_dsc.backward(retain_graph=True)\n",
        "                  self.actor.optimizer_discrete.step()\n",
        "\n",
        "                #if episode >= threshold:\n",
        "                  #print(\"FLAG\")\n",
        "                  #self.actor.optimizer_continuous.zero_grad()\n",
        "                  #total_loss_cnt.backward(retain_graph=True)\n",
        "                  #self.actor.optimizer_continuous.step()\n",
        "\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "                self.loss_history_detail.append(total_loss.item())\n",
        "\n",
        "                actor_loss_sum += actor_loss_dsc.item()+actor_loss_cnt.item()\n",
        "                critic_loss_sum += critic_loss.item()\n",
        "                entropy_sum += entropy.item()\n",
        "                kl_divergence_sum += torch.distributions.kl_divergence(Categorical(logits=log_old_probs_dsc+log_old_probs_cnt), Categorical(logits=log_new_probs_dsc+log_new_probs_cnt)).mean().item()\n",
        "\n",
        "                print(\"advantage[batch].size(),advantage[batch]:\",advantage[batch].size(),advantage[batch])\n",
        "\n",
        "        print(f'actor loss: {actor_loss_sum}, critic loss: {critic_loss_sum}, entropy: {entropy_sum}, KL divergence: {kl_divergence_sum}')\n",
        "        self.loss_history.append(np.mean(self.loss_history_detail[-self.n_epochs:]))\n",
        "        self.actor_loss_history.append(actor_loss_sum)\n",
        "        self.critic_loss_history.append(critic_loss_sum)\n",
        "        self.entropy_history.append(entropy_sum)\n",
        "        self.kl_divergence_history.append(kl_divergence_sum)\n",
        "            # Update sums\n",
        "        #self.actor.scheduler_actor.step()  # 学習率を更新\n",
        "        #self.critic.scheduler_critic.step()  # 学習率を更新\n",
        "        self.memory.clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PW7F3zTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkbRyHFHTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JV5KgBxTYPd"
      },
      "outputs": [],
      "source": [
        "#エージェントの初期化\n",
        "n_units = 2\n",
        "n_states = 5\n",
        "MAX_maintenance_time = 0\n",
        "#input_size = n_units * n_states + n_units * (MAX_maintenance_time) + 2 #MDPのため[残り時間]と[保全意思決定時]の2つの入力は入れない\n",
        "input_size = n_units*2 + 1 #flagを追加\n",
        "action_size = 2**n_units  # 行動数は2^3個\n",
        "batch_size = 512*4#512-5120\n",
        "interval = 24\n",
        "alpha_actor = 0.00003#ここを変更する0.003\n",
        "alpha_critic = 0.1#ここを変更する0.1\n",
        "n_epochs = 4\n",
        "policy_clip = 0.1\n",
        "beta=0.0005\n",
        "\n",
        "\n",
        "agent = Agent(n_units=n_units,\n",
        "              input_dims=input_size,\n",
        "              n_states=n_states,\n",
        "              MAX_maintenance_time=MAX_maintenance_time,\n",
        "              beta=beta,\n",
        "              interval=interval,\n",
        "              alpha_actor=alpha_actor,\n",
        "              alpha_critic=alpha_critic,\n",
        "              policy_clip=policy_clip,\n",
        "              batch_size=batch_size,\n",
        "              n_epochs=n_epochs)\n",
        "env = Environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation0=[0,0,0,0,1]\n",
        "state0 = torch.tensor(np.array([observation0]),dtype=torch.float).to(agent.actor.device)\n",
        "dist_dsc0, dist_cnt0 = agent.actor(state0)\n",
        "value0 = agent.critic(state0)\n",
        "print(\"state:\", state0, \"dist_dsc.probs:\", dist_dsc0.probs, \"dist_cnt.mean:\",dist_cnt0.mean,\"dist_cnt.scale\", dist_cnt0.scale,\"value0:\",value0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV6_pfkGblJK",
        "outputId": "deaf2f2f-4eb4-4640-9ebe-7ef437aee83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0100, 0.0100, 0.0100, 0.9700]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>) value0: tensor([[0.0222]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15IhPu1nTYPd",
        "outputId": "e61ded07-2d34-4415-dc83-262eb86adfb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1ec0023f76a2>:242: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.load_total=float(dist_N.rvs(1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03, -5.0002e+03, -5.5677e-02,  ..., -4.9999e+03,\n",
            "        -5.0001e+03, -5.0001e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.9874e-03, -5.0000e+03, -2.5005e+03,  ..., -5.0000e+03,\n",
            "        -5.0002e+03, -4.9999e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03, -2.5002e+03, -5.0000e+03,  ..., -6.4845e-02,\n",
            "        -5.0003e+03, -5.0003e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2500.0884, -2500.1072, -4999.9312,  ..., -4999.6445, -4999.9131,\n",
            "        -4999.9526])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.9800e-01, -5.0000e+03, -5.0001e+03,  ..., -5.0000e+03,\n",
            "        -5.0001e+03, -5.0001e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0007e+03, -4.9999e+03, -4.0995e+03,  ..., -2.4415e-02,\n",
            "        -5.0001e+03, -4.9998e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0527, -2500.1648, -2499.9299,  ..., -2500.0291, -4999.9585,\n",
            "        -2500.4121])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2.5014e+03, -5.0000e+03, -5.0001e+03,  ..., -2.5000e+03,\n",
            "        -5.0002e+03,  6.3056e-02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03, -3.0240e-03, -4.9998e+03,  ..., -5.0000e+03,\n",
            "        -1.5589e-02, -4.9986e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9536, -2499.9541, -4999.9863,  ..., -5000.2715, -5000.1855,\n",
            "        -4999.9102])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9746, -5000.0015, -5000.0127,  ..., -5000.0557, -5000.0176,\n",
            "        -5000.0068])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2500.0383, -2500.3870, -2297.9453,  ..., -5000.2422, -2500.0698,\n",
            "        -4098.9912])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2500.0557, -5000.0801, -2500.9973,  ..., -5000.1006, -2500.0203,\n",
            "        -4999.9946])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.6813e-02, -2.5000e+03, -1.1207e-01,  ..., -5.5853e-01,\n",
            "        -2.5001e+03, -4.9999e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0024, -2499.9514, -4999.9136,  ..., -4999.9766, -5000.2324,\n",
            "        -2502.2368])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03, -2.5000e+03,  3.1244e-03,  ..., -4.9998e+03,\n",
            "        -6.7101e-01, -5.0001e+03])\n",
            "actor loss: 116464.93910493053, critic loss: 180149545.0, entropy: 55174.801513671875, KL divergence: 0.08943432190556123\n",
            "... saving models ...\n",
            "状態[10.23246063060109, 8.221242881834367, 0, 0, 2.8430127815341466], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "0エピソード目の累積報酬：-282953.0525161782, 一つ保全の回数：7957, 二つ保全の回数：138, 三つ保全の回数：97, 違反回数：0,episode_reward_history：[-282953.0525161782]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1921.5387, -5025.1387,   -28.8875,  ..., -2556.0994, -5141.9678,\n",
            "        -2542.3838])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5025.2842, -5119.9746,    -6.1762,  ..., -4978.8906, -2512.8967,\n",
            "        -4990.0952])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4992.3115, -2530.4739,   -73.9755,  ..., -2493.1387, -5017.3262,\n",
            "        -2506.4702])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -14.8714, -2504.4070, -2511.3894,  ...,   -11.5009, -5000.9858,\n",
            "        -5000.4150])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2509.7651, -2501.3911,   -38.1455,  ..., -2630.8835,  -260.0039,\n",
            "        -2497.0237])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -969.0927, -5001.5000, -5004.4570,  ..., -5007.8745,   -16.5469,\n",
            "        -2495.9204])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2538.7637, -1277.3113, -2529.2244,  ..., -5029.0938, -5044.1372,\n",
            "        -2599.9587])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5071.5586, -5040.9165, -2518.1997,  ..., -4994.2744, -5010.8452,\n",
            "          -46.6003])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2549.6550, -4994.4448, -2511.3894,  ..., -5011.2456,   -29.8505,\n",
            "        -2625.7683])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2499.8894, -5007.5469,   -70.2483,  ..., -5031.0786, -2514.2729,\n",
            "          -72.8758])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4994.4849, -2531.7759, -2543.0369,  ..., -2526.6445, -2508.5120,\n",
            "        -2504.1714])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -26.5737, -5016.8140, -5008.6523,  ..., -2502.0552, -2522.0198,\n",
            "        -2484.0029])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4992.7925, -2506.1738, -5022.0308,  ...,   -24.0396, -2508.8193,\n",
            "        -4994.4487])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2584.7866, -5047.1172, -5249.2363,  ..., -5028.2139, -2514.5198,\n",
            "        -2512.0010])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5010.6450, -2513.7634, -5036.3911,  ..., -2528.6975, -4992.8760,\n",
            "          -13.5260])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2.5642e+03, -2.5486e+03, -3.6771e+01,  ..., -5.0093e+03,\n",
            "        -2.5845e+03,  1.2265e+00])\n",
            "actor loss: 98393.40309748745, critic loss: 98628892.5, entropy: 55943.762451171875, KL divergence: 0.012984558827491222\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[218.13917946931574, 324.71557699749576, 1, 1, 2.030989509330306], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "1エピソード目の累積報酬：-292983.54766904365, 一つ保全の回数：7808, 二つ保全の回数：301, 三つ保全の回数：83, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-589554544891>:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(self.checkpoint_file))\n",
            "<ipython-input-5-b57aabd33ec4>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(self.checkpoint_file))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   6.3553, -407.4289, -256.4860,  ..., -237.6520,  -70.9389,\n",
            "        -144.4411])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 251.3426, -135.5237,   69.2775,  ..., -150.2925, -454.1714,\n",
            "        -201.2375])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 142.2353, -164.2461,   73.1507,  ...,  -14.4012,   50.2529,\n",
            "        -137.7679])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-640.3618, -134.1410, -194.3224,  ...,   66.0692,  141.8049,\n",
            "           1.8108])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-159.6268, -332.3666,   79.7003,  ...,   -4.2506,   -8.0918,\n",
            "         -50.7627])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-223.0063,  170.3224,   56.1658,  ..., -254.3375,   94.3969,\n",
            "        -476.7338])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-186.3448, -222.6317,  -87.3673,  ...,   44.4948, -670.6686,\n",
            "         -78.5969])\n",
            "actor loss: 5909.9078697678215, critic loss: 3796328.921875, entropy: 64484.205322265625, KL divergence: 0.0019143092648145042\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[3.6946957759199046, 54.712480937281285, 0, 0, 2.0247530582887414], 離散行動：[1, 0], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "33エピソード目の累積報酬：-22662.414940247687, 一つ保全の回数：6737, 二つ保全の回数：799, 三つ保全の回数：656, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-340.3705,  198.3731,  204.4423,  ..., -208.4321,   17.9393,\n",
            "        -366.0142])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -61.3457,   -35.0978,   -28.9176,  ...,   -29.4717,   -66.7347,\n",
            "        -2163.9629])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-264.0095,   50.9078, -744.9070,  ..., -694.7286, -323.8233,\n",
            "        -239.3794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -260.0388, -1335.1144,  -758.7225,  ...,  -235.3057,    58.7352,\n",
            "         -215.5829])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.2366,    3.7895,  -42.6287,  ...,  256.3016, -323.3759,\n",
            "          39.2687])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  87.9104,  -19.3097, -796.9556,  ...,    5.0857,   70.0339,\n",
            "         117.6217])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.0279, -127.4300,  -67.3978,  ...,   49.5916, -297.9629,\n",
            "        -211.1961])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-732.7632,   69.5373,   33.3915,  ..., -204.5363, -170.2319,\n",
            "        -350.7495])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-154.1409, -207.1087,   87.7366,  ...,  101.8277,   25.9031,\n",
            "        -154.7723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -468.4430, -1221.7421,   -37.8026,  ...,  -186.5077, -1893.8975,\n",
            "           38.1865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-719.0201,  142.5622, -500.6750,  ...,   43.6519,   15.7061,\n",
            "        -213.0081])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -25.0670,   41.7785, -557.5895,  ..., -200.6924,   43.1764,\n",
            "         -79.5713])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-145.7036,  -45.5780, -143.1465,  ..., -516.4028, -391.3556,\n",
            "         196.6107])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 109.1046,  -39.3993,   57.4646,  ...,   -6.1332, -103.0599,\n",
            "        -102.3138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-264.7027, -124.3004,  153.3763,  ...,  104.9994,  -66.6076,\n",
            "         101.7730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-752.8110, -151.6501,  575.8568,  ...,  243.7719,   43.6519,\n",
            "         176.0988])\n",
            "actor loss: 5516.526964165163, critic loss: 3455598.796875, entropy: 64357.040771484375, KL divergence: 0.0016383857451213932\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 0, 0, 1.2832914609219679], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "34エピソード目の累積報酬：-24247.125952661954, 一つ保全の回数：6696, 二つ保全の回数：855, 三つ保全の回数：641, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   5.1122,  246.8295, -436.5806,  ...,   53.4896, -356.6172,\n",
            "         184.3144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -64.4507,   35.4475,  112.0967,  ..., -619.5046,   -5.2522,\n",
            "           7.2596])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -23.9906, -332.2002,    5.0160,  ...,   21.7539,  -92.4696,\n",
            "         122.0369])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -59.3638, -267.8362, -397.3178,  ..., -609.2995, -176.2650,\n",
            "          52.8238])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-147.4241,  -78.9380,  -40.1688,  ...,  256.8949,   93.4347,\n",
            "         -73.7364])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 106.1527,   39.4817, -399.3285,  ..., -391.5415, -245.0763,\n",
            "         -22.9474])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([162.6600, 149.2767,  97.1739,  ..., 157.2286,  82.6221, 177.1368])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-371.0912,  862.5692, -234.0383,  ..., -662.8786, -434.2610,\n",
            "        -971.0195])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -405.0906,  -190.1957, -1877.3411,  ...,    45.0687,    76.5201,\n",
            "        -2082.2024])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-50.1831, -20.3674, 234.9157,  ..., -54.2582,  43.3277, -80.3026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 355.7084, -249.9361, -224.5207,  ..., -792.7090, -222.0730,\n",
            "          -9.5972])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -43.2485,  -164.0894,  -159.5825,  ..., -2892.6963,    87.7681,\n",
            "         -204.7341])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-408.3794,  -98.8705, -221.6948,  ..., -194.9455, -159.6338,\n",
            "        -162.7919])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -85.5693,   53.2611,  -92.8525,  ...,   76.5201, -614.3400,\n",
            "         -72.0885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.2835, -200.8904,   77.2423,  ...,  228.1300,  -10.2439,\n",
            "        -148.3252])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  59.3156,   57.7122,  198.0676,  ..., -109.1205, -168.4021,\n",
            "        -185.3947])\n",
            "actor loss: 5205.598182117671, critic loss: 3090276.5625, entropy: 64337.12353515625, KL divergence: 0.0016277590969730376\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 0, 0, 1.5078360629500216], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "35エピソード目の累積報酬：-23949.52811975116, 一つ保全の回数：6748, 二つ保全の回数：837, 三つ保全の回数：607, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  10.9272, -248.1336,   78.2387,  ..., -279.0704,   68.8895,\n",
            "        -330.6493])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -20.7783,  -577.6565, -1912.0925,  ...,    -5.3440,   -59.9975,\n",
            "         -368.5970])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-181.7056, -107.8036, -301.3058,  ...,  -22.0355,    6.5225,\n",
            "         -98.1703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-299.4750,  -47.6829, -213.1892,  ...,  133.2073,  153.4847,\n",
            "          36.7818])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -25.8268,   41.9594,  217.6700,  ...,  -58.5318, -135.4908,\n",
            "         174.7843])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   7.5204,  130.8914,  -73.9841,  ...,   32.7583, -105.8198,\n",
            "         229.6501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 284.9767, -376.7731, -236.6100,  ...,   84.2546, -438.9372,\n",
            "        -262.9781])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-382.0727, -413.6451, -129.6700,  ...,  -99.9690,  193.1686,\n",
            "        -183.1412])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-141.6081,  332.8390,  319.1904,  ..., -301.6262,   86.7632,\n",
            "        -477.2251])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-181.6651,  784.9131,  -94.2377,  ..., -132.4292,   25.9786,\n",
            "         -10.7873])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.4353,   61.5638,  -28.9458,  ..., -200.5151, -975.0567,\n",
            "         -27.9463])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 217.0484, -291.9655,  -25.2971,  ...,   92.8106, -185.4391,\n",
            "        -166.8840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-203.2524,   -2.3036, -425.1395,  ..., -407.9702,  -96.9933,\n",
            "        -368.3006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-408.7767,  -67.0148, -386.4236,  ...,  289.2968,  455.8962,\n",
            "          71.3245])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -51.9615,   33.7989,  -20.3111,  ..., -197.6041, -236.2534,\n",
            "        -290.7773])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 138.5225, -152.1572,  147.9151,  ..., -161.3526, -191.5728,\n",
            "        -496.4696])\n",
            "actor loss: 5229.542104457465, critic loss: 3162303.625, entropy: 64114.519775390625, KL divergence: 0.0016829646922228328\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[1.0503909684964705, 9.01068289661653, 0, 0, 1.2557528201515216], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "36エピソード目の累積報酬：-29844.04606342391, 一つ保全の回数：6754, 二つ保全の回数：837, 三つ保全の回数：601, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-23.4034, 158.0020, -58.3938,  ..., 329.9644,  -1.9324, -86.5582])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.4775, -113.4586,  181.1444,  ..., -190.9811, -242.6955,\n",
            "          -5.9232])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-113.8264,  131.7105,  -39.8219,  ..., -129.1912,  341.3245,\n",
            "         -78.6427])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -53.5416, -157.2663, -148.5885,  ...,  236.2564, -493.0500,\n",
            "        -237.7865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 100.9236,  188.5560, -690.4579,  ..., -136.4370,  -28.9463,\n",
            "        -326.4751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-758.5706,  210.4742,   37.5959,  ..., -133.8978, -158.4449,\n",
            "         130.6326])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-149.2975,   26.4343,  -34.0072,  ...,  -50.9118,    2.6116,\n",
            "        -131.5173])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-437.7087,  -23.2508, -186.6615,  ..., -332.1471,  283.4958,\n",
            "         -51.5476])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  118.9326,   -11.5613,  -239.2760,  ...,    17.0003,   120.7039,\n",
            "        -1903.8049])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   36.7655,   141.5768,  -145.6081,  ..., -1788.0872,    83.3023,\n",
            "           68.3810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.2276, -465.3354, -341.4327,  ..., -999.0019,   80.4773,\n",
            "         303.0191])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.6339, -946.1258, -197.0783,  ..., -265.7119,   89.3023,\n",
            "         -11.9391])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  64.3927,   85.1200,  -18.9347,  ..., -120.8527, -258.4612,\n",
            "          37.5959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-408.8960,  -54.4656,   61.1636,  ..., -720.4368,  143.5624,\n",
            "        -149.0996])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2023.6160, -1875.6028,  -212.1747,  ...,  -119.0794,     4.3464,\n",
            "          -69.1356])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -26.5109,    2.1928, -143.4666,  ...,  188.9734,  -43.6575,\n",
            "          37.8755])\n",
            "actor loss: 5342.95652615032, critic loss: 3049891.09375, entropy: 64339.638916015625, KL divergence: 0.002060686745956773\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[10.387318822344078, 0.3164339769665649, 0, 0, 1.1716548982300954], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "37エピソード目の累積報酬：-27243.823311605564, 一つ保全の回数：6718, 二つ保全の回数：894, 三つ保全の回数：580, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([157.1372,  58.8582, 229.3149,  ..., -21.4681,  10.7127,  62.8022])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -73.6174,  -54.2001,   95.5591,  ..., -186.5907,  -16.0189,\n",
            "        -128.7986])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-507.5107, -366.0274, -252.5384,  ...,  176.5261,  -84.3446,\n",
            "          -8.4420])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-944.8500,   68.6813, -183.1020,  ...,  174.7390, -333.7028,\n",
            "         171.0086])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-215.9787,  292.7263,  -58.9772,  ...,  169.9288,  113.8964,\n",
            "        -254.1163])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -143.0048, -1122.8478,  -188.3717,  ...,   -72.2029,   -33.6580,\n",
            "          272.6175])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  250.3889,  -232.5031,  -110.8321,  ...,  -325.6191,  -378.8748,\n",
            "        -1257.0858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-285.4936, -213.9471,  -73.9405,  ...,  -87.1672, -183.9880,\n",
            "         -39.2988])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.5835, -137.2355, -192.6694,  ..., -215.9787, -167.6405,\n",
            "         416.1348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   63.8583, -2343.3618, -1156.0640,  ...,  -215.8200,  -498.3882,\n",
            "          146.2811])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-218.0686, -418.1035,  136.4990,  ...,   79.6229,  107.6747,\n",
            "        -986.3484])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-783.0812,   95.5161, -206.2679,  ..., -101.1897, -960.6780,\n",
            "        -191.1627])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -74.6701,  -16.1356,   96.9380,  ...,   87.6283, -476.5163,\n",
            "         231.7362])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -171.5550,   112.8513, -2617.8027,  ..., -1597.8153,   153.9884,\n",
            "          -48.8530])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 186.4311,  181.3691, -192.8368,  ..., -135.0058, -322.3251,\n",
            "        -413.1652])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -363.4614,  -446.2165,  -108.0201,  ...,   -35.9309, -2398.7249,\n",
            "         -731.4493])\n",
            "actor loss: 5148.3110245816915, critic loss: 3197342.25, entropy: 64293.798095703125, KL divergence: 0.002490726582997494\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[4.920296159556397, 37.135118817439896, 0, 0, 1.0867617703805676], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "38エピソード目の累積報酬：-25882.76114575636, 一つ保全の回数：6747, 二つ保全の回数：879, 三つ保全の回数：566, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.0385e-01, -1.8355e+02,  3.5877e+00,  ..., -2.0302e+02,\n",
            "        -2.0341e+03, -2.6235e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   70.2464,   -92.4776, -1658.1493,  ...,  -546.4999,    11.0529,\n",
            "         -262.9566])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-836.2277, -312.5616,   -1.1772,  ...,   24.5375,  144.7778,\n",
            "        -239.6189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-267.7084, -639.0784, -998.4535,  ..., -304.6476,  -77.3513,\n",
            "         -32.1674])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -17.0112,   59.7362, -395.1628,  ...,  179.4946,  -19.6095,\n",
            "        -193.4270])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -93.9508, -226.0412, -122.5164,  ..., -519.1000,  104.2863,\n",
            "         299.7032])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  89.3429,  102.6666, -394.3566,  ...,  107.4426,  133.6357,\n",
            "        -210.4035])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-174.1790,  151.2965, -173.7433,  ..., -309.1103,  -57.8298,\n",
            "         142.1823])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   20.8263,  -334.6868,   154.4130,  ...,  -123.9192,  -175.2776,\n",
            "        -2333.4751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-282.6430,   20.6170,  -95.1168,  ..., -105.1691,   86.0858,\n",
            "          94.7018])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.6159,  223.7741,  -20.8220,  ..., -720.1763,  -68.9420,\n",
            "          90.2291])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-351.6011, -248.6095,  -23.4189,  ...,  182.2553, -363.8791,\n",
            "         -89.9411])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-211.3824,  -73.9004, -427.2220,  ...,    5.8081,   86.7817,\n",
            "        -669.0872])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-321.0356,  172.2347,  120.1347,  ..., -185.7326, -459.4865,\n",
            "        -101.7871])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -41.9685,  -215.2330,   -70.9554,  ...,  -391.8695, -1776.1577,\n",
            "        -1190.3783])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-133.2036,   -7.6706,  -98.0718,  ..., -602.2839, -139.4561,\n",
            "           9.7063])\n",
            "actor loss: 5214.266388081824, critic loss: 3170785.828125, entropy: 64142.03955078125, KL divergence: 0.003025477347249425\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[13.257742270732475, 28.700835959926344, 0, 0, 2.0061587965625973], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "39エピソード目の累積報酬：-26038.788905050624, 一つ保全の回数：6715, 二つ保全の回数：915, 三つ保全の回数：562, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -245.1760,   -21.1430,     8.1876,  ...,  -395.7262,    71.0369,\n",
            "        -1155.2533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -330.1466,   122.8685, -2422.3140,  ..., -2694.9065,  -184.2742,\n",
            "         -593.6516])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  71.3878,  -49.7788, -328.4831,  ..., -171.0637,   -2.8887,\n",
            "         -26.5154])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -34.0192,  -186.9755,  -204.0366,  ...,  -505.1026, -1696.0170,\n",
            "           60.9601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   2.8533, -646.9669,  -69.6286,  ...,  -93.9759, -346.2531,\n",
            "          42.0960])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  76.1669, -342.1507,  157.4172,  ..., -198.1862,   25.9344,\n",
            "        -224.1937])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-239.7943,  107.7122,  -95.9604,  ...,  -88.1149,    7.1771,\n",
            "          71.6174])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-449.1712,  220.3393,  101.6297,  ...,   61.5746, -223.8121,\n",
            "         -70.5254])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-411.6180, -264.9020, -141.9960,  ..., -454.1143,   37.3070,\n",
            "         313.6218])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-11.5476,  63.8096, -25.7727,  ...,  12.0873,  58.7654, 191.1342])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([1099.7283,  -52.3131, -215.0238,  ..., -134.1535, -350.6921,\n",
            "          34.1871])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  77.6962, 1327.3098,  -85.5476,  ..., -345.0582,  267.5264,\n",
            "          13.9771])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-161.7372,  131.6547,   78.8792,  ...,   68.1635,  -62.7201,\n",
            "        -215.7127])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-57.1232, -86.3944,  67.1071,  ...,  74.4048, 221.3784,  86.8311])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-173.3902, -537.5524,   53.3115,  ...,    0.9067, -189.0683,\n",
            "         111.7024])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-186.8505,  -16.9913,  -95.4735,  ...,  -76.6916, -121.0894,\n",
            "         -24.5557])\n",
            "actor loss: 4963.852346308737, critic loss: 3388098.96875, entropy: 64204.624755859375, KL divergence: 0.0032882259424190236\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 0, 0, 2.3097061802510273], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "40エピソード目の累積報酬：-22603.141609311006, 一つ保全の回数：6748, 二つ保全の回数：956, 三つ保全の回数：488, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-394.9656, -124.6515, -549.1484,  ...,   52.6345, -206.2383,\n",
            "         -60.6127])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-168.8529,  -18.2682, -192.1282,  ..., -120.5941, -320.7567,\n",
            "        -337.5487])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.1630, -476.2861, -373.8405,  ...,  173.6284, -120.1655,\n",
            "         189.5708])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -53.8924, -104.6893,  129.4619,  ..., -142.9746,  213.2781,\n",
            "          98.0997])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-234.8761,   97.4730, -234.0405,  ..., -104.3824, -292.5350,\n",
            "        -120.2348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -174.9552,  -265.0536,    -9.2593,  ..., -1553.5696,   189.5340,\n",
            "          121.0154])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  70.0777,  707.9941, -255.3127,  ...,   74.5194,  -94.8955,\n",
            "          -8.9780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   95.5898,  -137.3175, -2491.2070,  ...,    17.7142,    40.3053,\n",
            "          179.2939])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-526.9040, -343.5879,  -33.6217,  ...,  -82.5362, -203.1083,\n",
            "        -121.4935])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -43.5957,  -14.9053, -461.0225,  ..., -108.1270, -193.9789,\n",
            "          45.4796])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -11.9197,  132.0826, -146.8661,  ...,  218.5363,  -35.9188,\n",
            "         -11.7710])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-193.3302, -256.3653, -152.5309,  ..., -277.9092, -150.3851,\n",
            "         193.6812])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-272.6035, -143.0733, -485.5477,  ...,   34.8231,   78.5540,\n",
            "        -129.2083])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-221.4242,  -42.6617,  -94.6716,  ..., -883.4211, -226.7908,\n",
            "        -286.3928])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-119.8731,  -23.1860,    8.2725,  ..., -142.0394,   32.4068,\n",
            "        -684.9520])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 417.3564, -153.4256, -509.3011,  ..., -146.7884, -307.9270,\n",
            "        -743.1149])\n",
            "actor loss: 4782.135222174545, critic loss: 2858650.15625, entropy: 64256.89892578125, KL divergence: 0.0033453973790803036\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[15.07264491531351, 18.58811493673548, 0, 0, 1.5381797036008669], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "41エピソード目の累積報酬：-24734.17237726064, 一つ保全の回数：6720, 二つ保全の回数：989, 三つ保全の回数：483, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-512.2491, -308.8945, -114.5536,  ..., -123.4012, -229.0212,\n",
            "         -32.3791])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  25.0500,  -87.5583, -366.7281,  ...,  -95.0112, -149.6450,\n",
            "        -144.2275])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2156.6138,    27.1187,   229.4123,  ...,   132.7397,    60.1550,\n",
            "          124.3764])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-359.2455, -135.1036, -304.6639,  ...,   77.0112, -563.6789,\n",
            "        -173.3099])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  88.8637,  -71.6813,  215.5287,  ...,   32.6279, -213.1260,\n",
            "        -115.9428])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-232.5789, -122.7970,   18.0171,  ..., -190.0905,   57.1330,\n",
            "        -114.8746])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1180.9628,   120.3510,   -50.0862,  ...,  -219.6831,  -489.1892,\n",
            "         -160.2567])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-277.7497,  -84.8855,  -71.9286,  ..., -485.2705,  -15.3020,\n",
            "          65.2474])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   9.1776, -324.2955, -282.1245,  ..., -344.8205,  148.3338,\n",
            "        -128.8608])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-108.0943,  -80.6822, -134.4991,  ..., -174.5856, -502.8419,\n",
            "        -588.0126])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -274.1231, -2302.5061,  -119.3988,  ...,  -775.9655,   -38.9042,\n",
            "         -263.4071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-469.2669, -460.4916, -243.3408,  ...,  -93.2122,  -17.3900,\n",
            "          26.5814])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 309.1100, -138.0282, -683.7712,  ..., -132.2332, 1085.8474,\n",
            "        -470.0943])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([192.6148, -83.2312,   2.3867,  ...,  58.7589,  47.6438, -89.4816])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  87.5731,   -1.2317, -228.6249,  ...,   31.2675,   41.4275,\n",
            "         269.5368])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-233.4557,   80.0237,   72.1971,  ..., -333.7692,  -51.4417,\n",
            "           0.6524])\n",
            "actor loss: 4903.53746788951, critic loss: 2998231.609375, entropy: 64006.630859375, KL divergence: 0.0033826757681056337\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[24.99254877722121, 52.19165814212364, 0, 0, 1.809018379971063], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "42エピソード目の累積報酬：-26393.076645984667, 一つ保全の回数：6695, 二つ保全の回数：1033, 三つ保全の回数：464, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -93.0082,  -75.4568,  -17.4650,  ...,   50.3982, -111.8204,\n",
            "        -230.3165])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-167.6497,  200.5937,  -30.7164,  ...,  -82.9970, -219.4487,\n",
            "        -440.0141])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-251.5415, -147.0413,  -76.3236,  ..., -122.8898, -311.4825,\n",
            "        -203.6477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-336.4306,   45.6222,  -67.8361,  ..., -107.0196,   56.2948,\n",
            "         -90.3805])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-210.7021,  139.4072, -150.8879,  ..., -207.5715,  -54.4341,\n",
            "        -263.0547])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  30.5353, -666.8226, -893.1122,  ...,   -8.2552,  -99.8140,\n",
            "        -173.1806])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-989.4601,   78.6905, -566.0666,  ..., -136.3083,  -27.5840,\n",
            "        -250.8376])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-312.9455, -411.5044, -260.7675,  ...,   18.5826, -188.3145,\n",
            "         -74.9149])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -406.0712, -1515.3075,   209.8986,  ...,   165.2402,  -275.1965,\n",
            "          -90.3601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 14.0784, 198.1649, 101.9590,  ...,  15.9757, -77.2866, -21.4612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 123.0648,   80.2764,   27.5566,  ..., -149.9456, -320.9423,\n",
            "        -325.7292])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -9.0183,   91.5866,   25.9957,  ..., -372.6743,  -67.6407,\n",
            "         232.4872])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 185.0429,  -50.5945,  -73.9607,  ..., -211.5063,  -52.6154,\n",
            "        -325.5543])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-866.8823, -680.3550,  335.2003,  ...,   43.6419,  -38.5706,\n",
            "         -93.9618])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -58.2072,  -84.2964, -107.1624,  ...,   13.8257, -537.8847,\n",
            "         127.9281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -82.2087, -146.3414,  101.4184,  ..., -444.3466,  -76.7950,\n",
            "         -77.9886])\n",
            "actor loss: 4668.950681306815, critic loss: 2591737.015625, entropy: 63761.326171875, KL divergence: 0.002589877781731931\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0.026844011659620387, 0, 0, 0, 1.8394450752526965], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "43エピソード目の累積報酬：-22565.415971282062, 一つ保全の回数：6700, 二つ保全の回数：1040, 三つ保全の回数：452, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  54.7208, -243.6872,   78.2213,  ..., -425.4619,  133.5697,\n",
            "          -9.8596])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-749.7342,  167.5573, -381.8088,  ...,  -52.8403,   12.4309,\n",
            "        -914.8581])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 222.7688, -271.1534,  -59.9053,  ..., -217.2470, -120.3213,\n",
            "          61.7135])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 105.8547, -195.5182,  -87.0646,  ...,   15.1806,  -85.3554,\n",
            "          20.3816])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   43.8242,  -357.8962,  -418.6152,  ...,  -172.5522, -1168.6216,\n",
            "         -206.2320])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-141.0886,  155.1718,  108.1828,  ..., -174.8440, -201.4389,\n",
            "          67.1851])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-552.8326,  111.2386,   20.6353,  ..., -179.0028,  101.3022,\n",
            "         -13.0535])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -66.2121, -259.0414, -146.0564,  ...,   21.5490, -262.9506,\n",
            "        -169.2590])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -11.1567, -194.0174, -132.7672,  ..., -120.3213, -287.3930,\n",
            "        -152.9360])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  34.0813, -379.1948, -365.9789,  ..., -192.2962,  -71.4062,\n",
            "        -271.8580])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  131.5440,  -422.1162, -1061.1284,  ...,  -374.5698,  -227.8240,\n",
            "         -141.3928])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  322.0147,  -197.9588,   110.6068,  ..., -1180.6699,   -20.3255,\n",
            "          110.6836])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  18.4724,  -35.0235,  -93.4205,  ..., -741.6128,  -99.8067,\n",
            "         -14.9736])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-254.1943,  -29.8954,  245.8927,  ..., -195.6369,  -18.6751,\n",
            "        -169.8755])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  42.1171, -123.5050,  -88.6766,  ..., -597.1125,   74.4282,\n",
            "        -156.9465])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  28.7301,  -16.5763, -153.5313,  ..., -370.2863,  -33.7169,\n",
            "        -490.8004])\n",
            "actor loss: 4530.061912911464, critic loss: 2500482.796875, entropy: 63807.772216796875, KL divergence: 0.002152394304483539\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[77.33391282073559, 2.2201617243371192, 0, 0, 2.347999745419937], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "44エピソード目の累積報酬：-23044.47659577465, 一つ保全の回数：6677, 二つ保全の回数：1097, 三つ保全の回数：418, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  28.8319,  -83.9773, -644.1467,  ..., -304.1566, -246.0971,\n",
            "         -11.8718])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -227.7353,    92.9951,  -205.4210,  ..., -2583.3784,  -250.4339,\n",
            "          839.5210])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -521.9017,    94.3601,  -151.2767,  ..., -1087.6572,   154.0573,\n",
            "          -94.8906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  86.8619, -288.4019, -234.1027,  ...,   67.7621, -686.5576,\n",
            "        -127.7145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 251.0262, -220.9732, -991.8327,  ..., -387.1786,  -59.4727,\n",
            "         287.7914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  23.4772, -292.0632,   36.7302,  ...,   90.8899,  -31.0816,\n",
            "        -156.5689])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-155.3453, -255.2538,  -82.6582,  ..., -100.7045, -103.6949,\n",
            "         -76.8478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-167.8975,   82.8786, -323.5605,  ..., -108.4452,  -25.3994,\n",
            "        -180.2243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-153.4130, -222.9481,   59.0927,  ...,  181.8547, -140.7527,\n",
            "        -149.5749])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -74.6124, -206.6933, -250.4339,  ...,  -66.2884,  -22.5590,\n",
            "        -723.0380])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1029.0529,    83.3024,   -39.5528,  ...,   143.2201,  -403.7294,\n",
            "          -57.5735])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -69.8954, -128.6734, -233.6811,  ...,  -33.8824,  -23.1214,\n",
            "        -769.6931])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.2064,  -58.1998,   59.6102,  ..., -344.7458,   -4.6148,\n",
            "        -290.7421])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3.9427e+02,  6.4988e+00,  6.3585e+01,  ..., -3.8599e+02,\n",
            "         9.5811e-02, -3.9046e+01])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -23.3009, -236.0956,    9.1484,  ..., -847.6846, -312.5164,\n",
            "         875.0711])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 158.3655,  -57.2577,   52.0486,  ..., -100.9409, -130.5387,\n",
            "          43.6098])\n",
            "actor loss: 4341.779277854164, critic loss: 2302338.4453125, entropy: 63766.01806640625, KL divergence: 0.0026527750921973314\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[60.34056138964579, 0, 0, 0, 1.4853816944598193], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "45エピソード目の累積報酬：-23814.767979638615, 一つ保全の回数：6703, 二つ保全の回数：1075, 三つ保全の回数：414, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.9252, -175.1116,   72.4284,  ..., -168.4076, -170.6785,\n",
            "          32.9570])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  43.7854, -172.3778, -247.8733,  ...,  -97.0491,  -44.5586,\n",
            "          48.2375])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  78.9244, -178.7156,    3.0669,  ...,   75.8615,  -16.7009,\n",
            "         -50.6126])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-100.3361, -205.9433,  110.0957,  ..., -276.7581, -249.6224,\n",
            "          90.7662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-154.9730,  -49.1993,  382.3576,  ...,  109.8496,  135.3687,\n",
            "         -14.6712])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-197.8212, -254.0694, -531.7729,  ..., -223.3151,  -35.0626,\n",
            "         132.9470])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-135.1330,  444.4135,  235.3310,  ..., -147.5308,  102.5198,\n",
            "        -358.7308])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -11.3293,   -4.7871, -106.9002,  ...,  132.2094, -119.6633,\n",
            "          77.7356])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-102.1767, -115.7603,   17.1465,  ..., -220.1844,  -97.0263,\n",
            "         150.7708])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  42.2047,    0.2594,  -65.0081,  ...,  106.5876,    3.2742,\n",
            "        -114.6409])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -32.0175,  -75.4447,   78.0512,  ..., -313.7147,   70.3517,\n",
            "         -41.8166])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-219.4438,   39.4510,  -22.0321,  ..., -236.8286, -391.6934,\n",
            "         245.8299])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-801.6336, -208.3040, -286.0779,  ...,  -67.3819,   90.5877,\n",
            "          34.4512])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-115.4095, -141.1884,  -32.1184,  ...,  347.2518, -370.7418,\n",
            "        -475.6541])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-228.8487, -183.7845, -101.8303,  ...,  105.6439, -545.0194,\n",
            "          79.4074])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-407.1776,  -10.5608,   70.8228,  ...,  -83.9447, -325.1773,\n",
            "         -31.8519])\n",
            "actor loss: 4484.664121400359, critic loss: 2853332.875, entropy: 63542.620849609375, KL divergence: 0.0032044954091022576\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[62.545444329375634, 79.16501239561312, 0, 0, 1.545484645412608], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "46エピソード目の累積報酬：-28551.9301261481, 一つ保全の回数：6676, 二つ保全の回数：1119, 三つ保全の回数：397, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-217.7198, -318.3061,  -88.0332,  ...,  115.4910, -381.4707,\n",
            "         -36.4487])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -83.5711,    6.2578,   -9.0121,  ..., -173.5162,  -61.5940,\n",
            "         117.1397])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 187.0214,  -35.6942,  110.0427,  ..., -213.4783, -197.6129,\n",
            "        -112.0452])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -56.7774, -109.8401, -553.2025,  ...,  229.7971, -358.0132,\n",
            "         -60.0158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-159.2361, -426.7219,  121.3496,  ...,    4.6064,   51.7146,\n",
            "          -6.0959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -85.6354,  188.8682,   66.6701,  ..., -240.2910,  357.9257,\n",
            "         -31.8240])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-219.2536, -526.5621,  -96.3158,  ...,   21.9362, -242.4470,\n",
            "         182.3533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 244.1164, -220.3348,   56.7107,  ...,   82.1437, -279.1339,\n",
            "         -73.4736])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-292.3206,  285.2445,  -63.5556,  ..., -206.0421,    2.2419,\n",
            "        -333.4368])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -94.0826, -799.8091,  -64.7165,  ..., -243.0533,   -6.4559,\n",
            "          79.7851])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-139.3900, -240.2797,  -82.7606,  ...,   30.9287,   -3.1089,\n",
            "        -206.6524])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -152.4368,  -153.6060,    13.1050,  ...,  -140.5417,  -161.7430,\n",
            "        -1810.6927])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 159.4831,  -89.5518,  218.7636,  ..., -226.5913,  299.3860,\n",
            "         -42.1407])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.2954,   18.0822,   14.1546,  ...,    9.5995, -206.0421,\n",
            "          13.7879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 302.2982,  -31.5400, -428.9057,  ..., -118.7731, -107.0854,\n",
            "         265.9827])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -11.2524,  106.4175, -383.5539,  ..., -218.5597, -541.5973,\n",
            "         -67.7708])\n",
            "actor loss: 4300.949461801623, critic loss: 2730622.734375, entropy: 63764.762939453125, KL divergence: 0.005604651983471486\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[10.628137148328243, 59.129099868583324, 0, 0, 1.5849360386447968], 離散行動：[0, 0], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "47エピソード目の累積報酬：-24917.749755662404, 一つ保全の回数：6654, 二つ保全の回数：1206, 三つ保全の回数：332, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 300.5822, -332.4278,   82.2848,  ..., -133.3939, -352.2865,\n",
            "        -371.9787])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2.0198e+02, -3.1629e+01,  5.1417e+01,  ..., -4.6334e+02,\n",
            "         2.3920e-01, -1.8533e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-117.2367,  173.9694,   41.6576,  ...,  -95.7296,   63.9521,\n",
            "          44.8501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-154.5307, -330.6921, -265.5566,  ...,  213.3419, -265.8846,\n",
            "        -129.1754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-618.3644, -281.1846, -243.4942,  ...,  128.0343, -127.0075,\n",
            "        -172.9138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-193.4627, -169.1777,   58.5123,  ...,  -60.7507,  -49.4383,\n",
            "        -756.2521])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-321.7327,   -1.6549, -111.5942,  ...,  -57.1415,  -43.7571,\n",
            "          73.5022])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -42.3999,  125.2674, -175.2021,  ..., -304.6261,    8.4117,\n",
            "        -532.0373])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  53.4395,  127.4737, -139.1407,  ..., -359.3575,  -50.0716,\n",
            "         -92.7288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-63.7982,  73.3027, 283.2249,  ...,  70.6375, -59.5164, 157.5064])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -167.8076,   -26.4953, -1441.6243,  ...,    76.9935,   129.7571,\n",
            "         -274.1863])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 136.3027,  -97.7458,  201.2019,  ..., -333.7452,  299.9437,\n",
            "         -50.9955])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-125.2073,  553.9639, -213.2722,  ..., -546.5271,  -11.3575,\n",
            "          44.8811])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-309.3791, -616.6708, -473.4558,  ...,    2.7374,  279.3780,\n",
            "        -567.8628])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 754.9384,   58.9609, -286.2814,  ..., -265.2072, -546.3942,\n",
            "         115.5594])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-456.9074, -101.0148, -514.2981,  ...,   18.9968,   69.0834,\n",
            "        -130.9141])\n",
            "actor loss: 4215.751229710484, critic loss: 2621487.609375, entropy: 63681.626708984375, KL divergence: 0.006492760181969712\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[3.6393137865180263, 20.968170650639514, 0, 0, 1.2515084314386804], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "48エピソード目の累積報酬：-25337.94575587208, 一つ保全の回数：6637, 二つ保全の回数：1210, 三つ保全の回数：345, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 282.3503,  174.2550,   -5.6387,  ...,  -96.4406, -720.5507,\n",
            "        -474.0359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-329.0242, -124.3788, -258.9699,  ...,   83.9684,  173.6494,\n",
            "         145.8668])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-193.2001, -339.5049, -193.2760,  ...,  -62.8503,   82.8771,\n",
            "         101.3684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -10.7143, -222.7648,  -36.6710,  ...,  -31.0533, -609.1425,\n",
            "         -34.3716])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-211.3708,   79.4798, -169.2502,  ...,   97.0620, -206.6355,\n",
            "          14.2437])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([    6.0441,  -177.8928,    34.9862,  ..., -1309.5911, -1088.0383,\n",
            "          111.5612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-156.9836, -686.2819,  151.0902,  ...,   81.2190,  205.3701,\n",
            "        -239.3660])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  27.5515, -322.9378,   38.8810,  ...,    4.6644, -429.7511,\n",
            "          57.9468])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  68.7670, -192.2778, -784.9243,  ..., -426.1580, -165.9853,\n",
            "        -181.2042])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-123.9413,   68.4289, -195.0260,  ..., -221.8933,  179.4639,\n",
            "          42.2348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-103.5584, -491.6698,  -61.6507,  ...,   87.6136, -127.5146,\n",
            "          87.1766])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   4.0471, -377.7964,  245.2458,  ..., -154.0149,   55.0673,\n",
            "         -52.6205])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  76.3193, -853.8456,   42.7673,  ...,   54.5626, -253.3542,\n",
            "         -97.6285])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 750.1882, -436.1463,  -18.2281,  ...,  -75.5685,   88.1863,\n",
            "         246.7824])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-150.9845,   75.9971, -198.4716,  ...,  -29.6020, -410.1579,\n",
            "        -115.4780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  85.6496, -369.4463,  -74.8562,  ...,  107.5062, -165.7603,\n",
            "        -282.1409])\n",
            "actor loss: 4004.568924413175, critic loss: 2461642.0625, entropy: 63476.55029296875, KL divergence: 0.005647980499399846\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[32.45806624756588, 21.54602222718924, 0, 0, 2.3780478547992394], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "49エピソード目の累積報酬：-23805.951472199493, 一つ保全の回数：6598, 二つ保全の回数：1274, 三つ保全の回数：320, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -28.9715, -120.4414, -189.7882,  ...,  -70.7697,  472.6008,\n",
            "         104.1936])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.5102, -145.2377,   92.6275,  ...,  -57.7797, -152.7376,\n",
            "        -486.3331])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-296.6556, -197.8092,   -5.6733,  ...,  296.6034,  -76.2714,\n",
            "         240.2781])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 119.8755, -228.6091, -834.9286,  ...,  415.1045,   78.9126,\n",
            "         322.6160])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-171.0076, -409.0808,  139.4759,  ..., -569.0483,  -93.7963,\n",
            "        -102.6706])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-136.6317,  -56.1991,   99.0996,  ...,  -67.3045,  -59.6377,\n",
            "         -11.2254])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-268.1627, -761.5565, -119.5650,  ..., -145.7347, -300.2190,\n",
            "         -72.0444])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 125.8969,  -53.0437,  -26.7662,  ..., -269.0045, -126.1220,\n",
            "         -29.3174])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -205.2467,   109.8419,  -327.5389,  ...,   -29.8485,  -256.3885,\n",
            "        -1222.0189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-174.2157, -449.0830,   59.1412,  ..., -307.7330, -331.7701,\n",
            "        -232.6151])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-137.9619, -253.1578, -246.4961,  ..., -138.7591, -222.9736,\n",
            "         -75.9848])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -89.9821,  -158.8713,    84.1642,  ..., -1069.9136,   -39.4957,\n",
            "           21.5485])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -768.1106,   163.5434,  -206.2623,  ...,    -4.3034,  -111.4647,\n",
            "        -1151.2900])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  87.4628,   80.9803,   31.8466,  ...,  -62.3905, -205.6644,\n",
            "        -268.8177])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-182.4999,  106.1032,  -26.2779,  ...,   13.0376, -129.3353,\n",
            "        -454.4341])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -215.6333,    77.9379, -1140.6990,  ..., -1083.6390,  -873.2012,\n",
            "         -145.7589])\n",
            "actor loss: 4023.4883374303454, critic loss: 2041414.828125, entropy: 63597.80517578125, KL divergence: 0.003978865330921631\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[8.581143046106122, 31.73704030869339, 0, 0, 1.33487449095028], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "50エピソード目の累積報酬：-28174.350037956385, 一つ保全の回数：6585, 二つ保全の回数：1315, 三つ保全の回数：292, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-217.9834, -167.3878,  194.0416,  ...,  -88.8077,   -2.3434,\n",
            "        -547.2501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.4266,   86.1365,  -35.7751,  ...,  -72.5426, -417.5829,\n",
            "        -129.9796])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -14.7151,  -79.4618, -120.6007,  ...,   23.3277,   58.1259,\n",
            "        -160.7057])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.1980,    2.6162,   20.9427,  ...,  -92.2865,  -95.7644,\n",
            "        -155.4712])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -34.2359,  111.1114,  134.3867,  ...,   37.4385, -526.4661,\n",
            "         189.5330])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 136.2867,  -71.7245, -458.9991,  ...,  203.1417,  -40.8943,\n",
            "          21.8936])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-879.9963, -413.6250,  -10.3170,  ...,  247.5092, -158.2408,\n",
            "        -473.0339])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -73.0532,  -31.3926, -295.8798,  ...,   80.3038,   88.5588,\n",
            "         -17.1669])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -28.7545, -465.9337,  138.5088,  ...,   27.4617,  155.1660,\n",
            "         304.4697])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  69.6779, -208.1129, -335.7708,  ..., -714.2463,  -81.2073,\n",
            "           7.0544])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 395.0389, -232.6059,   -8.8372,  ..., -453.5877,   31.4386,\n",
            "        -943.8273])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -46.5011, -127.2675,  111.4242,  ...,   -4.4620,   49.5938,\n",
            "         -11.0241])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-547.6317, -368.3377,  -34.8514,  ..., -506.7091,  -31.2530,\n",
            "         -22.3790])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-115.0467,  148.7182, -301.0924,  ..., -171.4030,  -12.3927,\n",
            "        -162.2060])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -40.7520, -151.7597, -913.3670,  ...,   14.7133, -399.0375,\n",
            "         -65.6658])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  95.4956,    2.1998, -117.1231,  ..., -802.3032, -301.4501,\n",
            "        -266.4942])\n",
            "actor loss: 4112.2270341972, critic loss: 2138107.34375, entropy: 63408.749267578125, KL divergence: 0.0034566816300567755\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[3.764974079133394, 4.568449527378827, 0, 0, 1.1229322547511742], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "51エピソード目の累積報酬：-23493.119403083107, 一つ保全の回数：6547, 二つ保全の回数：1365, 三つ保全の回数：280, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -144.0201, -1217.4940,  -235.9778,  ...,  -247.4846,   116.3370,\n",
            "          -74.5040])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 119.1733,   23.5169, -233.4045,  ..., -316.1060, -923.7189,\n",
            "        -215.8889])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -36.4048,   79.1713, -246.2371,  ...,  110.2287,  185.9140,\n",
            "         -46.4237])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 188.5253, -420.4261,  101.8476,  ...,  -48.5527, -148.8445,\n",
            "        -479.7510])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -26.6469, -172.0707, -191.0467,  ...,  -40.8623,  141.9435,\n",
            "         239.2569])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -3.1620, -387.0582, -267.9658,  ...,   20.8176, -464.8546,\n",
            "          29.7810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 375.9795, -126.8764,  153.1335,  ..., -129.9436,   51.8429,\n",
            "         253.9371])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.2592,  185.7973,  -98.1014,  ..., -354.2612,  120.0777,\n",
            "         104.9784])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-368.1693, -239.4510,  -41.0734,  ..., -139.3605,  -66.8639,\n",
            "        -290.1509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1487.5984,    69.8561,   186.9265,  ...,  -171.5838,  -104.1890,\n",
            "         -984.3168])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-183.2279, -159.3808,  114.8947,  ..., -713.8041,  396.3865,\n",
            "           6.5680])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.5331,  -42.2261, -825.8719,  ...,  -61.0026,  -25.0600,\n",
            "         -50.4568])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-283.1795, -579.4341, -965.5635,  ..., -257.6471, -165.0863,\n",
            "          78.6999])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  179.6455, -1055.4205,    35.3169,  ...,    84.2864,   137.3681,\n",
            "         -165.9907])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -61.7616,  -42.1847,  -72.1263,  ..., -692.8864,   15.1169,\n",
            "          55.1485])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 210.4626, -241.5957, -280.1266,  ...,  175.1056,  -47.5205,\n",
            "         273.7885])\n",
            "actor loss: 3943.2794130474686, critic loss: 2290980.9140625, entropy: 63224.68505859375, KL divergence: 0.0031464204994975497\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[42.629991435539374, 8.346684491837935, 0, 0, 1.3352794580570895], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "52エピソード目の累積報酬：-22342.707117480757, 一つ保全の回数：6562, 二つ保全の回数：1345, 三つ保全の回数：285, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -72.2200, -726.7401,  204.1741,  ...,   74.7312,   82.8323,\n",
            "        -241.6673])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -48.5784,   30.8975,  -21.6518,  ..., -443.2779, -168.9141,\n",
            "        -293.0610])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-711.0969, -968.0549, -938.0813,  ...,  -22.2544, -296.9095,\n",
            "        -261.1458])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  10.3068, -239.2520, -149.7620,  ...,  -99.2569, -314.8863,\n",
            "          50.9828])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-409.2802, -222.7825,  -76.8253,  ...,   39.1552,   23.6308,\n",
            "        -732.9575])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.0088,  -21.6040, -146.0281,  ..., -234.1829, -273.4877,\n",
            "         -31.8533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-308.7230,  -57.6062,  288.7399,  ...,  149.0481,  226.2648,\n",
            "        -982.1693])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -52.5341, -705.7852,  295.0768,  ...,  -88.7703,  187.3102,\n",
            "         -28.6276])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([248.2876, -60.5028,  38.4177,  ...,  37.9431,  25.5080, 155.3905])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-386.2328,  182.0146, -125.8346,  ..., -211.3253,  -88.6913,\n",
            "        -422.3250])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-319.9175,   71.2989,   30.6552,  ...,  -15.4417, -145.3690,\n",
            "        -187.0880])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -40.2192,   -10.9563,   138.4310,  ...,  -432.8592, -1134.6011,\n",
            "         -208.0092])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-244.7746,  -98.0892,   11.9867,  ...,  173.0659, -833.1508,\n",
            "        -166.6794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-325.0150, -121.8415,  120.8551,  ..., -415.5565,  -71.7393,\n",
            "          45.7657])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -144.0806,  -258.9868,    86.0745,  ...,   -16.7780, -2258.4539,\n",
            "           -3.5845])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-165.5753, -237.2000,  -79.1838,  ..., -130.5555,  577.2883,\n",
            "         -33.3071])\n",
            "actor loss: 3726.5811739688834, critic loss: 2196576.3046875, entropy: 63095.47119140625, KL divergence: 0.0029190851947679267\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[22.021319904506917, 0, 0, 0, 2.345646059307666], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "53エピソード目の累積報酬：-25292.67491216678, 一つ保全の回数：6541, 二つ保全の回数：1387, 三つ保全の回数：264, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-436.3178, -112.8206, -123.7285,  ...,  248.7890, -133.7159,\n",
            "        -161.2751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -33.7960,   11.4587,  -47.3383,  ..., -117.1586,   64.8307,\n",
            "        -144.0366])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-139.6002,   80.4504, -212.0318,  ..., -309.7742, -349.4168,\n",
            "        -219.0153])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-281.3581, -131.4611,  -66.6582,  ...,  140.9856,  165.9499,\n",
            "        -180.3989])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  31.0630,   26.4615, -138.9637,  ...,   82.2026,  203.3430,\n",
            "        -326.8344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -94.7812,   13.7596,  239.1584,  ...,   51.2804,  -41.2625,\n",
            "        -506.0714])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  47.1331, -122.2346, -114.2705,  ...,  115.5795,  -89.2351,\n",
            "         -98.3125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -10.8174, -772.7627, -185.6880,  ...,  118.4743,  196.7793,\n",
            "        -595.2879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-262.3142,  -31.8391, -158.8836,  ...,   96.5986, -116.0503,\n",
            "          24.5621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-289.0674, -208.4182,   17.3825,  ...,  372.6101, -293.3006,\n",
            "          55.7917])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 338.0665,  -72.7987, -119.3425,  ...,  -18.4791,  -10.2595,\n",
            "        -576.6953])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-190.9844,  -43.5258,   56.1933,  ...,   67.0215, -124.4649,\n",
            "         117.6524])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-553.0472,  184.1780, -129.6665,  ..., -114.7252,   44.1774,\n",
            "        -411.8145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-129.7235,  -83.5464, -153.9066,  ...,  119.7825,  -79.9600,\n",
            "        -465.2481])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-224.0828, -565.0087,   61.0341,  ...,   90.2643,  118.3619,\n",
            "          95.1506])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 178.2368,  -42.7747, -187.8474,  ...,  169.2182,   29.3043,\n",
            "         -78.5222])\n",
            "actor loss: 3730.4243204480817, critic loss: 2037235.8515625, entropy: 63121.2138671875, KL divergence: 0.002841596133872201\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[19.370783682910595, 0, 0, 0, 1.8329956900923356], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "54エピソード目の累積報酬：-21887.574031240696, 一つ保全の回数：6528, 二つ保全の回数：1419, 三つ保全の回数：245, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 395.2146,   16.1338, -578.1475,  ...,   28.8422,   48.6628,\n",
            "        -188.3619])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 185.8371,   22.6025, -630.0892,  ..., -210.4416,  155.8574,\n",
            "         219.6151])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-225.8440, -480.5089, -153.9816,  ..., -251.2511, -217.2046,\n",
            "         250.6762])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   96.0259,  -156.7429,  -551.3471,  ...,    71.9724, -1015.0134,\n",
            "         -631.1654])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.1334,  325.4539, -226.0014,  ...,  463.2590,  305.3886,\n",
            "         -88.5388])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-193.5581,   20.8427,  -73.2074,  ..., -244.0946, -163.4300,\n",
            "         139.1266])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-328.1478,  -83.7857,  -12.8429,  ..., -134.2026, -432.7023,\n",
            "        -322.5300])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-241.2548,  -61.7412, -445.4581,  ..., -277.3043,  376.9640,\n",
            "        -653.9734])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -40.8922, -447.3523,  -14.8023,  ...,   56.4605,   42.7711,\n",
            "          33.1504])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-738.1703,   83.7522, -317.6844,  ...,  201.0702,   52.0451,\n",
            "         -49.1991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-231.6523,  124.8592, -176.9751,  ...,   96.5506,   29.0139,\n",
            "        -296.7987])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-289.1898,  357.4675, -129.6586,  ..., -401.2517,  -28.2723,\n",
            "        -263.1064])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.3488,  -15.0017, -929.9589,  ...,  -25.7806, -744.4839,\n",
            "         -11.8776])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  28.8617, -494.5991, -278.0101,  ...,  -65.4652,  252.0301,\n",
            "         -70.6448])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-256.7548, -197.5861,   50.3446,  ...,  -18.9304, -180.2200,\n",
            "          24.1224])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-193.9259, -250.7563, -526.5084,  ...,  208.5656,  -60.3076,\n",
            "        -512.9565])\n",
            "actor loss: 3810.1426073851667, critic loss: 1810163.8046875, entropy: 62735.734619140625, KL divergence: 0.0025616009030241223\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[4.501639296786955, 7.901881899412431, 0, 0, 1.524776767969663], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "55エピソード目の累積報酬：-22896.30519222649, 一つ保全の回数：6491, 二つ保全の回数：1448, 三つ保全の回数：253, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 251.3630, -451.6278, -205.7068,  ...,   42.9849, -109.0388,\n",
            "         -33.8916])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -75.9441,  -247.7375,   157.3174,  ..., -1227.3279,  -406.1933,\n",
            "           78.3551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  82.9721, -501.6391, -195.8788,  ..., -972.9321, -192.6084,\n",
            "          19.8051])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-318.2538,   92.4467,  -23.6767,  ..., -117.0192,  143.0966,\n",
            "         -90.3059])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -21.9200,   135.0010,  -633.3705,  ...,   211.6054,  -188.5404,\n",
            "        -1227.3279])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-153.1655,   40.1654, -140.9641,  ..., -186.8968, -221.2469,\n",
            "          74.9603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-166.0539,   78.8645,  -68.7568,  ...,  122.5142, -229.0323,\n",
            "           6.2872])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.7292, -100.8055,  -86.9668,  ..., -302.2725,  149.3942,\n",
            "        -461.2447])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   1.7872,  168.9834,   72.8100,  ...,  187.6976,  149.4084,\n",
            "        -326.9596])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.9307,   32.0130,   50.8881,  ...,  132.3324, -245.0617,\n",
            "          72.3093])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-103.1300,  104.0722, -242.6622,  ...,   13.8727,   -3.5315,\n",
            "        -280.0451])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-105.5668,   71.1618, -351.2255,  ..., -107.3415, -137.5127,\n",
            "         -92.9327])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   7.5980,  112.9421,   61.6037,  ...,  -69.2413, -593.4429,\n",
            "        -457.4082])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -918.8975, -1717.7648,  -170.0819,  ...,   150.2471,   -75.9876,\n",
            "         -169.3535])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-166.3689, -125.8012, -193.8900,  ..., -299.5153, -456.7020,\n",
            "         250.3937])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 146.6859, -326.4580, -398.9878,  ..., -191.4355,  -36.2913,\n",
            "        -674.8765])\n",
            "actor loss: 3845.7339766355003, critic loss: 1991206.2890625, entropy: 62758.335693359375, KL divergence: 0.002876138082911389\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 7.467290517421715, 0, 0, 2.1970283664927943], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "56エピソード目の累積報酬：-25056.148256185123, 一つ保全の回数：6470, 二つ保全の回数：1493, 三つ保全の回数：229, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.8388, -791.2827, -594.4086,  ..., -290.9332, -588.0392,\n",
            "         251.7506])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -43.4037, -284.2094, -161.6343,  ...,   10.7936,   46.1427,\n",
            "          55.4462])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  63.4883,  116.7595,  -27.6729,  ..., -106.5831,  -35.0902,\n",
            "         -92.2938])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -5.8815, -187.7193,  -34.1400,  ...,  -60.3046, -169.4709,\n",
            "         -10.5499])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-137.0749,   40.5944,  166.7293,  ...,  -59.6930,  -13.2597,\n",
            "          -0.5774])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -85.5743, -328.6317,  -72.2639,  ...,  -17.0463,  -24.5058,\n",
            "        -458.1023])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-106.5819, -174.4834, -171.5534,  ...,    0.3829,   79.2677,\n",
            "        -260.6214])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-156.0520, -343.5081,   56.5774,  ...,  -50.4187, -191.8010,\n",
            "         -68.6472])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-293.2907,  -13.0088, -158.2310,  ..., -102.9513,  -57.1346,\n",
            "         -96.2248])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-913.1901, -624.8616,  285.4074,  ...,    2.1778, -392.3977,\n",
            "           8.3094])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 254.9476, -318.5873, -896.1832,  ...,   13.1894, -501.2829,\n",
            "        -168.9269])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-179.6615,  -86.7525,  -59.8069,  ..., -229.2191, -451.9550,\n",
            "         189.3556])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-224.8483, -243.7852,   24.8383,  ...,  -12.5584,   68.5151,\n",
            "        -219.5657])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 131.3719, -183.9048, -227.7610,  ...,  -63.5680, -727.4514,\n",
            "         -39.2421])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  12.9914,  -82.9475, -787.4067,  ...,    2.1778, -164.2163,\n",
            "          63.2918])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  71.1811,  354.9947, -111.7530,  ...,  187.3595,   20.3790,\n",
            "        -233.9220])\n",
            "actor loss: 3722.577015748717, critic loss: 1898234.640625, entropy: 62678.032958984375, KL divergence: 0.003766189857677811\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[69.15899303495843, 28.117588412392987, 0, 0, 2.1804896484441265], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "57エピソード目の累積報酬：-20358.181621124313, 一つ保全の回数：6487, 二つ保全の回数：1465, 三つ保全の回数：240, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-316.2316,   69.9496,   11.2368,  ...,   66.0162,   93.9756,\n",
            "        -121.3458])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -13.7348,  201.0295, -446.7012,  ..., -116.0366, -181.4267,\n",
            "          38.9420])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 212.3401, -182.7117,    5.9007,  ...,   20.0154, -380.6647,\n",
            "        -241.7288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   5.8339,  467.9005,   60.6254,  ..., -814.5592,   29.6660,\n",
            "        -252.5279])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-303.5629,  225.8213, -213.1552,  ..., -833.5168, -336.6309,\n",
            "        -281.4163])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -588.3645,  -107.1837,  -201.3018,  ..., -1698.5308,    62.0030,\n",
            "          -68.1292])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 634.3179, -124.2224, -414.5596,  ...,  -63.6765,    1.9398,\n",
            "         257.3148])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3.1024e+02,  3.7342e+01, -4.9189e+02,  ..., -1.9174e+01,\n",
            "         1.5300e-01, -2.1951e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -71.2408,   121.8981, -2083.2805,  ...,  -480.0262,    43.4339,\n",
            "         -141.6870])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -5.1858, -488.1371, -449.6649,  ...,  132.2350, -423.5503,\n",
            "         -54.0335])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-144.1994,   72.9133,  -33.6189,  ...,   -7.2620,   31.1514,\n",
            "          -7.2795])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-222.6297,   90.4836, -282.9231,  ..., -371.5226, -131.4575,\n",
            "         -31.8036])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-340.8976,  655.0377,  138.9539,  ...,   -5.0650,  118.4555,\n",
            "        -115.5386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  26.8427,  565.9642, -134.0676,  ...,  -95.2110, -129.7802,\n",
            "        -110.7504])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  234.5256,  -249.4066, -1734.5703,  ...,    75.0338,  -240.4464,\n",
            "          135.6377])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -63.8226, -355.4100, -706.0107,  ..., -164.6262, -197.2287,\n",
            "         840.7941])\n",
            "actor loss: 3544.7503225998566, critic loss: 1801123.0078125, entropy: 62659.486083984375, KL divergence: 0.0035032826528892677\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[9.607451854060768, 2.2045412259092396, 0, 0, 2.3454213511527224], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "58エピソード目の累積報酬：-20749.60500177104, 一つ保全の回数：6483, 二つ保全の回数：1493, 三つ保全の回数：216, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -26.8725, -110.9719,   33.2086,  ..., -154.4080,   84.9360,\n",
            "          39.6937])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.9816,  304.2170, -884.9140,  ..., -153.4824,  -53.2046,\n",
            "          23.8442])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-120.6026,  -64.5085, -785.7225,  ..., -121.9661, -533.2217,\n",
            "        -104.7149])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-176.8419, -142.0183,  205.3653,  ..., -367.1331,   63.2634,\n",
            "        -170.3981])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-273.5905, -265.3828,  -69.4402,  ..., -682.8313,   26.3742,\n",
            "         157.6597])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   0.8171,  126.9373, -365.5078,  ...,   78.1166,  408.5371,\n",
            "        -488.9434])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1035.5441,    56.8804,  -529.7198,  ...,    60.3134,  -305.2837,\n",
            "         -244.4189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.9734, -182.9221, -258.2872,  ...,   55.3375, -271.1676,\n",
            "           8.7314])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-430.1904,  308.0543,   87.1124,  ..., -196.8732,  -44.2201,\n",
            "           4.6808])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-160.2211, -195.9982,   76.5590,  ...,  -69.3710, -374.1172,\n",
            "         190.2879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -32.0073,  -24.9212,  -80.7410,  ...,  216.0437, -329.5583,\n",
            "         109.3610])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 250.5258, -166.1894,   -3.2700,  ..., -142.7512, -167.0109,\n",
            "        -144.1757])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.5663, -496.1517, -245.3408,  ...,  -61.0463,    4.6808,\n",
            "        -212.3533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 492.4817,  120.3525, -829.2610,  ...,  -19.7044,   19.1472,\n",
            "        -217.1373])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   -6.5959,    38.4953, -1064.7939,  ...,    23.3901,    97.7358,\n",
            "         -239.9552])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -50.3134, -155.2012,   95.9057,  ...,  575.9052, -392.2371,\n",
            "         111.1535])\n",
            "actor loss: 3472.970977803636, critic loss: 1525950.0390625, entropy: 62509.39404296875, KL divergence: 0.0023894142406496355\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 0, 0, 1.6498930387498052], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "59エピソード目の累積報酬：-22765.788548306482, 一つ保全の回数：6475, 二つ保全の回数：1540, 三つ保全の回数：177, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -28.0118, -287.3610,  -19.9480,  ..., -377.2724,  -44.5121,\n",
            "        -147.3798])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-291.0996, -251.8891, -132.1667,  ..., -152.4548,  340.9639,\n",
            "          -0.6029])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 138.9972,   55.9606,  137.0934,  ..., -113.8577, -123.7954,\n",
            "         -22.3721])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  31.3001,   20.0898,   71.4006,  ...,  515.9808, -114.1788,\n",
            "          16.4465])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   8.5743,   40.9595,   78.9003,  ..., -638.0538, -126.3893,\n",
            "          83.5697])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-102.9269, -245.5224, -171.4641,  ...,   31.0115, -142.4052,\n",
            "          71.4698])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -2.6464,   38.9691,  -64.9448,  ..., -136.5086, -416.4849,\n",
            "        -514.9744])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-203.2867,   32.7010,  -88.3693,  ..., -184.7829, -230.1512,\n",
            "         173.7066])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -82.2574,  -38.9399, -865.6086,  ..., -344.9443,  -34.8635,\n",
            "          29.7288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -47.9534, -290.4992, -512.7602,  ..., -334.2568, -301.6178,\n",
            "        -661.0439])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  94.5021, -723.9796, -238.3581,  ...,  -50.3116, -185.0009,\n",
            "          41.1757])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-216.4979,   53.5708,  -50.0414,  ..., -288.2062, -164.4699,\n",
            "        -232.9305])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-150.0648,  -22.1287,  116.0575,  ...,  -22.8494,  -71.2404,\n",
            "        -724.8531])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -29.2700,  -14.8152, -147.8970,  ..., -456.9106,   45.1838,\n",
            "          93.2185])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-238.3802,  272.1299, -175.7948,  ...,   46.9070,   77.6143,\n",
            "        -286.8313])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-6.7732e-01, -7.7051e+02, -8.4646e+01,  ..., -1.3453e+02,\n",
            "         1.3128e+02,  1.1275e+02])\n",
            "actor loss: 3397.752655972374, critic loss: 1702848.3046875, entropy: 62113.14599609375, KL divergence: 0.0010848749609476199\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[1.0298488971446238, 8.825093503450528, 0, 0, 2.0662115725813575], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "60エピソード目の累積報酬：-25510.229374526705, 一つ保全の回数：6493, 二つ保全の回数：1525, 三つ保全の回数：174, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-246.7610,  162.6489,  119.5984,  ..., -214.5841, -355.3151,\n",
            "         136.7143])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 160.5779,  -65.8603, -120.9411,  ...,  -48.1216, -226.9923,\n",
            "         129.9577])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -92.9959,  -33.5354, -180.9804,  ...,  158.3960,   49.6069,\n",
            "        -150.5030])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   86.6590,    73.0811,  -166.4032,  ...,  -158.9306,  -188.8185,\n",
            "        -1652.9225])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-9.3600e+01,  1.9320e+02,  7.0667e+01,  ...,  3.6855e+01,\n",
            "        -2.2361e+02,  3.9502e-02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-310.0905, -265.5807, -268.4956,  ..., -503.6723,  -80.1891,\n",
            "         -19.6509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -97.7600,   74.2330, -220.3224,  ...,  186.2836, -202.8601,\n",
            "         -42.2478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-208.6612, -212.3716,  -23.7517,  ..., -276.4873,  104.4095,\n",
            "         -10.4621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   30.3681,   -48.6581,  -560.0016,  ...,  -323.8661,   100.2287,\n",
            "        -1442.1630])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  47.4243, -816.6055,   69.3794,  ..., -441.7670,  -10.7579,\n",
            "         147.3846])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -38.1404, -169.0795, -310.0905,  ..., -450.2697,  166.5450,\n",
            "        -270.8478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  96.8915,  166.3861, -420.7682,  ...,  -30.5197,  183.9996,\n",
            "          33.0858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   0.7856,   40.1022, -356.4457,  ...,  -77.1244, -372.5300,\n",
            "        -276.0192])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-297.1258, -154.2000,   39.7747,  ...,  194.9568,   35.5113,\n",
            "         -75.4439])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -36.8595, -110.4265, -207.5685,  ..., -107.7398,   95.9910,\n",
            "         101.4338])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  72.2325,  110.3079,  -45.8292,  ...,   74.4381,   98.0113,\n",
            "        -254.7189])\n",
            "actor loss: 3627.25315304553, critic loss: 1570828.75, entropy: 61797.3466796875, KL divergence: 0.001121465838694429\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[8.606463607771998, 17.417498449679808, 0, 0, 2.700709611911679], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "61エピソード目の累積報酬：-24956.92435889103, 一つ保全の回数：6459, 二つ保全の回数：1547, 三つ保全の回数：186, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-393.1196,   63.8812,  -55.0658,  ...,   16.9879,  -18.0243,\n",
            "        -300.1993])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 318.2186, -192.5386,  -32.1912,  ...,  123.4696, -437.8392,\n",
            "         118.2519])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 151.3015, -624.6562, -331.5535,  ..., -875.2791,  -90.0089,\n",
            "          93.1867])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-368.0098,   75.9113,  -47.3311,  ...,  -31.8113,   31.4285,\n",
            "          30.4928])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-285.0330,  -15.9285, -143.0621,  ...,  -11.6237,  166.4984,\n",
            "          -0.7544])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -75.2101, -390.3806, -424.3488,  ...,   49.8187, -283.9886,\n",
            "         309.4076])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-312.7596, -223.2592,  117.4365,  ...,   71.1762,  -65.9489,\n",
            "         -32.1551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 118.4224,   -2.5765, -436.7500,  ...,   28.9376,  -30.5209,\n",
            "        -424.1985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-280.8382, -473.3251,   11.0898,  ...,  -71.1937, -426.7639,\n",
            "        -415.2495])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-108.2470,  106.9369,  181.9990,  ...,  -76.7730,  157.6929,\n",
            "         -99.9108])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 31.1472,  44.8492,  48.3021,  ..., -49.2909, 375.4967, -18.7328])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2.6949e+02, -2.2175e+02,  2.2033e+01,  ...,  1.6475e-01,\n",
            "        -4.2520e+02, -2.6104e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1403.0817,  -121.1610,  -358.4902,  ...,  -108.1573,  -156.8085,\n",
            "          -92.7940])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -41.3469, -116.3556,    6.9754,  ...,  246.5343,  541.2928,\n",
            "          68.2314])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-214.1426,  131.0452,   93.1406,  ...,   80.3875,  129.1778,\n",
            "        -764.0389])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -70.0633,  -97.7402,  -80.7309,  ..., -327.2260, -169.5311,\n",
            "         176.4146])\n",
            "actor loss: 3251.2326891454018, critic loss: 1496944.4609375, entropy: 61583.1591796875, KL divergence: 0.002454711067158786\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[19.974638856719412, 12.090749794337968, 0, 0, 1.2576267158459413], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "62エピソード目の累積報酬：-26463.25925935395, 一つ保全の回数：6524, 二つ保全の回数：1497, 三つ保全の回数：171, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -7.3099, -423.6958,  257.5172,  ...,  -97.9907,    2.7424,\n",
            "        -241.2315])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  30.9939,  139.7604,  106.4549,  ...,  -93.7255,  140.7498,\n",
            "        -321.8948])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  25.2268,  -22.6121, -160.5262,  ...,  -43.8069, -459.1871,\n",
            "          33.5941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-289.6940,   55.0307,    1.2078,  ...,   98.0958,   93.1302,\n",
            "          16.4265])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -4.0014, -142.6712,  -94.0120,  ...,   55.9896, -249.4998,\n",
            "         -29.1862])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 300.6625, -178.1058,   27.0712,  ...,  -34.6709,  -69.8257,\n",
            "         -83.3102])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-111.9821, -573.9666,   12.8771,  ...,  158.7863, -268.3222,\n",
            "          14.2737])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-482.7444,   94.6322, -548.2194,  ..., -181.8534,  146.0886,\n",
            "        -100.7144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 57.1319, -48.0208,  93.2553,  ..., 160.8799,  -1.7355,  51.2333])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-159.8207, -116.9169, -162.0161,  ..., -124.7722,  -23.9079,\n",
            "        -231.8627])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-276.6626,  -29.4266,  177.5825,  ...,   96.8956,  -15.3047,\n",
            "          43.5240])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -14.6486, -252.4977,  273.6383,  ...,   60.1660,  -71.9397,\n",
            "         -42.5397])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-92.2471, 415.8585,  28.8626,  ...,  51.7660, 173.8835,  12.6869])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -60.1118,  -23.3853, -205.3527,  ...,   94.5476,  309.5375,\n",
            "          67.0452])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-465.7473,  -26.8213,  -86.9772,  ..., -225.0205,    7.9186,\n",
            "           9.0630])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-105.4964, -443.0306,  139.2332,  ...,   -8.0084,  -57.1829,\n",
            "         151.5888])\n",
            "actor loss: 3159.6972162319335, critic loss: 1963078.83203125, entropy: 61390.023193359375, KL divergence: 0.001651702979106159\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[52.38492750827441, 19.979803912521774, 0, 0, 1.2046588544040429], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "63エピソード目の累積報酬：-22577.33289287758, 一つ保全の回数：6562, 二つ保全の回数：1469, 三つ保全の回数：161, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -1.2413, -198.8226, -127.3025,  ..., -273.8345,  -91.2134,\n",
            "        -101.8891])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -84.4533,  104.4727, -106.6651,  ...,  -44.8752, -527.1952,\n",
            "        -136.5969])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-673.4266, -214.3875,   19.0974,  ..., -477.7389, -336.6413,\n",
            "           7.5828])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-154.8979,   95.8051,  422.0862,  ..., -135.8972,   -8.3282,\n",
            "        -307.7589])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  10.4908, -185.8148,  561.3222,  ..., -556.9252,  -22.0175,\n",
            "         104.5702])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-272.9225,  -97.7320, -231.3537,  ..., -160.7798,  105.3879,\n",
            "         -37.2735])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -83.6352,  -87.6513,   56.8244,  ...,   19.9696, -369.4160,\n",
            "        -258.8982])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-321.4574, -286.8070,   64.5188,  ...,   32.2999,  -69.4315,\n",
            "        -210.5000])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -72.9499, -316.5222, -211.0477,  ...,  -31.1626,  -38.2137,\n",
            "        -159.9466])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  43.7984,  -35.7164, -639.7781,  ...,  157.4827,  -68.3720,\n",
            "         121.9137])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  10.4630,  121.0301, -381.7123,  ..., -202.1521, -149.5719,\n",
            "          75.4434])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 54.9613, -44.8752, -53.6159,  ..., 287.2471,  76.9861, 445.3362])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -100.6032,   226.7357, -1733.5826,  ...,    67.3718,    13.5590,\n",
            "         -280.3407])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-320.2279,  418.3036,  401.0278,  ...,  257.9246, -262.4951,\n",
            "         -27.3612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-318.3430, -651.1172,  -36.6113,  ..., -484.0511,  119.1001,\n",
            "         -21.3540])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-370.6290, -388.1431,   34.8619,  ..., -152.6401,  140.7049,\n",
            "          45.2493])\n",
            "actor loss: 2991.7682542962903, critic loss: 3302274.53125, entropy: 61553.794677734375, KL divergence: 0.004985495760743914\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[82.82004788866902, 51.86443919956217, 0, 0, 1.9166358550644438], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "64エピソード目の累積報酬：-19039.549942147765, 一つ保全の回数：6571, 二つ保全の回数：1491, 三つ保全の回数：130, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-622.7534,    6.6420,  209.5383,  ...,  -79.5025,  -63.2035,\n",
            "        -953.4607])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -31.9079, -224.1703, -200.0111,  ..., -110.8880, -663.4539,\n",
            "         -95.3808])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-281.0150, -192.3836, -322.0449,  ...,  178.6871,   25.2999,\n",
            "        -203.7669])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -41.5410,  118.4234,  342.7372,  ...,  -91.4369, -286.4299,\n",
            "         165.4251])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-260.5756,  427.4183, -136.5495,  ...,  803.5906,  167.1837,\n",
            "        -538.9105])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -47.6506,  -42.3840,  538.6802,  ..., -446.3004, -130.6151,\n",
            "        -124.2889])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -44.7574,  636.4211,  -12.7107,  ..., -884.3518, -113.2333,\n",
            "         -94.1732])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  26.4614,  154.1317,    9.4431,  ..., -168.2635,  180.0432,\n",
            "         -83.8803])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.6224,  -51.4589, -293.7115,  ..., -114.4089,   -5.0819,\n",
            "          12.3054])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  40.0352,   57.0008, -240.4304,  ...,   62.3297, -181.7896,\n",
            "         137.0385])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  38.2246,   -3.7708, -204.7043,  ...,  111.1986,  -23.9096,\n",
            "         121.4429])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-36.8485, -52.6782, 208.1074,  ..., 194.3544, -79.6739, -60.9923])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-102.3260, -410.6996, -192.3578,  ..., -885.9938,   42.0911,\n",
            "         110.8423])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 464.4521,   34.8166,  330.3910,  ..., -355.7921,  -52.6782,\n",
            "        -237.5309])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -15.6328,  141.9263,   55.4336,  ...,  115.3160, -190.7583,\n",
            "        -155.6556])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -96.6157,  -186.9934, -1049.9313,  ...,  -631.2076,   934.6060,\n",
            "          475.1528])\n",
            "actor loss: 3015.788010077792, critic loss: 2629072.4140625, entropy: 61636.11083984375, KL divergence: 0.009915315520065495\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[30.28028294700687, 0, 0, 0, 1.2900144984893243], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "65エピソード目の累積報酬：-22809.712104322745, 一つ保全の回数：6555, 二つ保全の回数：1510, 三つ保全の回数：127, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 217.9899,   46.8971,  -14.1273,  ..., -415.7410, -416.2808,\n",
            "         -68.7422])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-141.6501,   34.9907, -341.9913,  ..., -235.3317, -505.7652,\n",
            "         150.1060])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-359.2662,  -90.4475, -347.0457,  ..., -313.4524,   19.4835,\n",
            "         -26.1212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -216.7172, -1475.1924,   -95.5492,  ...,    98.6786,   -18.9495,\n",
            "          -78.9675])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -67.4852,  228.4730, -456.2868,  ...,  412.1343,   42.9024,\n",
            "         250.7442])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 109.5312,  219.5289,  -60.1223,  ...,  -29.8429, -124.6013,\n",
            "         -95.9623])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-117.0554,  343.3616,  -82.3913,  ...,  115.5694, -293.3521,\n",
            "         316.7155])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-237.3472, -183.3788,  -65.9778,  ...,  -54.8163,  341.6550,\n",
            "        -205.5038])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-147.9553, -216.4553,   -1.8143,  ...,  -15.8593,  -22.1119,\n",
            "        -698.2618])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-225.3671,  137.2036,   52.9475,  ...,   77.3089,  -18.7565,\n",
            "         -76.0632])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 210.9299, -187.3940,  -97.7071,  ..., -728.8983, -162.2014,\n",
            "         -15.5386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  74.4003, -106.2036,  150.1060,  ...,   39.5071,   76.8574,\n",
            "        -261.9477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -32.0156,   93.5030,   41.5133,  ...,   38.3211, -194.5738,\n",
            "          34.8885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-392.8054,  129.9748, -193.9110,  ...,  -83.0569, -226.2877,\n",
            "        -216.5295])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-348.0173,  -15.9907, -197.9104,  ..., -110.1955,   95.8959,\n",
            "         157.0253])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -291.6086,  -130.7619,  -212.4325,  ...,  -181.7362, -1946.7712,\n",
            "         -212.6939])\n",
            "actor loss: 3295.5051953509346, critic loss: 1873117.6015625, entropy: 61314.4619140625, KL divergence: 0.0013319890045741913\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[21.70028899945004, 12.749284998319428, 0, 0, 1.416026322054748], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "66エピソード目の累積報酬：-22378.541843758092, 一つ保全の回数：6500, 二つ保全の回数：1547, 三つ保全の回数：145, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  145.6470,   -95.8419,    74.2140,  ...,   167.7912, -1175.5941,\n",
            "         -231.1005])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-115.3646, -446.4170,  -90.9471,  ..., -329.2977,  224.2224,\n",
            "         186.0776])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  33.8055, -216.8395,   77.2938,  ...,   32.7714,  259.3878,\n",
            "         -14.7551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  79.5483,  146.6287,  259.6781,  ...,  123.3216,  150.7014,\n",
            "        -256.7640])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -91.7325,   -5.7600,  224.2224,  ..., -119.0103,   10.1510,\n",
            "        -389.1783])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-467.5827, -225.6416,  -50.0150,  ..., -177.3958,  -19.1776,\n",
            "        -387.7466])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -70.1609,   83.5637, -120.2762,  ...,   79.8221, -127.1201,\n",
            "         -16.4480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-183.3110, -218.3458, -135.4874,  ...,  -22.4756,  384.4334,\n",
            "         -45.2450])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  62.9667,  170.9940, -286.1556,  ..., -280.5107, -222.6719,\n",
            "        -283.4238])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-261.5367, -273.6142,  260.1911,  ..., -266.6142,  383.7787,\n",
            "         -31.1819])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-352.5164, -548.8307,   69.2615,  ..., -429.7355, -248.6415,\n",
            "        -307.0966])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -94.5383, -122.6636, -141.7277,  ..., -320.8246,  -52.0530,\n",
            "          -2.2206])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  59.9327,  230.2490, -141.2132,  ...,  -55.7794, -285.4896,\n",
            "         -78.5968])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -92.5557, -298.9901,   11.4429,  ...,  -78.5651,   30.5077,\n",
            "          14.2372])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  46.7248,  -47.4928, -223.3021,  ..., -178.0967,  -63.2108,\n",
            "         473.7664])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -28.7542,    73.7828,  -226.7601,  ...,    69.8670,   217.6283,\n",
            "        -1542.7593])\n",
            "actor loss: 2979.4848630772985, critic loss: 1820216.7734375, entropy: 61160.37744140625, KL divergence: 0.0008775937987060168\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[46.08947778439084, 16.131091254018862, 0, 0, 1.1036249805483702], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "67エピソード目の累積報酬：-22523.68077286674, 一つ保全の回数：6523, 二つ保全の回数：1543, 三つ保全の回数：126, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -64.5089, -123.5118,  -46.4051,  ..., -170.7403,   52.0180,\n",
            "         -97.5415])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   98.6807,   168.6693,  -265.3546,  ..., -1239.5018,   129.1227,\n",
            "         -388.6138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -115.7663,   110.0122,   -39.5372,  ...,    70.9409, -2413.8818,\n",
            "           77.0851])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 293.3748, -103.4253,  227.2760,  ..., -212.4901, -183.1451,\n",
            "         -77.3300])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.6454, -119.4376, -170.8070,  ...,  124.5983,  -79.7966,\n",
            "          -6.4443])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-163.4791,  -75.6915,   74.7402,  ...,  126.6056, -107.6778,\n",
            "        -353.1237])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -8.5108, -300.3644,   31.1215,  ...,   83.5030, -331.7526,\n",
            "         -68.3686])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  69.8979, -875.3751,   38.3240,  ..., -106.0244,  135.8264,\n",
            "        -235.7466])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  65.2791,   42.7005, -199.5646,  ...,  -98.1111,  210.8989,\n",
            "        -559.6861])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 131.6190,  -17.8628, -130.0248,  ...,    5.9416, -753.2549,\n",
            "          54.1818])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-111.3609,  -45.1953,  129.3863,  ...,  198.7710, -174.7121,\n",
            "        -598.0859])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -77.2290,  251.8094,  -42.8202,  ..., -100.7289, -104.9081,\n",
            "         100.8463])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -10.8300, -102.9225,  162.4802,  ..., -123.5106,  -65.4418,\n",
            "         -47.3285])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -83.7486,   27.1667,  -93.5510,  ...,  214.3120,  -79.7510,\n",
            "        -122.4066])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-151.1476, -177.0695,   22.5950,  ...,   63.8022, -229.2715,\n",
            "         148.9575])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-244.4860, -172.4949,  155.0864,  ..., -262.8870, -281.3479,\n",
            "         -52.0192])\n",
            "actor loss: 3069.2341658607556, critic loss: 1596052.3125, entropy: 61023.841796875, KL divergence: 0.0007427658630356317\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[17.790834072036336, 49.07743260196385, 0, 0, 1.495483281832712], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "68エピソード目の累積報酬：-21744.472410006434, 一つ保全の回数：6535, 二つ保全の回数：1545, 三つ保全の回数：112, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-349.3558,    4.1235, -140.2762,  ...,  101.6349, -179.0047,\n",
            "        -140.3974])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.2348, -252.8106, -192.8939,  ...,  295.7420, -223.3193,\n",
            "           0.9098])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-351.4708,  -28.8943, -141.5528,  ..., -478.0608, -179.2347,\n",
            "          83.4391])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 146.0309, -135.3902,  -45.4966,  ...,  122.8708,  153.7219,\n",
            "          29.0938])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 39.1801, -16.5003, 112.5935,  ...,  51.8163, 193.9741, -25.7511])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 141.3059,   47.2178,  140.2177,  ..., -100.0292,  -60.8909,\n",
            "        -156.0724])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-123.5778,  -95.1496,  -53.5817,  ..., -334.7724,   58.9713,\n",
            "         -63.0812])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  81.1749,  219.6512,  -62.3591,  ...,  146.5083,    3.5006,\n",
            "        -191.9477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -48.8008, -682.7969,  -92.7398,  ...,  -19.2557,   24.0331,\n",
            "         194.8260])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 75.0216, -18.0209, -81.1102,  ..., 200.0062,  67.3447, 111.2863])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-149.5880,  234.4796,   36.0271,  ...,  -21.4132,   24.2463,\n",
            "          27.9308])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  74.3246,  -77.2259, -229.5610,  ...,  -57.3368,  -46.3267,\n",
            "        -326.3071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -98.8480, -117.6963, -446.3374,  ..., -146.2550,  156.7523,\n",
            "        -212.0224])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 107.8309,   74.3246,   24.9309,  ...,  -11.8817,   99.8650,\n",
            "        -202.2991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -17.2293,  256.0448, -458.2588,  ...,   77.6441,  -57.7997,\n",
            "        -161.1671])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  49.3745, -317.7764,  101.6349,  ..., -147.5672,  159.1207,\n",
            "          81.4943])\n",
            "actor loss: 2859.0976909388214, critic loss: 1503549.09375, entropy: 60691.530517578125, KL divergence: 0.0038282801343760314\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[38.12689851149885, 24.53090987631769, 0, 0, 1.2315654292521572], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "69エピソード目の累積報酬：-19888.058218363374, 一つ保全の回数：6575, 二つ保全の回数：1513, 三つ保全の回数：104, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -34.3626, -381.5551,   26.7983,  ...,  103.6857, -214.0439,\n",
            "        -102.3490])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-180.0529, -287.5390,   56.8299,  ...,  181.6839, -336.9490,\n",
            "         -15.9142])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-103.3869, -509.5970, -153.4973,  ...,  212.0633, -196.7953,\n",
            "         -78.5203])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -25.8296,  -17.6604,  121.9588,  ...,  -17.3775, -193.8708,\n",
            "         163.4906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.7867,   13.5530,  -24.9587,  ..., -757.3882,   97.5582,\n",
            "        -433.5982])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-150.9614,  310.4281,  -83.4235,  ..., -143.1338,   66.6371,\n",
            "         -60.3450])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -53.5156, -223.6899,  -68.5303,  ...,  -91.0983, -230.8822,\n",
            "         -11.5201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-158.6523,  -69.0119,   31.0572,  ...,   87.9955, -471.6668,\n",
            "         119.4307])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-122.9987, -241.1585, -267.9309,  ..., -339.7905, -241.8552,\n",
            "         -56.4355])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 107.2553, -842.9080,   15.4946,  ...,  102.2551, -800.0369,\n",
            "        -107.0304])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-227.0052,  -61.3941,  -22.0997,  ..., -461.0665,  -76.6508,\n",
            "         129.7401])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-134.8485,   17.6745,    8.0932,  ...,   16.6156,   49.6040,\n",
            "          44.5019])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-210.8644,   63.3759,  -53.6863,  ...,  -18.4655,   29.1116,\n",
            "        -620.9092])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  24.0514,   76.3289,   -8.9735,  ..., -108.1488,   99.3647,\n",
            "         -12.7531])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  69.0861,  -12.9456,  154.4975,  ..., -139.8200,   25.9870,\n",
            "          61.1578])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-328.4279,  -27.8109, -117.3540,  ...,  -91.9257,  -19.0637,\n",
            "        -111.9672])\n",
            "actor loss: 2907.353421867076, critic loss: 1795171.6875, entropy: 60208.64111328125, KL divergence: 0.0038823985255316534\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[28.75771928794824, 6.5899271389015865, 0, 0, 1.7031685711299076], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "70エピソード目の累積報酬：-27681.57467485969, 一つ保全の回数：6590, 二つ保全の回数：1489, 三つ保全の回数：113, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([202.1941, 103.2334, -18.2869,  ...,  22.5316, -31.7263, -35.0850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-181.1250,   51.6587,   53.4541,  ..., -182.6082,   43.5801,\n",
            "         144.3288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-565.7374, -171.5634,    6.7247,  ..., -272.0734, -138.8418,\n",
            "          69.3300])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-577.1699,  110.8471,  -84.8752,  ..., -179.7018,   69.3723,\n",
            "         -44.4263])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 270.8037, -184.1363, -128.1002,  ..., -267.4156,  486.6710,\n",
            "        -624.2197])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -28.6364,   28.3410, -141.8012,  ..., -119.5396,   95.7887,\n",
            "         -35.7811])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-139.7943,  750.9456,   17.3836,  ...,   86.7714, -181.6624,\n",
            "         211.5205])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -90.2036,  341.9735, -131.1734,  ...,  144.1542, -267.0450,\n",
            "          92.6661])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-433.8588,   69.3300,  -79.6132,  ...,  -60.4814,  169.4801,\n",
            "          -8.2595])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -14.2285,    21.1472,   -14.9902,  ..., -1446.5872,  -231.1194,\n",
            "          190.9972])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-123.9174,   77.1727,  150.2608,  ..., -152.1836,  175.8711,\n",
            "        -181.7900])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-252.0850,  -58.5922,  -91.2051,  ...,    9.6982,  131.4331,\n",
            "         -87.1105])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  37.5835,  -31.2631, -113.2739,  ..., -249.3312, -179.2260,\n",
            "        -114.6068])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  23.5640,   40.8575,  -60.5904,  ...,  -34.7395, -249.6898,\n",
            "         295.7259])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-146.9811,  -72.5080, -220.1072,  ..., -195.5063, -209.2659,\n",
            "          66.6130])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([20.3421, 51.3613, -3.1798,  ...,  6.7782, 51.2357, 41.9308])\n",
            "actor loss: 2745.4507888808594, critic loss: 1983893.140625, entropy: 60174.854736328125, KL divergence: 0.0004702589670947313\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[69.89039610768249, 28.063406450595387, 0, 0, 1.894312447176111], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "71エピソード目の累積報酬：-20857.365750052602, 一つ保全の回数：6634, 二つ保全の回数：1446, 三つ保全の回数：112, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -528.1846,  -278.7422,  -224.4880,  ...,  -223.9973, -1065.4214,\n",
            "           68.9909])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-201.6272,  -56.6277,  -34.3792,  ...,   96.6434, -439.6312,\n",
            "         -86.6469])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-176.1990,  -62.6211,   25.3797,  ...,  -70.7006,  -65.9313,\n",
            "         146.1658])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-23.0015,  93.1984,  47.3679,  ...,  11.5944,  26.5211,   6.7941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([1007.7910,   77.2127,   66.8366,  ...,  340.9954,   50.0393,\n",
            "         -26.1834])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([155.1481, 309.1412, 114.5558,  ..., 167.4803, 169.7552, -71.7824])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([366.7518,  35.7020, 147.3860,  ...,  35.7578,  85.1650, 143.7445])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.3406,  -17.5631, -112.4060,  ...,  -80.6376, -162.5644,\n",
            "         -49.9105])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 193.6104, -235.0518,   45.5548,  ...,  617.8303,  -86.7761,\n",
            "          63.9455])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-124.4102,  297.6857,   54.8969,  ...,  -38.7952,  -31.7595,\n",
            "          -6.3484])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  47.9987,  -30.4724, -139.3064,  ..., -349.9429,   51.6254,\n",
            "        -874.3645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   46.5783,   157.4880,   134.8026,  ...,   -15.3714, -1051.9602,\n",
            "         -528.1846])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.2622,    3.0361, -140.7379,  ..., -158.4323, -114.8115,\n",
            "         -31.7885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-201.9608,   40.6543,  -68.2911,  ..., -427.1887,  271.9009,\n",
            "        -153.4708])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-203.8969,   68.0899,   93.1984,  ..., -167.3855,   81.2899,\n",
            "        -540.1127])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-24.3830, -41.8099, -94.9159,  ..., 286.9908, 251.1103, -85.9367])\n",
            "actor loss: 2844.2601372361237, critic loss: 2200523.4296875, entropy: 60570.454345703125, KL divergence: 0.004735627091279113\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[44.6296966678795, 25.231361561602633, 0, 0, 2.1289507748401295], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "72エピソード目の累積報酬：-24582.618135710025, 一つ保全の回数：6625, 二つ保全の回数：1491, 三つ保全の回数：76, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 165.5969,  405.7082,  142.9952,  ...,  233.0677,  194.1230,\n",
            "        -159.3539])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 448.3319, -192.3406, -278.3702,  ...,  154.7920,  173.0773,\n",
            "        -393.2172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-106.1244,   17.3825,  -30.8661,  ...,  -96.5633,   -4.0806,\n",
            "        -352.6591])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 123.0723, -120.2824, -189.5873,  ...,  180.8204, -367.3914,\n",
            "        -119.5155])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-176.8393, -108.4394, -293.6856,  ...,   91.7662,  412.0030,\n",
            "         124.7753])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 89.5736, -41.7703, 100.9044,  ..., -61.2669, 257.5908, 148.5452])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-148.6275, -108.1465, -437.9174,  ..., -289.8225,  -75.4346,\n",
            "         194.2991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-221.2222,  -73.1621, -194.3207,  ...,  -13.6119,  130.4721,\n",
            "          62.3835])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   2.5579, -347.4748,   48.2057,  ..., -169.2395,  -62.1030,\n",
            "        -242.1943])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  63.9496, -172.4150,  135.5792,  ..., -114.3660, -302.9723,\n",
            "           2.5868])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-198.1191, -241.3011, -183.3485,  ..., -155.9094,   56.0512,\n",
            "        -151.0294])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-40.5742,  -3.7073,  27.0805,  ..., -18.5438, 186.2786,  46.3601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.1751, -409.4286,  -46.3249,  ...,   75.5033,  392.9346,\n",
            "         281.6753])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  71.8378, -172.1714, -110.1714,  ...,   68.3281,   30.2295,\n",
            "         256.8721])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-38.2288,  88.7700,  43.5615,  ..., 151.3412, -71.1295, -61.7865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.0320, -243.3066, -155.7087,  ..., -192.8135,  -17.3741,\n",
            "         183.9996])\n",
            "actor loss: 2493.441876819059, critic loss: 2457702.0234375, entropy: 60583.126953125, KL divergence: 0.006755285120312185\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[15.888573572914407, 0.35775788339551945, 0, 0, 1.2928857463384051], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "73エピソード目の累積報酬：-20345.258218218372, 一つ保全の回数：6614, 二つ保全の回数：1496, 三つ保全の回数：82, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([138.7245, -53.4737,  86.6739,  ..., -56.7570,  77.4374, -82.1546])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 157.4572,   85.7248, -505.5175,  ...,  -14.2755, -131.2439,\n",
            "         -65.3125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.0749e+00, -1.4560e+02, -1.3864e+02,  ..., -3.3843e+02,\n",
            "         1.1071e+02, -1.8616e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-132.5508, -182.1441,  116.1418,  ..., -306.5761, -462.7543,\n",
            "         -25.7089])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 154.9163, -174.2188,  143.0014,  ..., -678.4655,  -65.2687,\n",
            "        -316.7606])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 110.5040,  114.2128,  128.1625,  ..., -153.6841,  -69.2571,\n",
            "         253.9607])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -10.8070, -342.3476,  455.6593,  ..., -183.5481,  399.0305,\n",
            "         248.5290])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -6.4946,  646.0444,   51.3851,  ..., -156.0649,   52.1537,\n",
            "        -247.0866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -24.0727,  228.9924, -292.5269,  ...,   70.1024,  -41.7433,\n",
            "        -310.4153])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1589.6519,  -820.1669,  -102.4501,  ...,    67.8025, -1095.1569,\n",
            "         -104.4204])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-543.8668, -722.8047,  224.8442,  ..., -212.6814,  272.9623,\n",
            "        -378.3172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -59.5910, -148.4282,  164.5136,  ..., -351.5122, -178.5930,\n",
            "          72.4204])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 203.5621,  -58.2226,  166.5547,  ...,  301.7157, -641.3692,\n",
            "         193.5660])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 118.1773, -296.3437,  253.6359,  ...,  -33.1130,   39.4160,\n",
            "        -343.2836])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-119.4996,  186.1308,   85.7352,  ...,  348.7675,  161.3827,\n",
            "        -362.4416])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-292.5525,   13.7103, -260.3628,  ...,   50.5677, -145.8276,\n",
            "        -214.4396])\n",
            "actor loss: 2737.0412332417955, critic loss: 2558706.7578125, entropy: 60664.053466796875, KL divergence: 0.010677076645287071\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[37.12931219497492, 57.923502709482335, 0, 0, 1.3494992280874332], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "74エピソード目の累積報酬：-22860.54932644582, 一つ保全の回数：6561, 二つ保全の回数：1546, 三つ保全の回数：85, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -4.9932,  227.7003,  -31.1393,  ..., -408.5357,   61.1424,\n",
            "         -80.9437])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-141.2828, -193.6945,   67.1145,  ..., -126.0283, -335.0026,\n",
            "         241.8914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-875.0123, -237.6853, -544.7145,  ...,  -42.2853,  -50.8628,\n",
            "          60.4804])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-332.8069, -216.9372,  204.4180,  ...,  131.4894, -196.0434,\n",
            "        -156.4703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 133.5899,  339.4711,  -54.1463,  ...,   85.3909, -652.0140,\n",
            "         -11.2045])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-385.1966, -334.0653,  -29.5136,  ..., -303.5923, -129.9722,\n",
            "        -196.6985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  37.6955,  -73.6796,  -59.5967,  ...,  107.1588,  230.1630,\n",
            "        -427.6234])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-276.4584,  601.3286,  129.0301,  ..., -708.3331,   12.3622,\n",
            "         -37.8125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 801.9294,   69.1634,  -49.9258,  ..., -257.7404,   67.6038,\n",
            "        -103.5120])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.3774, -358.6428,  -73.8632,  ...,  175.8849,  -77.9963,\n",
            "          18.9330])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 54.7597, 167.6009, 122.7816,  ...,  -0.7905,  22.1931, 112.4918])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -45.7710,   73.1339,  526.0217,  ...,  -47.4761,   18.3217,\n",
            "        -238.2983])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-206.8603, -470.6802,   45.4859,  ..., -283.6316, -148.2017,\n",
            "         128.0850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 155.7024, -375.3351,  155.3649,  ...,  -30.4577,   64.1210,\n",
            "        -409.6825])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-237.6853,  -71.2489, -645.9150,  ..., -131.3256,  134.7024,\n",
            "        -244.6917])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.7548,  -83.5658,  -49.9404,  ..., -297.5820,   87.5428,\n",
            "        -230.6475])\n",
            "actor loss: 2734.409947090282, critic loss: 1889028.5, entropy: 60841.0615234375, KL divergence: 0.00786479197998916\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0.7273367189262696, 0, 0, 1.2690229163655247], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "75エピソード目の累積報酬：-24866.534585932255, 一つ保全の回数：6511, 二つ保全の回数：1584, 三つ保全の回数：97, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-141.6465, -179.8695, -188.8736,  ..., -336.9175, -205.1794,\n",
            "         102.7939])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   4.9389, -338.4398,  192.1455,  ..., -140.2895, -167.0216,\n",
            "        -108.3786])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-244.6636, -185.7825,   68.0515,  ..., -368.5992,  -61.2526,\n",
            "        -343.7321])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 103.5871, -225.7256, -333.6748,  ...,  -26.7983,  117.5919,\n",
            "        -112.7551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.2864,  140.4594,   80.4295,  ...,  -42.8188,   -6.5575,\n",
            "        -538.9250])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  45.5805, -160.0047,   13.7242,  ...,  145.2458, -110.2698,\n",
            "        -244.6935])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-936.5745,  197.2515, -296.3087,  ...,  203.2490,   35.9490,\n",
            "          60.7576])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-426.6737,   62.2685,  -21.5171,  ..., -402.6553,  -22.0206,\n",
            "         -22.1648])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  50.0276, -191.3995, -157.2233,  ..., -100.0873, -232.4979,\n",
            "          -4.4763])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-142.2022,   -9.1512, -263.5164,  ..., -195.2823, -265.8371,\n",
            "         186.3649])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.1439,  127.3668, -456.7604,  ..., -166.5376,  -82.6470,\n",
            "         -99.5619])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -62.7803,   54.7104,    9.4936,  ..., -468.1271, -130.3929,\n",
            "        -429.9343])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-189.5127, -443.6602,   -1.0017,  ..., -120.7163, -110.0046,\n",
            "          65.7788])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -1.0718, -337.6201,  698.7848,  ..., -495.4239,  100.4459,\n",
            "          25.6269])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 141.9906,   90.5483, -463.9471,  ..., -142.5868,  -19.1743,\n",
            "          51.3893])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 339.1012, -248.0672, -595.0703,  ...,  -97.0259,  -62.1951,\n",
            "         257.7672])\n",
            "actor loss: 2589.3130983165693, critic loss: 1882319.2265625, entropy: 60823.206298828125, KL divergence: 0.004781936497863273\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[74.27402848341896, 0.20489018691117333, 0, 0, 1.157350535330389], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "76エピソード目の累積報酬：-22525.938654018817, 一つ保全の回数：6516, 二つ保全の回数：1600, 三つ保全の回数：76, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-400.9605,  218.7982,  186.9103,  ...,  -73.5093,  112.4553,\n",
            "         -40.0783])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 317.4637, -257.0761,   62.9195,  ...,  -15.0501,  164.0089,\n",
            "         -69.6438])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  33.8204,   12.7868, -101.4872,  ..., -265.6545,   37.3315,\n",
            "          11.9107])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-211.7858, -773.8317, -280.7251,  ...,   19.5357, -124.3443,\n",
            "         -92.1780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  80.3178,   27.5741, -232.8795,  ...,  391.1908,   32.7471,\n",
            "        -302.5669])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-330.9306, -131.3759,  -43.1277,  ..., -155.2936,  546.3468,\n",
            "         114.1503])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -82.7995,  -28.9858, -223.0115,  ...,   91.2174,  210.7878,\n",
            "        -145.9408])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  60.4269,   55.7897, -454.2213,  ...,   12.7868,   59.1892,\n",
            "          99.9133])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -90.6942,  -50.2736, -534.9973,  ...,   90.1502,   31.1689,\n",
            "          20.4110])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-191.4867,  617.7649, -220.9949,  ...,   13.0754,  158.4312,\n",
            "        -490.5883])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -69.6407,  240.8924, -219.0076,  ..., -382.6849,  -70.2726,\n",
            "         -51.9865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -54.4468, -491.4598,  -93.4976,  ...,  108.4786,   52.9881,\n",
            "         -84.9186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -79.5107,  -53.4556,   -5.4319,  ...,  -30.6766, -163.0090,\n",
            "         -16.7601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-317.2593,  -82.5970, -160.9371,  ..., -222.9285,   56.3964,\n",
            "        -119.0863])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-354.3065, -387.5769,  -35.2044,  ...,  200.9605,   48.4441,\n",
            "         354.8039])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -15.0501, -294.7650,  389.9113,  ...,   84.8045,   43.8556,\n",
            "         163.1042])\n",
            "actor loss: 2810.9618964540978, critic loss: 1963407.734375, entropy: 60720.872314453125, KL divergence: 0.0024792970886371125\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[43.21427946510551, 12.648887285665653, 0, 0, 1.4057941790926256], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "77エピソード目の累積報酬：-22624.91653168549, 一つ保全の回数：6456, 二つ保全の回数：1636, 三つ保全の回数：100, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   5.8216,  -74.6682, -269.0132,  ..., -177.0331,  114.9927,\n",
            "          82.4999])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-234.6142, -141.6178, -300.0208,  ...,   79.5411,   97.1594,\n",
            "          82.6413])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-291.1954,  -47.1955,   75.3381,  ...,   45.4990, -135.3469,\n",
            "         -79.9761])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 159.8364,  281.6784, -389.6241,  ...,   55.8047, -101.2513,\n",
            "          34.8735])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-185.9232,   58.2913,  -36.6650,  ..., -147.0802, -275.7276,\n",
            "        -169.1621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 11.7827, -84.2208,  36.4301,  ...,  93.7082, -86.1163, 133.4017])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-190.5612, -438.5044,   18.1106,  ...,  -93.3640,  107.0758,\n",
            "         -19.6026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-234.5760, -259.2176, -113.1165,  ...,  125.5310, -613.8881,\n",
            "          35.6121])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-246.4355,   90.7515, -454.4796,  ..., -137.8913, -154.4376,\n",
            "         -31.1560])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -74.7665, -107.9232,  267.9863,  ...,   -9.3233,  364.1967,\n",
            "          38.2286])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-312.6594,  146.7230,  -50.6129,  ..., -269.7757,  -26.0101,\n",
            "        -398.9885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -7.6537,   62.2429,   19.1253,  ...,  113.0279, -332.9793,\n",
            "           5.0336])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  83.6938, -282.1724,  -79.6978,  ..., -145.1622,   24.7072,\n",
            "         252.7478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.7359,  -48.0073, -140.0265,  ...,   31.3200, -179.7343,\n",
            "        -568.1575])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  133.7002,    60.0832,    64.3162,  ...,    26.6029,  -663.0101,\n",
            "        -1476.6049])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-229.2266,  -67.0700,  -99.4373,  ..., -362.5165, -989.4921,\n",
            "        -573.8434])\n",
            "actor loss: 2780.7874242365115, critic loss: 1416486.62109375, entropy: 60436.571044921875, KL divergence: 0.002153478586398475\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[14.460702965374725, 12.771806746405712, 0, 0, 1.1730734388166317], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "78エピソード目の累積報酬：-24660.538244237076, 一つ保全の回数：6463, 二つ保全の回数：1646, 三つ保全の回数：83, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-134.5962, -169.6486, -664.7651,  ..., -102.5464,   44.1041,\n",
            "          28.5509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-617.6763,   94.7218,    3.6057,  ...,   32.2156, -122.9366,\n",
            "         212.7712])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  12.9598,   80.7251, -167.7790,  ..., -118.5539, -457.1734,\n",
            "        -240.1571])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-404.9367,  132.3909,   80.0296,  ...,  -54.9859, -195.9707,\n",
            "         280.2662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-110.7087,  136.6789, -143.9767,  ..., -428.3178, -288.6256,\n",
            "         232.7672])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 28.5294, 243.9116,  40.3634,  ...,  83.1716, 319.8640, -48.8813])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -61.2126,   66.3541,  -19.7993,  ...,  129.5069,   46.4425,\n",
            "        -105.8892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -86.3486,    12.8456,  -109.0806,  ...,  -177.2248, -1405.5341,\n",
            "          -42.1424])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-532.2008, -140.1289, -262.6143,  ..., -182.0211, -158.3004,\n",
            "        -241.1448])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -56.9171,   -5.9670, -158.6861,  ..., -160.8867, -666.1251,\n",
            "          42.3176])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  14.5488, -139.3021,  176.9301,  ..., -194.3849,  234.9383,\n",
            "         -10.9234])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -72.6105,  -40.5377,  293.9781,  ..., -220.1588,   28.6004,\n",
            "         -54.8465])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-286.3108,  201.7384, -242.0782,  ..., -772.2406, -104.2102,\n",
            "         211.5059])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-122.4343,  -56.9171,   48.1362,  ..., -192.2805, -161.2006,\n",
            "         -12.0935])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-277.8604,   20.2748,  163.0385,  ..., -226.1535,   79.3086,\n",
            "         162.1886])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([    8.6518,    55.1707, -1031.3824,  ...,   -20.1466,   -55.1907,\n",
            "           19.2793])\n",
            "actor loss: 2726.9837054301206, critic loss: 1477259.14453125, entropy: 60168.61572265625, KL divergence: 0.005271790310205588\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[2.9649344319040907, 10.077922134979495, 0, 0, 2.1919988571086617], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "79エピソード目の累積報酬：-20997.22990589221, 一つ保全の回数：6483, 二つ保全の回数：1630, 三つ保全の回数：79, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -58.1884, -502.1719,  121.5202,  ..., -123.7477,   90.9762,\n",
            "         -90.1352])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   3.5888,  -71.3047, -248.3883,  ...,   45.4904,  -49.6281,\n",
            "        -272.0158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 74.0036, -67.7431,  85.9543,  ...,  27.6011, 139.3713, -99.0809])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-490.8052,   -4.0920,   99.4284,  ...,  278.0521,  107.9453,\n",
            "         -13.0929])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 174.9932, -805.8998, -212.9704,  ...,  253.5225,   -7.4574,\n",
            "         -23.2839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-130.5101, -213.4457, -274.5275,  ...,  -41.1831,  -65.9528,\n",
            "         127.9060])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   96.6645,   -19.6940,   -15.1774,  ...,    45.2630, -1460.6893,\n",
            "         -164.9794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -132.2089,  -154.2105,    19.0887,  ...,   103.3703, -1039.5762,\n",
            "          -90.6768])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-423.4760,   -7.3685,   25.4645,  ..., -338.2729, -100.8631,\n",
            "         191.8597])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  25.7362,    8.4388,   67.4089,  ..., -370.2492,   -6.9380,\n",
            "        -349.3934])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-106.2740,  142.5130,  -50.0558,  ...,   56.9678, -169.3551,\n",
            "         162.6794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   29.3444,   140.7595,     1.5461,  ...,  -854.5234,  -187.7140,\n",
            "        -1242.4963])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 122.9667, -196.4252,   12.0669,  ...,  -89.3896, -246.8198,\n",
            "         -81.8639])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-307.8249, -329.7042, -534.9373,  ...,  -44.3083,  264.1781,\n",
            "         102.5886])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  26.4492, -140.8520, -181.9326,  ...,  127.0955,  -81.8597,\n",
            "          67.0524])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.8494,    8.3620, -189.7153,  ...,   -8.0022,  -14.0249,\n",
            "         -35.4030])\n",
            "actor loss: 2408.310982906381, critic loss: 1576857.203125, entropy: 60117.83935546875, KL divergence: 0.0009638832994804548\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[59.392284875770294, 36.18193821493379, 0, 0, 1.1490023788228334], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "80エピソード目の累積報酬：-20773.190783154812, 一つ保全の回数：6554, 二つ保全の回数：1555, 三つ保全の回数：83, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-127.4767, -201.4129, -214.0724,  ..., -171.1145,  114.4118,\n",
            "         -52.6360])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 157.5093,  -35.7368, -152.0754,  ...,  205.9500, -202.1724,\n",
            "         136.3426])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-173.1650,    5.9980,   -6.5709,  ...,  -62.5478,  115.6821,\n",
            "         213.4022])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-301.7567,    8.0496, -151.3827,  ...,  268.4348, -572.1031,\n",
            "          18.0241])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   7.5879,  -73.1790,  -94.9082,  ...,  168.1378, -114.6243,\n",
            "        -331.1830])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1956.7717,   157.7544,  -202.7485,  ...,    77.0695,   -24.5194,\n",
            "         -692.7715])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -57.2501,  167.2575, -149.6805,  ...,   25.0918,  169.0029,\n",
            "         190.2782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  13.1259, -242.5733,  -67.4198,  ...,  -13.8130,   56.7752,\n",
            "         -36.7160])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.1148,   74.1470,   80.8890,  ...,  136.5326,  -56.5368,\n",
            "        -207.8645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-77.4714,  31.3100, -25.0752,  ...,  14.0806, -22.1696,  84.0177])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-201.7571,   29.8252, -252.1358,  ...,   69.1717, -612.3103,\n",
            "         -87.9933])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  52.8882,  -73.1790,  -27.2970,  ..., -325.9301,  -25.4959,\n",
            "        -171.7122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-183.9121,   97.6123,  101.9468,  ...,  -66.7696, -165.4011,\n",
            "         -89.0684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  67.8984,   34.0899,   73.5641,  ..., -390.4007,  148.5281,\n",
            "          21.0509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -109.1772,   -33.2948,  -183.9601,  ...,  -300.0508, -1405.9365,\n",
            "           81.0691])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -62.9646, -148.6349,   23.0532,  ..., -192.1601, -120.8014,\n",
            "        -160.0590])\n",
            "actor loss: 2698.775295216067, critic loss: 2018179.796875, entropy: 60119.109130859375, KL divergence: 0.00035593735809569824\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 3.8281238752635067, 0, 0, 1.7404286099498436], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "81エピソード目の累積報酬：-23738.015460517923, 一つ保全の回数：6546, 二つ保全の回数：1558, 三つ保全の回数：88, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 259.2870, -336.9511, -156.4392,  ..., -166.9529,  165.5723,\n",
            "          72.2679])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 151.0353, -551.3396, -253.9958,  ...,    4.8547, -394.9919,\n",
            "          76.0459])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 180.6781, -284.4046, -565.9402,  ...,  154.8361, -173.7457,\n",
            "         -27.9175])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -9.5425,   49.6517, -221.2634,  ..., -355.1276,  -46.3846,\n",
            "         -49.2354])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([236.5276,  47.5698, 251.5641,  ..., 145.7365, 247.6442, -22.3655])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  82.7755,   89.8654, -150.4134,  ...,  253.8279,  -27.3369,\n",
            "        -166.4545])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -27.9175, -132.8680,   74.5227,  ...,   18.4643, -152.3051,\n",
            "        -426.3449])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-203.8678, -207.3815, -132.9721,  ..., -394.9919, -102.2003,\n",
            "         -14.8079])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-177.8097,  149.6173,  186.5638,  ..., -151.3898, -136.0768,\n",
            "         -93.0065])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -13.0714, -158.5649,  -56.4023,  ...,  -45.7118,  -14.6560,\n",
            "         -27.0056])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-188.5082, -278.3344,  -81.9314,  ...,   99.4852,  131.9966,\n",
            "         327.2645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 134.1722,  100.3074, -120.6430,  ...,   30.5839, -817.9014,\n",
            "         156.3205])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4.8138e+02,  2.2785e+02, -4.3483e-01,  ..., -9.6651e+01,\n",
            "         1.4166e+01,  1.1415e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  54.1970, -161.7165,  -87.8932,  ..., -401.7408,  108.9823,\n",
            "          10.8585])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.9283,  103.9778,  149.8929,  ...,   30.3037, -330.6770,\n",
            "         104.6041])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-190.9649,  -73.2042,   93.7502,  ..., -212.0721,  -11.9919,\n",
            "         -28.6610])\n",
            "actor loss: 2359.051943815142, critic loss: 1520009.1875, entropy: 60155.414794921875, KL divergence: 0.0003046307875027104\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[20.11815544551878, 4.341088390319449, 0, 0, 1.1136246093472666], 離散行動：[0, 0], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "82エピソード目の累積報酬：-23564.70506220424, 一つ保全の回数：6566, 二つ保全の回数：1564, 三つ保全の回数：62, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 219.1415,   71.0708, -193.2244,  ..., -424.8867, -236.9927,\n",
            "          -8.9206])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-424.8892,  101.5282,  -55.4368,  ..., -321.4923,   13.1829,\n",
            "        -153.3826])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  28.2200,  -31.9060, -110.5257,  ..., -124.8035, -230.9550,\n",
            "          65.9211])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-17.1604, -36.8797, 100.2260,  ...,  63.6154,   8.1624, -64.6054])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   8.6676,  -84.3887,  215.7655,  ..., -148.4403,   -3.8771,\n",
            "        -159.8488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-555.5532,  -78.9759, -176.1553,  ..., -155.9317,   33.2958,\n",
            "         -31.1896])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.6373,  110.4334, -114.5961,  ..., -384.0099, -244.2785,\n",
            "        -286.5365])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  33.5543,  -45.1745, -199.9343,  ...,   44.3845,  147.3301,\n",
            "          94.5318])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-509.2067, -751.0662,  -27.0593,  ..., -239.7116,  -74.2690,\n",
            "        -203.8849])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-798.0899,    4.3240,   -5.1660,  ..., -209.1709, -263.2254,\n",
            "          -2.6016])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-441.7711,  157.6661,  275.0048,  ...,   -7.3435,  -39.1010,\n",
            "          32.3906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -71.2450,   21.9052,  -65.5522,  ...,   88.2748, -202.6805,\n",
            "        -244.2183])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 698.3747,   17.2262, -330.3859,  ...,   -7.8509, -190.2689,\n",
            "        -335.6365])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 662.6722, -132.0068,  -82.4267,  ...,   90.9251, -159.6907,\n",
            "         132.5866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-11.4143,  54.7198, 137.5329,  ..., 109.7685,  43.1211,  42.0164])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  86.7270, -211.3009, -642.6528,  ...,  -72.9659,   28.5923,\n",
            "          28.6382])\n",
            "actor loss: 2464.071603948949, critic loss: 2461247.78125, entropy: 60091.607177734375, KL divergence: 0.0003892679814485635\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0.0008334227521663894, 0, 0, 1.1619348796819775], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "83エピソード目の累積報酬：-20962.77881779239, 一つ保全の回数：6522, 二つ保全の回数：1607, 三つ保全の回数：63, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-815.2362, -288.8347, -139.9184,  ..., -131.3173,  -82.5769,\n",
            "        -580.1707])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 345.6800,   47.8099,  -60.3697,  ...,   43.1259, -103.5357,\n",
            "          77.2532])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  64.7952,  190.2317, -128.8192,  ...,   27.6364,  230.1255,\n",
            "         -23.2191])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.1622,    2.3756, -333.4286,  ...,  -23.6842, -132.1406,\n",
            "         -31.1413])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  201.7146,   288.6490,    20.5730,  ..., -1552.3972,  -193.5125,\n",
            "          125.9171])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-158.4535,   92.3310, -260.5999,  ...,  -38.0436, -399.9799,\n",
            "         110.4885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 211.2076,  399.6166,  171.3209,  ..., -115.6256,  186.4621,\n",
            "          41.4613])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-116.8939,  109.2109,  -60.1750,  ...,   34.3964,   -9.1740,\n",
            "        -351.3694])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  76.9492,  121.2165, -126.5475,  ..., -405.0316, -111.2890,\n",
            "          77.5617])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-197.0193, -344.0701, -262.1599,  ..., -219.7804,   81.5928,\n",
            "         196.7100])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  54.1177,  -75.7629,   -0.3497,  ...,  -56.4943,   31.8092,\n",
            "        -259.7879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-253.2547, -199.8093, -106.1776,  ..., -164.0001, -259.0312,\n",
            "        -158.4443])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  87.8235,  -74.7657,  -63.6828,  ..., -447.7046, -194.9266,\n",
            "         -13.6762])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-131.6923,   18.1305, -235.4910,  ..., -142.9857, -139.3268,\n",
            "        -333.3200])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 118.3230, -485.5371, -157.7274,  ..., -329.1041,  -10.5987,\n",
            "        -202.3001])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-147.9517,   18.2632,  -70.0395,  ...,  -98.8553,  -13.0087,\n",
            "         -74.9164])\n",
            "actor loss: 2365.285069520438, critic loss: 1510920.625, entropy: 60176.4033203125, KL divergence: 0.0005944011707060588\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[10.770941134228135, 39.86008569211832, 0, 0, 1.1378338246838169], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "84エピソード目の累積報酬：-21173.65785882577, 一つ保全の回数：6562, 二つ保全の回数：1576, 三つ保全の回数：54, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   0.9272, -109.7899,  203.1251,  ...,   88.2553,    6.4391,\n",
            "        -502.4915])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-108.9427,   38.4043,  -55.1834,  ...,  -37.7677,  -69.7040,\n",
            "         227.7524])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -55.8772,  533.0215,  180.1091,  ..., -104.0895,  -17.5139,\n",
            "         119.9899])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-129.9689,  306.0049, -759.2747,  ..., -174.0837,   85.7113,\n",
            "         -44.3139])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([220.0102, 196.1562, 386.6655,  ..., -63.5759, -44.5921, 226.7334])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -1.9909, 151.5059,  72.6896,  ..., 224.0594,   4.9974, -78.3881])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([114.3371, -68.5280, -64.4290,  ...,  43.5287, -65.6070,  61.2582])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   -2.0701, -1092.9371,  -107.3703,  ...,   149.4189,   -62.8467,\n",
            "           71.7098])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -80.6495, -317.8966,   -8.3653,  ...,   19.3859,  204.4278,\n",
            "         271.0389])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 494.6072,  -78.0847,   17.4675,  ..., -178.8282, -179.0231,\n",
            "          47.4925])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-555.8196, -163.5242,  129.5163,  ..., -125.0130, -224.8642,\n",
            "         -87.4922])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -24.6749, -202.7498,  155.5794,  ...,   44.1381,    8.2351,\n",
            "        -180.6508])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -27.8755,  134.9324, -169.6648,  ...,  -47.0707, -674.4897,\n",
            "          27.6908])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 241.9933,  -29.4633, -314.1602,  ...,  168.0790, -231.9072,\n",
            "         -13.5731])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-263.2414, -555.2382, -334.5033,  ..., -259.5586, -157.2177,\n",
            "          28.1486])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -25.6055,  222.9960, -594.8629,  ...,   79.5013,  -40.9502,\n",
            "          97.8819])\n",
            "actor loss: 2359.660279864719, critic loss: 1736291.9609375, entropy: 59918.857177734375, KL divergence: 0.0003848198511397587\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 37.20083588396553, 0, 0, 1.5604099509027298], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "85エピソード目の累積報酬：-26564.886787827723, 一つ保全の回数：6538, 二つ保全の回数：1590, 三つ保全の回数：64, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  48.7855,  125.3661,  430.1698,  ..., -627.1711, -560.2557,\n",
            "         108.8291])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-369.1502,  -81.1457,  -50.6060,  ...,    1.7153,  194.8393,\n",
            "         -39.1115])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -112.8290,   145.5090,  -674.9337,  ...,    90.1009, -1803.7787,\n",
            "         -264.5701])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-105.6128,   30.4380,   -8.9288,  ...,   17.9513,  236.5177,\n",
            "          33.8723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-12.0185, -55.8531,  25.1349,  ...,  -5.3008,  -0.1435,  97.7547])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 577.4482,  -64.0187,  -64.0184,  ..., -339.4633, -440.7782,\n",
            "        -207.4297])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -0.9841,  22.3169, 161.6219,  ..., -89.1747, -63.6421, -36.9924])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-322.4698, -703.7060,   54.2551,  ...,  306.1635, -318.1883,\n",
            "         272.1850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-413.2014,  -89.8090, -195.9871,  ...,  -73.9814,   91.7768,\n",
            "        -238.8358])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 289.7477,   29.8081, -123.0410,  ..., -120.0876, -972.8795,\n",
            "          78.7093])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-104.3762,    4.6740,   69.8070,  ...,  145.1304,  -39.1115,\n",
            "        -376.3536])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  61.0644, -115.2680,  242.9196,  ...,  -79.9493,  -42.8450,\n",
            "         160.1101])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([103.2421, 156.5123, -87.5029,  ..., -72.6604, -47.0168, 380.5164])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   1.8603, -224.9602,  -94.7004,  ..., -188.9186,   61.2687,\n",
            "         -35.5726])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 165.3347, -353.3708, -244.8420,  ...,  122.3021,  306.1635,\n",
            "        -116.2425])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  62.3547,   70.6512,   44.8510,  ...,   42.0634, -312.0074,\n",
            "         -88.6880])\n",
            "actor loss: 2305.105612217274, critic loss: 1867284.3203125, entropy: 59933.294921875, KL divergence: 0.0007482878981039373\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[32.97227753276722, 5.220947917428712, 0, 0, 2.936841675933344], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "86エピソード目の累積報酬：-21535.354368824905, 一つ保全の回数：6550, 二つ保全の回数：1581, 三つ保全の回数：61, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-187.9854,  180.2166, -366.9826,  ..., -158.3687,  -33.1640,\n",
            "         -57.0756])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 198.1218,   -8.4875,  -80.6041,  ...,  -32.7112,  186.4878,\n",
            "        -200.5952])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -3.8641,  105.1704, -254.3272,  ...,  -20.5832,  168.1356,\n",
            "         -19.1762])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  92.6290, -201.5991, -105.1584,  ...,   76.8621,   64.6605,\n",
            "        -105.5900])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  11.9958,   33.9819,  186.4373,  ...,    1.3532, -274.4369,\n",
            "        -209.3911])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-131.0228,  106.2289, -349.9234,  ...,  -96.9116,  102.2358,\n",
            "        -552.7936])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -80.2560,  116.9723, -223.6031,  ...,  -83.3201,  -80.0147,\n",
            "          10.3880])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1342.3900,   145.6508,   252.7243,  ...,   114.9555,   169.4988,\n",
            "           83.4434])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  67.0879,   65.9441,  130.5382,  ..., -383.9704,   61.9166,\n",
            "        -385.1623])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-448.2081, -357.0359, -213.1414,  ..., -344.1155,   75.8663,\n",
            "        -337.8594])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 269.7494,   10.9784, -296.0117,  ..., -501.7926,   41.1055,\n",
            "        -175.5679])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  31.6344, -133.2413, -256.4542,  ...,  -20.4099,  131.9373,\n",
            "           1.7496])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-927.1598, -201.6757,  -48.6794,  ...,   -9.9848, -339.5494,\n",
            "         179.7899])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -24.4050,   -20.8548,    98.1367,  ..., -1454.5286,   200.5583,\n",
            "          108.0417])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 117.2504,   35.4159, -621.5323,  ..., -761.7054,  149.8218,\n",
            "         -35.1257])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-104.3307,  102.2358,  143.9321,  ..., -160.0031, -167.6244,\n",
            "          22.8110])\n",
            "actor loss: 2409.853273001588, critic loss: 1652501.26953125, entropy: 59733.80029296875, KL divergence: 0.0010242924631736932\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[39.213652097425765, 29.51766665666545, 0, 0, 1.237867336250672], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "87エピソード目の累積報酬：-24252.37141700874, 一つ保全の回数：6524, 二つ保全の回数：1605, 三つ保全の回数：63, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  72.4173, -115.2926, -555.0500,  ..., -591.3918, -298.4879,\n",
            "         185.1470])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 293.5080,  196.0528,   45.2076,  ...,   16.4865, -236.1126,\n",
            "        -408.5976])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-309.5718, -148.1094,    8.7272,  ...,   83.3328,  119.7495,\n",
            "        -558.3447])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  63.7167, -647.0956, -251.0951,  ...,   79.8548, -410.0972,\n",
            "          35.9542])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-693.4548, -461.0926, -133.8810,  ..., -203.3227,  147.6043,\n",
            "        -381.6080])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 150.1493,  -38.7393,   10.5474,  ...,  205.6258,   20.5876,\n",
            "        -186.4127])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -36.8948,  -10.8535,  177.5183,  ...,  -86.0424,    8.2330,\n",
            "        -110.6963])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 109.9619,  -80.5508, -111.6035,  ..., -214.6440,  -42.0964,\n",
            "         -66.8683])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-151.6146,   97.5261, -185.7315,  ..., -169.2204,  -34.1590,\n",
            "         -25.7613])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 177.6747,    3.5266,    0.7827,  ...,   40.0912, -174.6521,\n",
            "          -3.3294])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 137.4388,  114.1590,  -55.2221,  ...,   63.3442,   19.5695,\n",
            "        -273.9443])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([    8.5117,   -73.2082, -1211.7527,  ...,   147.9055,    38.4305,\n",
            "           -3.9025])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  81.4585,  -25.1686,  -33.4306,  ..., -165.9712,   43.8837,\n",
            "          74.1092])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 144.9980,   78.5479,  228.6283,  ..., -125.6617, -314.4022,\n",
            "        -215.3385])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-217.5080,  -45.3645, -128.2080,  ...,   30.3718,   -5.1503,\n",
            "          57.7879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-283.7189,   62.8012, -141.0041,  ...,  155.9355,   34.3079,\n",
            "          79.2064])\n",
            "actor loss: 2351.689820069672, critic loss: 1872908.4609375, entropy: 59662.1103515625, KL divergence: 0.0007348968855652703\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[27.85280003550833, 28.578814272613343, 0, 0, 2.3879887080284625], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "88エピソード目の累積報酬：-22932.701936714555, 一つ保全の回数：6580, 二つ保全の回数：1553, 三つ保全の回数：59, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -316.1503,   -45.2974,  -143.6196,  ...,   107.9693, -1516.4525,\n",
            "         -184.3306])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  27.8556,   43.1076,  -25.3599,  ...,  110.9473,   88.3217,\n",
            "        -249.7628])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-196.2550,   23.0811,  303.1167,  ..., -158.8266, -156.9272,\n",
            "        -284.3733])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-6.6484e+02, -2.3717e+02,  1.6754e+02,  ...,  1.9333e+02,\n",
            "         5.7499e-01, -9.1847e+01])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  173.1092,   103.4002, -2300.6262,  ...,   -28.4511,  -194.1485,\n",
            "           65.3089])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -6.7112,   99.1978,   35.7232,  ..., -152.1328,   61.4707,\n",
            "         154.0526])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  46.1694,  -28.7753, -260.2068,  ..., -141.3883,  -70.3777,\n",
            "         -31.9429])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-168.7155, -235.9567,   21.4356,  ...,  371.3707,  -57.3256,\n",
            "        -107.2130])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([    5.9929,   -60.6622,    55.5432,  ...,  -135.5419, -1070.3597,\n",
            "          -76.9722])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  27.9913,  -29.8695,   61.4707,  ...,    6.8277, -160.4838,\n",
            "        -243.4166])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  43.9470, -546.5912, -408.7225,  ...,  169.4909,   55.6320,\n",
            "        -259.5264])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 311.8058,  152.4877, -119.0303,  ...,  288.6279, -209.1155,\n",
            "         -60.9764])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.9759,  -82.4202,   30.6335,  ..., -243.4166,  -69.1869,\n",
            "         -40.6806])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  68.3925,  -57.9804, -144.2541,  ..., -180.4928,  139.5210,\n",
            "          81.4621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  52.4847, -515.0168,  165.6743,  ..., -135.6353,  153.3097,\n",
            "         129.3903])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  88.9207, -156.5711, -123.1957,  ...,   55.0802, -236.1999,\n",
            "        -251.1328])\n",
            "actor loss: 2217.1712268686133, critic loss: 2525832.1796875, entropy: 59757.63671875, KL divergence: 0.0007818695651772148\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0.49131346329126674, 0, 0, 0, 1.6589753991858847], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "89エピソード目の累積報酬：-27148.416610185417, 一つ保全の回数：6578, 二つ保全の回数：1564, 三つ保全の回数：50, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -86.5639,   26.2521, -538.6933,  ...,   55.2715,  -69.9481,\n",
            "         -60.8932])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-105.4223,  -32.1356,  202.0283,  ...,  -60.7921, -128.9808,\n",
            "          14.8306])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   64.5318, -1056.7355,  -361.6393,  ...,   -85.3301,   -59.4882,\n",
            "         -271.0219])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4.9106e+02, -8.6400e+01, -1.2259e+01,  ..., -1.3326e+00,\n",
            "        -1.8253e+03, -1.5111e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 42.1394, 137.8335, -34.8511,  ...,  60.3517,  66.7219, -80.9217])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-104.1029,   88.6008, -576.3898,  ...,   83.7093,   45.0992,\n",
            "          20.1446])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-376.2682,   69.2995,  -72.4324,  ..., -171.9639,  -76.5905,\n",
            "         110.5976])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.5576,  -48.2777,   52.1680,  ...,   10.4565, -380.8498,\n",
            "         324.8552])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-103.7760,  163.2787,  238.8964,  ...,  -70.0618, -235.8436,\n",
            "          98.5480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.9966,  132.6853,   79.5017,  ...,  -42.5124, -149.1733,\n",
            "        -137.0835])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-276.4583,  -63.8575, -394.4776,  ...,   98.6024,   -6.9734,\n",
            "         196.4754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 245.0501,   -8.4682,  -98.6019,  ...,  -53.9443, -172.1573,\n",
            "          87.5274])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 166.3934,   31.2449,    2.1242,  ..., -206.3700,  -37.9927,\n",
            "         -67.7452])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -234.8802,    22.4354,   109.0450,  ...,   -89.2229, -1709.2408,\n",
            "          -52.7845])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 244.1109,  -23.6067, -568.4421,  ...,  136.5024, -309.2026,\n",
            "        -195.7839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-314.6866,  -81.1295,   91.9215,  ...,   29.5924,  276.0689,\n",
            "         274.3718])\n",
            "actor loss: 2289.3670222995097, critic loss: 1603370.30078125, entropy: 59432.43310546875, KL divergence: 0.0014179449110806321\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[10.766180493682164, 0, 0, 0, 1.4883643956494645], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "90エピソード目の累積報酬：-21778.892758673097, 一つ保全の回数：6555, 二つ保全の回数：1574, 三つ保全の回数：63, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-469.1884, -128.2706, -197.5035,  ...,  -32.6326, -169.0982,\n",
            "         -93.0754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -51.8495,   71.2810,  163.5743,  ...,  -77.8043,   30.9048,\n",
            "        -213.7953])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.7468,  -74.5265,  142.3562,  ..., -655.4916,  123.5464,\n",
            "          16.2114])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -24.8374,  188.7308,  129.7402,  ..., -123.4481, -314.4096,\n",
            "        -100.8851])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -330.3718,  -285.3658,   -54.3625,  ...,    53.2931,   -89.3682,\n",
            "        -1289.0941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 304.1322,  -57.9434,   11.4821,  ..., -154.0732, -169.6930,\n",
            "         118.6218])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-231.7076, -283.3879,  192.6841,  ...,   70.0158, -192.6874,\n",
            "         175.2631])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-173.8338,  -20.6391, -538.0157,  ...,    7.2717,   44.2374,\n",
            "         -59.5084])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   9.8991, -114.3216, -146.0146,  ...,  353.6902,    6.6144,\n",
            "        -384.3373])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  31.9408, -280.7178, -342.9250,  ...,  240.2534,   -3.9711,\n",
            "         143.5373])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.3801,  259.5093,  204.5264,  ...,   -6.6076,  -30.5973,\n",
            "        -126.5236])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -2.2179, -256.8378,   30.0761,  ...,  -26.7175,  -33.6384,\n",
            "         -63.0480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-236.5914,   64.2190,  119.6751,  ..., -105.7207,  121.3802,\n",
            "        -219.8228])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -9.7058, -112.5748,  148.3706,  ..., -102.3222, -150.1459,\n",
            "         -75.5213])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -105.6740,   -85.1926,   184.5704,  ...,    59.4661, -1837.4957,\n",
            "          -15.4612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-256.2802,  -15.8949,  -24.3993,  ...,   78.4560, -277.8343,\n",
            "        -190.6127])\n",
            "actor loss: 2103.038560623677, critic loss: 1705138.69921875, entropy: 59107.24853515625, KL divergence: 0.005702160964232925\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[26.127768596311853, 0, 0, 0, 2.499426142207234], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "91エピソード目の累積報酬：-23187.850027415872, 一つ保全の回数：6576, 二つ保全の回数：1572, 三つ保全の回数：44, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([185.3647, -67.2959, 342.7309,  ..., 156.1749,  92.8556, 162.0190])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-846.8813,   37.6158, -380.9038,  ...,  243.4260, -111.2723,\n",
            "         -53.0631])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -34.2768,   29.6941,  100.4534,  ..., -289.1908,   40.3059,\n",
            "         104.0827])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-194.6945,   37.5423,  192.2965,  ...,  -21.4722,  104.5766,\n",
            "         -81.0250])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-175.7303, -267.9411,  104.1775,  ...,   69.6747,  109.8774,\n",
            "          25.2542])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 70.8593, 172.5308,  56.3760,  ..., 116.9368, 210.2363, 154.5651])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-359.3209,  226.8698,   41.5934,  ...,   62.7199,   84.0365,\n",
            "        -332.7755])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-18.0599, 111.3900,  55.1132,  ...,  51.5051,  63.0873,  37.8370])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-169.7574,  151.8026,   54.7946,  ...,  -46.7312,   77.7633,\n",
            "        -275.3723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-142.1234,  -90.8173,   59.6361,  ...,  264.0571,   34.3640,\n",
            "          65.8327])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   65.9849,    11.2837,  -253.2532,  ...,   145.3015,  -427.3131,\n",
            "        -1192.4045])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 103.1477,   68.9081,  145.5717,  ...,  101.0840, -167.5723,\n",
            "         -47.3596])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([128.6470, 208.5011, -34.0140,  ..., 124.3914,  23.2945, 126.9800])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 219.6041, -348.4293,  180.8936,  ...,   44.3430,   59.4224,\n",
            "           7.5369])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 108.9324, -113.8305,  -18.1489,  ...,  173.9041,  -47.1924,\n",
            "        -168.7702])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-260.3709, -113.1444,  -49.0974,  ...,  -65.6191,  189.3411,\n",
            "        -216.8282])\n",
            "actor loss: 2246.767616203957, critic loss: 3451619.609375, entropy: 59225.9638671875, KL divergence: 0.0008915110935217268\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[9.188916681737858, 53.915679655981386, 0, 0, 1.9270717382728253], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "92エピソード目の累積報酬：-22462.70531714696, 一つ保全の回数：6638, 二つ保全の回数：1517, 三つ保全の回数：37, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 60.9220,  67.3370, -45.4618,  ..., 194.6620,  69.1452, -73.9663])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 545.3563, -196.0114,  634.7888,  ...,   35.7400,   18.2989,\n",
            "          66.0584])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 28.0078,  61.2472, 379.2093,  ..., 153.6501, 390.3953,  65.4546])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-200.6320, -324.2627, -330.0757,  ...,  -15.2193,  -10.9338,\n",
            "         105.5280])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  50.1579,   65.4546,  210.5354,  ..., -261.4217, -295.7408,\n",
            "         115.4426])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-206.5134,  -62.1943,  -94.4885,  ...,   91.9913,  572.4128,\n",
            "         510.4697])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-50.8905,  65.8945, -71.0941,  ..., 265.2538, -32.2895,   1.5986])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  65.9612,   -1.1765,   59.5800,  ...,  -25.2096,   65.0977,\n",
            "        -101.6774])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-17.2898,  -6.4426, 153.4480,  ..., -40.2945, -71.2541, 182.2159])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 91.4395,  88.2807, -85.0211,  ..., 242.4143,  56.7906, -93.9610])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 842.4879,   26.3153,  394.0720,  ...,  477.4344,  195.0364,\n",
            "        -161.5297])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   44.4305,   335.0790, -1729.8687,  ...,   -88.7988,   -43.3225,\n",
            "         -275.7013])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   4.8235, -358.5815,  -26.5126,  ...,  183.8925, -164.5382,\n",
            "        -102.6245])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -81.6214, -121.8709, -374.1552,  ..., -199.5602, -320.3951,\n",
            "         611.5604])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -85.9146,   53.7600,   27.1744,  ...,   17.6647, -117.6626,\n",
            "        -221.5094])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.4292, -493.9093,  -91.6061,  ...,  -57.7135,  157.8904,\n",
            "        -191.2957])\n",
            "actor loss: 2048.6941613544027, critic loss: 4318864.5, entropy: 59798.634521484375, KL divergence: 0.013308746343065468\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 7.237715494088631, 0, 0, 1.9406304805317178], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "93エピソード目の累積報酬：-22000.13585923402, 一つ保全の回数：6615, 二つ保全の回数：1541, 三つ保全の回数：36, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  93.1689,  126.4354, -292.5181,  ..., -536.1509,  -59.2585,\n",
            "         102.0684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -254.2745, -1350.3622,   -21.4501,  ...,   -42.0711,    15.5459,\n",
            "          145.1895])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-383.4826, -309.0750,   61.6812,  ...,   95.8851,  -34.3712,\n",
            "        -238.5547])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-483.8183, -203.4866,  343.5675,  ..., -329.2578,    3.5507,\n",
            "          78.2956])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  65.7045,   83.1200,  -19.0936,  ...,  363.5858, -269.2871,\n",
            "        -474.4828])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-273.8957,  113.1922,   69.2917,  ...,   24.2038, -159.9560,\n",
            "          46.1031])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.0763, -143.8175,  -14.0363,  ..., -154.3667,  475.6306,\n",
            "         -62.0395])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-173.2918,   51.9057,   99.8201,  ...,   92.8301, -467.3661,\n",
            "          49.9008])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 107.2293,  367.9414, -119.4137,  ..., -336.4280, -215.0990,\n",
            "        -108.3366])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1.3805e+01,  1.2212e+01, -7.9080e+01,  ..., -1.9289e+02,\n",
            "         1.4848e+04, -2.5061e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 306.8383,  210.3921,  -64.2595,  ...,   54.3941, -235.2526,\n",
            "         102.0508])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-870.0963, -381.6968,  222.9639,  ..., -151.7173,  208.1127,\n",
            "        -124.9902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  135.1510,   -39.2698,   138.9126,  ...,  -407.3734,  -118.4754,\n",
            "        -1912.9990])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-858.3185,  266.1526,   10.7110,  ...,  -39.9444,  -91.7323,\n",
            "        -227.4508])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([164.7286, -94.3870,  77.8665,  ..., 150.2087, 100.9544,  44.8021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   7.5944, -195.4432, -275.1013,  ..., -628.2957, -286.7938,\n",
            "          50.1947])\n",
            "actor loss: 2228.4450991601457, critic loss: 2230572.7890625, entropy: 59340.722900390625, KL divergence: 0.0038513673263273996\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[85.09532847222764, 11.779577752096824, 0, 0, 2.945131642786553], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "94エピソード目の累積報酬：-23657.69727516782, 一つ保全の回数：6536, 二つ保全の回数：1618, 三つ保全の回数：38, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.1143, -194.9675,  182.3302,  ..., -701.5956,  -10.4468,\n",
            "        -156.9946])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -76.9595,   49.4002, -539.2253,  ...,  145.8771,  210.7704,\n",
            "          61.9086])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  12.8168,    9.4172, -225.4052,  ...,  204.3132,  148.3032,\n",
            "          24.3071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  275.1578,   121.6862,    27.6333,  ...,   244.9062,  -343.0709,\n",
            "        -1232.6179])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 179.9620, -450.3900,  -32.1049,  ..., -183.4500, -482.7496,\n",
            "        -741.4188])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -79.8313,  169.5471,   16.6940,  ..., -108.6097,  224.3283,\n",
            "         354.9246])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([128.3262, 158.5574, 140.8660,  ..., 103.7628,  99.2153,  87.1128])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -16.4603,  128.1709,   96.2519,  ...,  117.8016,  -76.3794,\n",
            "        -155.9480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-495.8600, -581.6912,   22.5348,  ..., -482.5753, -184.8418,\n",
            "          68.4430])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-443.6787,  -94.6890,  180.5969,  ...,  -37.1336, -444.8101,\n",
            "        -120.7366])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 172.3379,  -54.6504, -320.3048,  ...,  122.4072, -203.0466,\n",
            "         -71.8927])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   26.6303,  -389.3092, -1013.7701,  ...,  -130.6819,  -478.2318,\n",
            "           78.5600])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 268.1427, -569.6772,  -43.9490,  ...,   84.5418,   -5.4178,\n",
            "        -306.1201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 56.7620, -99.2108,  73.7036,  ..., 100.3553, -93.6404,  68.9277])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 158.0527,  200.9730, -667.1441,  ...,  325.8700,  201.4095,\n",
            "          -2.9381])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -0.7353,   44.3885, -267.1803,  ...,  -22.8721,  116.4215,\n",
            "          83.9415])\n",
            "actor loss: 2004.806499039441, critic loss: 1812192.87109375, entropy: 58938.736572265625, KL divergence: 0.018405261415691397\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[21.267347076895174, 29.82280781760133, 0, 0, 1.6136077013967214], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "95エピソード目の累積報酬：-21876.503286638057, 一つ保全の回数：6613, 二つ保全の回数：1542, 三つ保全の回数：37, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-216.2057,  -10.7616,  -81.5359,  ..., -146.0419,  109.5097,\n",
            "          85.3235])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  174.4468,   355.2053, -1675.5099,  ...,   118.1066,   -13.5425,\n",
            "         -492.5882])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   83.0240,   128.7552,   -14.8384,  ..., -1124.8153, -1416.0927,\n",
            "          124.9726])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -4.4770, -133.5505,  121.4187,  ..., -237.3377,  -65.9928,\n",
            "        -431.3612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-101.8494,   81.6358, -198.9911,  ...,   51.5452,   49.0370,\n",
            "         -44.7064])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   6.8456,  -94.3865,    2.4461,  ...,  108.2492, -170.8163,\n",
            "         -32.9372])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-31.8599, -15.3105, 100.0000,  ...,  81.4545, 121.3458, 166.7986])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.8421,  145.9262, -527.2150,  ..., -147.0409,  132.6151,\n",
            "         -62.7966])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  11.5571,  -63.8727,  107.6513,  ..., -246.0755,   62.0480,\n",
            "         247.3722])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([114.1223, 124.9726,  52.4606,  ...,  54.8781, -98.2203, -90.9871])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   80.3754,  -111.4080, -2432.3684,  ...,    92.6331,  -622.0934,\n",
            "          231.0116])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   4.7623,  192.6014,  -73.8716,  ..., -107.7042,    6.1246,\n",
            "          97.2148])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.1664,  -81.7231,  155.1249,  ...,  -64.6785, -135.6007,\n",
            "           8.7877])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 185.1909,   67.2541,   23.2437,  ..., -105.8778,  154.4030,\n",
            "        -180.5818])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-297.5697, -118.2182,   56.1075,  ...,   44.2440,   94.3875,\n",
            "          67.4687])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 125.3039,  262.5331, -139.4207,  ...,   71.0919,  161.8962,\n",
            "          49.0169])\n",
            "actor loss: 2194.045821901645, critic loss: 2501175.1953125, entropy: 59020.95458984375, KL divergence: 0.003834690035939195\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[32.0301153136287, 8.67445969146878, 0, 0, 1.054322954326604], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "96エピソード目の累積報酬：-24957.38447292349, 一つ保全の回数：6660, 二つ保全の回数：1495, 三つ保全の回数：37, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -21.2737,  -69.9321,   69.7803,  ...,  -21.3812, -328.3373,\n",
            "          94.8038])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 217.3950,    3.2371,  107.9268,  ..., -148.2056,  -68.5147,\n",
            "         131.4931])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2216.7556,  -104.2786, -1647.3153,  ...,   135.9585,  -607.3504,\n",
            "         -109.0529])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.1190e+01,  1.1592e+02, -1.8360e+02,  ...,  2.6832e+02,\n",
            "         8.1602e-02,  1.9347e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  16.6875,  143.9659, -259.1039,  ...,  158.4429, -172.7899,\n",
            "         104.0029])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.3451, -111.2042, -182.6177,  ...,   80.2994, -224.4162,\n",
            "         -89.3896])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-782.9758,   42.5160, -329.8476,  ...,  233.3896, -151.5913,\n",
            "          16.0340])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   -6.1601, -2626.9480,  -168.6828,  ...,    43.0234,   180.0421,\n",
            "          -52.1565])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  12.3740,  132.0242, -223.7854,  ...,   58.8125,  -25.8585,\n",
            "         167.3110])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([370.6198, 138.5329,  72.4231,  ...,  51.4142,  78.4246, 119.4501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 209.1314,   48.7846, -229.8521,  ...,   10.0617,  125.9980,\n",
            "          72.7172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([106.4254,  86.8047,  56.4228,  ...,  -0.8669, 163.8602, -53.2972])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-147.4650, -118.4040,   69.4436,  ...,   24.7619, -115.8432,\n",
            "         -76.3319])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -281.4606, -1555.4427,   -97.1498,  ...,    -9.3488,   157.4721,\n",
            "         -289.9711])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([291.7827,  80.6629, -30.0719,  ..., -91.7889,  71.8548,  59.4550])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([    5.9249,   291.3815,  -144.1720,  ...,  -552.5958, -1662.4011,\n",
            "          -61.6044])\n",
            "actor loss: 2097.4795507451645, critic loss: 2602510.359375, entropy: 59021.921142578125, KL divergence: 0.0017647323522937298\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[40.938033179215225, 0, 0, 0, 1.638207444454551], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "97エピソード目の累積報酬：-19528.92923377098, 一つ保全の回数：6690, 二つ保全の回数：1468, 三つ保全の回数：34, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  158.6937,   -58.2023,   126.3232,  ...,    73.4870,   194.7523,\n",
            "        -1645.8876])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 484.4325, -138.4924, -149.8500,  ...,  -90.0616,   60.0329,\n",
            "        -364.2718])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-510.5847,   27.1242,    2.4890,  ..., -203.4162,  -70.9877,\n",
            "        -336.6061])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 54.7309, -63.3606, -26.5878,  ...,  54.0763, -49.1246,  19.2230])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -46.0926,  119.6674,  130.0726,  ...,   90.6339, -266.0888,\n",
            "         -59.5467])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -41.6231,   -12.4320,   407.6601,  ...,    76.6204, -1927.0612,\n",
            "          -89.2441])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 79.9087,  29.9000, 140.1433,  ...,  11.1268,  74.7437,  91.2584])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-225.8189, -111.6621,   26.5689,  ...,  -70.4162, -159.6656,\n",
            "         125.6443])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 114.9124,  -65.0522,  200.1922,  ..., -405.4421, -216.6221,\n",
            "         198.5866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -219.5912,  -143.8256, -2206.5547,  ...,  -473.9884,   219.8597,\n",
            "           58.0551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  144.0540, -2510.0813,  -120.6802,  ...,    -7.9299,   171.2393,\n",
            "         -440.0176])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -5.2042, -400.3286,   75.6013,  ...,  -32.3340, -158.1115,\n",
            "        -171.8890])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-119.5459,   40.3609,   81.9693,  ...,  -48.8872, -329.4065,\n",
            "        -176.0070])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  72.0215,   34.7750, -103.6358,  ...,  132.2874,  -94.9922,\n",
            "        -117.0359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-253.3292,  192.4198,  -50.0760,  ...,   50.8671,  101.5943,\n",
            "         137.4213])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([244.2767,  91.1587,  69.8228,  ..., -29.7768,  42.4615,  69.9883])\n",
            "actor loss: 1894.7322298461816, critic loss: 2658631.7890625, entropy: 59238.438232421875, KL divergence: 0.0010334521026589213\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 53.799024929164275, 0, 0, 1.3384996947065293], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "98エピソード目の累積報酬：-22644.19589263626, 一つ保全の回数：6696, 二つ保全の回数：1457, 三つ保全の回数：39, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1347.9553,   183.7810,  -352.8799,  ...,   -43.8879,   143.9989,\n",
            "         -135.4567])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-139.3799,  112.6424,  209.2180,  ...,   -7.8421,  159.1751,\n",
            "         -30.5207])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -88.1377,  163.7715,  135.7195,  ...,   82.1677, -177.0220,\n",
            "        -145.6048])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  144.7879,    52.8195,   -22.5755,  ..., -1623.8027,   -32.1830,\n",
            "         -275.5806])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  90.2834,   81.5241, -167.2887,  ...,  -31.3333,  191.5116,\n",
            "          -7.6902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  85.1314,   82.1098,  450.4856,  ...,   -1.5475,  195.7318,\n",
            "        -137.2811])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -139.3799,    28.7822,   135.2020,  ...,   -72.5771, -1195.1929,\n",
            "          -23.3007])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 101.0933,   87.1186, -162.4333,  ...,  252.7350, -238.8950,\n",
            "        -214.6908])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-26.2274, 147.2798,  57.7840,  ..., 249.7399, 142.2778,  79.9254])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 223.8372,   38.7632, -287.7170,  ...,  -96.7269, -130.4388,\n",
            "        -175.3549])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -13.1098,  -65.6076, -500.4991,  ...,  302.6069,  -48.9475,\n",
            "          -4.0339])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.5707,  219.3251, -116.9059,  ...,   38.2874, -466.7455,\n",
            "          97.7889])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 135.2666,   59.4472,   38.1552,  ..., -212.3454,  204.6655,\n",
            "          64.6641])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-228.9287, -130.6316, -171.1543,  ..., -576.6447,   50.7464,\n",
            "          94.9829])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  59.6467,  -92.3874,  -58.0924,  ...,  -83.4726, -153.9335,\n",
            "        -296.2240])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 116.5473, -876.1534, -172.1623,  ...,   10.2004,   25.4344,\n",
            "         -85.4209])\n",
            "actor loss: 1811.603839984807, critic loss: 2759650.1484375, entropy: 59176.927490234375, KL divergence: 0.0016452121134468201\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[93.79901869808543, 2.314094203534633, 0, 0, 1.4437852392751305], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "99エピソード目の累積報酬：-21245.86149471463, 一つ保全の回数：6699, 二つ保全の回数：1459, 三つ保全の回数：34, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-202.3600, -135.0603,   76.6142,  ...,  -48.4391,   85.3941,\n",
            "         225.8639])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -58.0451,   65.3881,  141.4515,  ..., -133.1491,   53.9293,\n",
            "         -53.9469])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 219.0997, -304.0455,  134.8074,  ...,  -98.4819, -251.6124,\n",
            "         -61.0442])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2447.2214,    92.3192,  -320.1819,  ...,    66.3118,   147.8564,\n",
            "           91.9853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.8492, -118.1457, -132.8889,  ...,  153.1041,  -90.9158,\n",
            "         -63.9993])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  238.6410,  -518.5526, -1791.2806,  ...,    18.7150,  -107.9056,\n",
            "         -197.3494])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-168.6198,  203.8683,  -40.1022,  ...,  161.3128,  -67.9059,\n",
            "         236.7372])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  239.7081,  -222.0632,  -509.6173,  ..., -1170.5536,    -7.6328,\n",
            "          189.6291])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-108.0190, -274.4428,   76.3838,  ...,  150.2054,  121.3860,\n",
            "        -135.7147])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-291.4721,   93.4486,  168.5436,  ...,  192.9816,   34.7774,\n",
            "        -130.6754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-152.4706, -233.7351,  -30.6974,  ...,  177.0364, -182.0562,\n",
            "         -89.7957])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-144.5690, -167.3459,  -80.9759,  ...,    3.7502,  111.5943,\n",
            "         -66.1780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 146.3358, -117.7624,   47.0659,  ...,  -91.1806,  111.1948,\n",
            "          19.3133])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 127.0874,  155.1854,  203.4095,  ..., -269.2992,  157.9216,\n",
            "        -211.9555])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.1398,   63.0594,   86.0797,  ..., -337.4271,    1.5580,\n",
            "          31.3843])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -74.2848,  254.5186,  187.7230,  ..., -221.0708, -119.0505,\n",
            "        -296.5808])\n",
            "actor loss: 1904.8384300429605, critic loss: 2383517.25, entropy: 58970.04638671875, KL divergence: 0.0005465841549563162\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[5.776484646613709, 70.39749113672806, 0, 0, 1.966522279611301], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "100エピソード目の累積報酬：-17141.777692249514, 一つ保全の回数：6678, 二つ保全の回数：1489, 三つ保全の回数：25, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1545.0868,    53.4223,    -9.7187,  ...,   -22.0966,  -108.5453,\n",
            "          150.3401])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 108.9143, -212.9700,  -17.0129,  ...,  159.8569, -651.2111,\n",
            "        -191.5412])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-128.8793,  -10.9372,    6.2477,  ...,  -30.0874,   81.7063,\n",
            "        -144.2666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   6.0366,  -63.9050, -211.9970,  ...,   33.8508,  269.0608,\n",
            "        -206.2200])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-54.3442,   0.9165, -77.4600,  ...,  92.2445, 173.3061,  94.1168])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 195.0604, -124.5015,  -64.9513,  ..., -181.7866,   30.5791,\n",
            "          17.7101])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-125.2024,  188.7448, -136.8988,  ...,   86.9339, -112.3973,\n",
            "        -231.4765])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1783.6395,    -2.4856,    55.5409,  ...,    44.7859,    16.5332,\n",
            "         -212.3192])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-312.7916, -644.9252, -121.4052,  ..., -119.6017,  169.5767,\n",
            "         -37.7904])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -351.4920, -1439.0491,    77.6674,  ...,    75.7052,  -311.3405,\n",
            "         -265.2360])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -35.7914, -164.3104,  181.7861,  ...,  293.9864,   -7.3976,\n",
            "        -551.9036])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -37.6197,  185.4705,  132.3209,  ..., -230.7445,    6.6781,\n",
            "         -16.3264])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-134.4529, -352.2045,   23.1309,  ...,    4.9109,  147.0315,\n",
            "          68.7495])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 196.2536,   93.1802,  173.5842,  ..., -155.9646,  -17.3105,\n",
            "         -19.5774])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  92.9450, -230.9756, -116.7897,  ...,  248.5504,  245.8188,\n",
            "          22.5418])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-87.4779,  84.6216, 197.3458,  ...,  65.9713, -60.0964, 164.4872])\n",
            "actor loss: 1932.6118846391694, critic loss: 2401430.609375, entropy: 58763.56884765625, KL divergence: 0.004509725375400191\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[57.12159807913861, 40.127251987187634, 0, 0, 2.2222011281349325], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "101エピソード目の累積報酬：-18384.641118152456, 一つ保全の回数：6664, 二つ保全の回数：1502, 三つ保全の回数：26, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -52.8749,  -386.6659,   -16.4752,  ...,   140.8866, -1483.8124,\n",
            "            3.2387])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 135.7733,  113.3780, -110.1556,  ...,   -3.6896,   40.1992,\n",
            "         126.7523])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.7834,   43.8938,   19.5356,  ..., -108.7787,  -47.3472,\n",
            "        -696.5656])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-112.5238,  444.4182, -340.6372,  ...,    7.1430,  -37.9925,\n",
            "           7.6787])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 123.2240, -181.0485,  197.1058,  ...,   80.9846,   55.7861,\n",
            "        -197.4942])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  36.2583,  180.1774,  101.4557,  ..., -164.4767,   54.4096,\n",
            "        -137.6727])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  74.2281, -202.1974, -400.9142,  ...,  -52.3739, -130.1845,\n",
            "          90.2316])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([208.4281, -56.3129, -71.9976,  ..., 149.4516,  40.4060,  21.5444])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-327.9730,  -52.5850,  222.3456,  ...,   18.0703,   60.2980,\n",
            "         264.8193])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -3.4022,  106.2812,   83.3616,  ...,   37.9296,   27.2782,\n",
            "        -512.9998])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -402.1394,  -379.2691, -1830.5813,  ...,    69.5121,   113.3780,\n",
            "          -17.2759])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 205.9140,  106.6070, -140.0973,  ...,  268.6890,  146.5694,\n",
            "         270.0212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -77.4496, -1331.6350,    69.4161,  ...,   -17.3100,  -194.9949,\n",
            "         -116.2597])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-181.5511,   20.9926, -198.2516,  ...,  359.1646,  146.7592,\n",
            "          62.6824])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -77.6606, -237.5521,   66.0702,  ...,  -32.5965, -324.4925,\n",
            "           1.5629])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 184.4085, -289.5146,   50.9367,  ...,  124.5334,  124.2471,\n",
            "         -35.0218])\n",
            "actor loss: 2018.9359596148388, critic loss: 2244001.1171875, entropy: 58524.375, KL divergence: 0.0021700745731902667\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[57.22450398517109, 42.92638959918048, 0, 0, 1.292260130983447], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "102エピソード目の累積報酬：-21087.017058786143, 一つ保全の回数：6688, 二つ保全の回数：1480, 三つ保全の回数：24, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1844.8414,  -188.4512,  -346.8617,  ...,   -95.7595,   149.0542,\n",
            "          158.9646])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  87.4281,  229.0384, -152.9046,  ...,  -54.2966,   15.7117,\n",
            "          -1.6144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 309.9550,  -96.6715, -198.7718,  ...,  -79.3121, -318.1757,\n",
            "        -146.5050])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-108.3461,   77.3830,  -55.4759,  ..., -144.1266,   -5.7230,\n",
            "         111.8493])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 130.6611,  153.7011, -178.5307,  ...,  -84.4919,  -15.6861,\n",
            "         219.8332])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -72.3441, -240.7641,   12.3275,  ...,  -26.4562,  -54.2966,\n",
            "          80.5241])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -13.9288,   22.1049,   10.0839,  ..., -177.0814,   26.3355,\n",
            "         -76.5618])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  91.1507,    1.8067, -155.1464,  ...,  -25.8345, -190.9988,\n",
            "          -4.1741])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  97.4110, -207.0241,  136.6356,  ...,   53.2703,  141.5766,\n",
            "        -209.9176])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -49.6061,  212.5675,  119.0732,  ..., -102.3235,   58.2434,\n",
            "         102.4538])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  13.1158, -146.3044, -170.6601,  ...,   69.3656,   25.8802,\n",
            "        -233.2292])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 154.4237,  177.5499,  -73.4381,  ...,  104.4596, -418.2758,\n",
            "          57.1239])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  53.0978,    2.2239,   29.7054,  ...,  144.9197, -142.7325,\n",
            "         216.6582])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -406.4813,   167.2305, -2611.6355,  ...,    87.0552,   -14.9364,\n",
            "          -47.7951])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 186.3989, -499.5187, -283.9548,  ...,  -95.4407,  -73.5170,\n",
            "         166.0216])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 228.5379, -126.8616,    2.8263,  ...,   20.1288,  -47.5812,\n",
            "        -106.1256])\n",
            "actor loss: 2150.550238756502, critic loss: 2342948.328125, entropy: 58708.7763671875, KL divergence: 0.0014717340719751881\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[9.77845942457929, 70.90149975337185, 0, 0, 1.41669764748892], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "103エピソード目の累積報酬：-25188.57646721826, 一つ保全の回数：6693, 二つ保全の回数：1474, 三つ保全の回数：25, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -11.7329,  168.3982,   73.9930,  ...,  -23.6657, -444.9018,\n",
            "         269.8948])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 109.1438,   32.1861,   26.2930,  ...,  -24.6090, -135.7664,\n",
            "         113.4255])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-709.1294, -342.6234,  -91.7877,  ...,  153.4046,  111.6234,\n",
            "         -85.6116])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.5746e+01, -1.2782e+02,  1.3130e+02,  ..., -2.0437e+02,\n",
            "         8.6772e+00,  1.1994e-01])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-30.8787,  74.8826, 227.5208,  ..., -73.4817, -67.6786,  26.1023])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 166.9323,   52.5549, -307.5552,  ...,   83.6824,  -80.9163,\n",
            "         -67.8894])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-106.1652,   61.6225,  269.2708,  ...,   95.6857,   68.7705,\n",
            "         172.5717])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   5.2483,   42.3046,   -7.5587,  ...,   57.8696,  -72.8845,\n",
            "        -280.7201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -58.5278, -886.0637, -488.0360,  ...,   63.8059,  288.1008,\n",
            "         -23.5663])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-102.2264,  256.6090, -146.0343,  ..., -368.9840,  154.8507,\n",
            "         -17.9281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  59.1748,  -68.6254,   36.9889,  ..., -110.4835,   83.6717,\n",
            "           1.4759])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  96.0198, -124.1275,  277.0690,  ...,  176.1626,    3.8842,\n",
            "         -80.4370])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  93.4643,  -78.5707, -483.7172,  ..., -488.8537,  174.0957,\n",
            "         -27.6621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 192.5870,  130.2388,   92.5697,  ..., -167.2695,  154.8914,\n",
            "        -255.2829])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -97.2477,    42.0951,  -393.7231,  ..., -2503.0068,   105.2591,\n",
            "         -204.6774])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -479.6397,  -264.3154,   297.4475,  ..., -1375.4231,    82.3565,\n",
            "           13.9619])\n",
            "actor loss: 1892.300627215225, critic loss: 2504038.4765625, entropy: 58989.5380859375, KL divergence: 0.0010890430055950587\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[92.81442065196113, 37.08495660338786, 0, 0, 1.003692364597837], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "104エピソード目の累積報酬：-22382.593746753602, 一つ保全の回数：6715, 二つ保全の回数：1450, 三つ保全の回数：27, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -92.3984,    -2.5372,   289.0399,  ...,    48.3165,   -36.8370,\n",
            "        -1620.9396])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 249.2222,  145.9734, -198.2587,  ...,  125.7690, -225.1191,\n",
            "         131.1536])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1225.1853,   133.2673,   130.7036,  ...,   155.8560,    85.5419,\n",
            "          -56.5883])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-121.0984, -280.0873, -180.8503,  ..., -271.3726,   -5.2347,\n",
            "          40.0362])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 131.6922, -396.6989, -305.1402,  ..., -139.0277,  144.7890,\n",
            "          86.4429])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -16.0244,   138.9445, -2546.1333,  ...,  -248.2748,   348.6894,\n",
            "         -202.1104])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 137.5288,  167.9780,  -15.7370,  ..., -111.2059,  124.8924,\n",
            "         -17.0529])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 120.1792,  134.2430,  117.9287,  ...,  -88.3121, -463.5980,\n",
            "         -10.4738])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-123.7466,  317.9323,   45.2361,  ...,   94.3371,  -82.4035,\n",
            "         -30.3184])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 193.4420,   92.5014,  -48.6865,  ..., -110.5994, -111.0858,\n",
            "        -135.8460])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  223.0944, -1403.1301,    16.4701,  ...,   -13.4674,   161.6270,\n",
            "         -185.6780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.2025, -479.2396, -172.3360,  ..., -461.6709, -245.9219,\n",
            "         155.9151])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 46.3791,  60.7606, 240.1304,  ..., 201.0921,   8.9889, 241.7802])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  128.9614,    85.5744,    22.2225,  ..., -1425.2233,   -12.2766,\n",
            "           39.3664])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-245.3673,   69.2055, -151.8094,  ...,  196.1897,   94.2369,\n",
            "          52.7318])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.8664,   -7.5285, -108.4512,  ...,   60.1275,   40.8193,\n",
            "          39.1942])\n",
            "actor loss: 2065.8012255253925, critic loss: 2591836.546875, entropy: 58601.412841796875, KL divergence: 0.0017122278947948478\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[21.170843713598295, 30.661992968072962, 0, 0, 1.1857027193174534], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "105エピソード目の累積報酬：-23312.39406798249, 一つ保全の回数：6726, 二つ保全の回数：1440, 三つ保全の回数：26, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 81.0893,  36.8644,  25.5225,  ..., 173.1087, 227.7447, 155.2850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  43.8835, -342.3602, -200.2694,  ...,  498.8604, -111.6352,\n",
            "           4.6696])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-184.5798,  -85.1792,  110.4153,  ...,  182.5241,  105.3372,\n",
            "         152.7477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-181.0383,   98.8487, -149.5128,  ...,   48.7992, -117.0521,\n",
            "         114.0617])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-165.9608,  126.8913, -689.2734,  ..., -108.3095,  134.4597,\n",
            "         204.6021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-427.9797,   71.2757,  -14.5307,  ..., -251.8827,  -53.8868,\n",
            "         148.8722])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -20.5560, -434.6443,  308.1347,  ...,  -24.1565,   98.4683,\n",
            "        -141.4536])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 167.7095, -234.6725,  108.1525,  ...,  119.7540, -211.2393,\n",
            "        -152.9749])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 143.7810,   85.8356,  126.7936,  ...,  -34.6122,   88.7152,\n",
            "        -149.1345])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 146.0821,   41.1723, -154.3660,  ..., -132.2822,  -17.2692,\n",
            "         132.8860])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  60.8019, -218.6313,  150.4460,  ..., -134.7420,  213.1715,\n",
            "        -127.5016])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 164.0133,  301.2230,  -13.2712,  ..., -101.2832,  158.4942,\n",
            "         334.8684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 289.9980, -227.8191, -419.2277,  ..., -130.0555,  176.0795,\n",
            "        -194.6887])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -65.1149,  121.3051, -491.4965,  ..., -114.5016,  -12.7438,\n",
            "         134.4723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1542.4696,   -96.3716,    -5.9526,  ...,   -92.2602,   151.2351,\n",
            "         -445.8910])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-91.7240, -10.5941, -60.2718,  ..., 166.0602,  99.5821,  23.4381])\n",
            "actor loss: 2016.1241436869057, critic loss: 2927344.0078125, entropy: 58647.9208984375, KL divergence: 0.0008941665117396549\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[22.970406003405216, 3.4919647427505693, 0, 0, 2.081093709378517], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "106エピソード目の累積報酬：-20655.326372754014, 一つ保全の回数：6725, 二つ保全の回数：1444, 三つ保全の回数：23, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-163.0996,  -83.8721,  -54.6394,  ...,   26.2877,  259.8806,\n",
            "        -330.9028])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -57.0841,    58.4861, -1659.4174,  ...,  -987.1741,   322.7863,\n",
            "          129.6931])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -57.8704,   155.5997,    85.1832,  ...,  -316.8159, -1387.4974,\n",
            "         -246.7486])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-333.7685,  -15.8803,  -18.6318,  ...,   38.7050,   83.1876,\n",
            "         -33.8726])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 75.5605,  16.0449, 267.6496,  ...,   9.7721,  21.8200, -30.7682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 38.4783, 191.1932, -28.4921,  ..., 127.4072, 196.0179, -90.8983])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -4.5484,   41.6138, -534.4283,  ...,  231.6540, -252.9494,\n",
            "         121.8090])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-476.7673,   41.1656, -209.5790,  ..., -178.5788, -361.1064,\n",
            "         308.7188])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([166.3533,  85.9270, -62.7877,  ...,  47.5793, -56.5637,  54.5439])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-265.4205,  120.4445, -116.1258,  ...,  116.6052, -505.0706,\n",
            "        -223.1501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-27.9334, 169.6479, -41.4587,  ..., 131.7406,  49.8003,  35.0187])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -52.4503,  250.3726,  -15.5456,  ...,   25.6863, -898.9307,\n",
            "         -76.3493])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.6217,  -83.9073,  -86.2032,  ...,  -66.8930,   67.6723,\n",
            "        -147.2290])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1355.6602,   390.1246,  -202.4971,  ...,   281.0142,  -825.4028,\n",
            "           -6.8026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -280.5049,   325.2295,    83.0567,  ..., -1981.8433,   137.7378,\n",
            "         -410.1551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  189.9340,  -315.3793,  -101.2330,  ...,    83.1876,  -219.0822,\n",
            "        -2501.5034])\n",
            "actor loss: 1978.3157162809568, critic loss: 2449512.4375, entropy: 58797.761962890625, KL divergence: 0.0011941092705714908\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[35.75995722730675, 0.39288254350488344, 0, 0, 1.8360270118778905], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "107エピソード目の累積報酬：-19324.757056728507, 一つ保全の回数：6720, 二つ保全の回数：1446, 三つ保全の回数：26, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -84.8395,  163.9324,   57.3900,  ..., -150.7416,  -72.1780,\n",
            "        -122.1202])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 193.9534,   83.7198, -106.0493,  ...,  117.7673, -193.5274,\n",
            "        -776.0071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  189.6418, -2061.9663,  -246.3214,  ...,   -97.5358,    34.6492,\n",
            "           41.1243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-112.9993,   -8.6613,  192.4982,  ...,  -26.5123, -180.1781,\n",
            "          99.3005])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  201.3060,  -217.3763,  -206.7331,  ..., -1539.4048,    -8.7819,\n",
            "         -243.1103])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 104.6234, -259.1765, -245.5096,  ..., -202.4525,   85.4087,\n",
            "        -153.6097])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -57.7794,  -89.4561,  -89.1995,  ..., -188.7752,  212.3581,\n",
            "         226.9919])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 75.7804, 143.7860, 209.8443,  ...,  39.0496, 154.7925,   2.2000])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 217.7789, -208.6338,  130.0429,  ...,  166.2182, -226.5567,\n",
            "          15.8125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-233.7909,  -19.5892,  177.7129,  ...,  -75.2061,  -20.4624,\n",
            "        -110.2167])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-267.8005,  196.3449,   78.8741,  ...,  268.5937,  228.4313,\n",
            "        -551.0048])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-101.7284,  111.4921,   66.1649,  ...,   73.6542,   -2.2811,\n",
            "        -365.7603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-203.1806,  -87.3155,   49.0979,  ...,  109.0397,  267.2580,\n",
            "        -235.8141])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -19.7046,  131.1102,  -86.4059,  ...,  188.0859,  212.7999,\n",
            "        -259.6007])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  48.3285, -316.4349,    6.7186,  ...,   94.0400,  236.6552,\n",
            "          37.2177])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  60.2898,   58.6669,  -10.4155,  ...,  -74.1329,  158.4019,\n",
            "        -129.6882])\n",
            "actor loss: 1703.975937792174, critic loss: 2590757.578125, entropy: 59002.604248046875, KL divergence: 0.0024725551222588432\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[28.39333882301432, 0.22821820014904137, 0, 0, 2.6547332359650686], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "108エピソード目の累積報酬：-22654.18442604987, 一つ保全の回数：6728, 二つ保全の回数：1443, 三つ保全の回数：21, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-543.9366,   24.8783, -239.6589,  ..., -170.6564,   29.2508,\n",
            "          17.4442])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 124.2855,  194.9872,  -89.5575,  ...,  111.9330,  -91.7111,\n",
            "        -306.7552])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-259.6839, -146.8523,   23.0050,  ...,   88.9401,   26.4029,\n",
            "        -418.4893])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -30.0097,   144.9328, -2471.2896,  ...,  -356.7437,  -164.0353,\n",
            "           96.9895])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 75.3462, -61.7378, -60.4562,  ...,  -0.6042,  63.3561, 165.5090])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -60.6621,   78.9268,  200.2114,  ..., -131.6063,  468.4320,\n",
            "        -210.5607])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -75.1156,  197.5173, -158.9603,  ...,   31.1364,  118.2307,\n",
            "         110.7552])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([174.6796,  92.7804, 104.5781,  ..., 122.4436, 278.7817,  19.3194])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-225.7475, -296.8299, -113.7936,  ..., -476.4747,  106.8881,\n",
            "         -56.9985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 179.9675, -129.9227,   53.9812,  ...,   10.7465,  -25.8833,\n",
            "         -76.0266])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 134.3700, -117.2360,  138.6169,  ...,  194.8658,   47.7421,\n",
            "          76.0488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 241.6169, -227.2331, -189.4287,  ...,   78.4862, -337.0654,\n",
            "          28.4995])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-192.3358,  261.2206,   15.4316,  ...,  234.9198,  177.4349,\n",
            "         138.6090])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  66.2324,  -10.2416,  147.9859,  ...,  144.0359, -392.9602,\n",
            "          11.7706])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-153.8620, -473.6833,  129.1525,  ..., -119.1923,  -34.2714,\n",
            "         -91.4892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([140.8355,  45.7530, 146.8581,  ..., -62.9982, -55.0676, 151.0479])\n",
            "actor loss: 1913.981037577158, critic loss: 2193647.4765625, entropy: 58949.802978515625, KL divergence: 0.0009711125907010317\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[17.840205887737678, 46.180306285258915, 0, 0, 2.187689642142817], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "109エピソード目の累積報酬：-20081.608527653665, 一つ保全の回数：6669, 二つ保全の回数：1498, 三つ保全の回数：25, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-195.6798, -313.6996,  313.9534,  ...,  -90.1047,   76.7610,\n",
            "          10.2100])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-344.3917,  -21.5382, -127.3638,  ...,  123.8632,  -59.8525,\n",
            "         258.7328])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -115.8160,    13.5537,   -34.8096,  ...,    76.3897,    60.1095,\n",
            "        -2590.8796])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 157.9484,   52.7305,  215.4752,  ..., -996.8263,  -89.6031,\n",
            "         -38.1991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 145.5197,  146.7223, -128.1353,  ...,   -4.4354,  118.0030,\n",
            "          13.3544])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   6.3729, -312.9113, -128.5813,  ...,  182.9216, -267.8296,\n",
            "        -266.7959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 112.4750,  238.0898, -164.5193,  ...,  -43.0544,  163.7480,\n",
            "         211.1053])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   90.8191,   -89.0073,  -108.4696,  ...,   119.9385, -1132.9769,\n",
            "           22.7864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-34.4366,  75.9316, 150.1481,  ...,  43.3685, 145.2698, 255.1914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 308.1038,  -93.5887,   14.3211,  ..., -711.4520,  -14.8128,\n",
            "          46.7485])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -66.1480, -365.1029,  178.0875,  ...,  -70.6852,  124.2175,\n",
            "         176.5706])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   60.0510,    -1.5065,  -433.7612,  ..., -1126.3077,   -13.0693,\n",
            "          295.0844])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-407.0979,  190.3238,  -88.3647,  ..., -384.9675, -541.2291,\n",
            "          47.3258])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 248.1160,  181.8640, -332.9570,  ...,  101.7535,   95.7160,\n",
            "          14.1890])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  6.6145,  97.6304, 223.2076,  ...,  30.9346,  13.9667,  17.0217])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -16.7009, -667.2285,   97.1707,  ...,  244.3006,  -32.8138,\n",
            "        -338.8451])\n",
            "actor loss: 2046.2350344010272, critic loss: 2240972.28125, entropy: 58576.2734375, KL divergence: 0.008035068850623436\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[40.86540550789635, 62.90022311695991, 0, 0, 2.0742661970447234], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "110エピソード目の累積報酬：-21435.34747184617, 一つ保全の回数：6653, 二つ保全の回数：1518, 三つ保全の回数：21, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-239.8234,   15.5513,   78.0509,  ...,  315.1376, -233.6803,\n",
            "          92.8336])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   -6.8092,   136.3733,   -41.7291,  ..., -1852.3729,  -292.5562,\n",
            "         -185.9839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -64.7134,   94.5195,  -11.5768,  ...,  -33.2861,   37.0436,\n",
            "        -208.4262])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -141.4049,   248.0185,    86.8985,  ...,   112.4824, -2521.3979,\n",
            "         -137.4517])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-262.2797, -211.5936,  -25.7166,  ..., -319.3091,  -12.7525,\n",
            "         244.5934])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -76.3788, -128.7824, -842.4963,  ...,  289.2720,  133.1361,\n",
            "         -10.8484])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-42.7108,  78.8030,  49.9988,  ..., 209.6499,  55.2187,  67.9768])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-270.8264,  141.8168, -222.0572,  ...,  113.0844,  120.6246,\n",
            "        -242.6470])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -16.5916, -122.6373,  216.9736,  ...,  336.2338, -180.0630,\n",
            "         178.8662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-274.8627,   17.3094, -194.7188,  ...,   69.0834, -113.3739,\n",
            "         186.2807])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  46.9445, -122.8423,   23.9285,  ...,   59.8947, -386.8565,\n",
            "          46.8815])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([116.3661,  73.4251, -94.7673,  ...,  -7.9160,  70.7519, 156.5929])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 129.1060, -309.5253,   93.2679,  ...,  -11.8970,   31.3233,\n",
            "         146.8181])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 173.0361,  163.3579,  106.8531,  ...,    4.6306, -140.5453,\n",
            "          13.3177])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -461.4605, -1442.4493, -1690.5121,  ...,  -249.4742,  -418.7634,\n",
            "          -63.9962])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  76.7245,   29.9850, -314.0633,  ...,  -98.3234,  225.6871,\n",
            "           1.8533])\n",
            "actor loss: 2036.4372326505695, critic loss: 3455363.921875, entropy: 58619.210205078125, KL divergence: 0.001284215599361016\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[6.243079048493152, 0, 0, 0, 2.16043222953836], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "111エピソード目の累積報酬：-20331.809050030355, 一つ保全の回数：6698, 二つ保全の回数：1465, 三つ保全の回数：29, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -32.5073, -304.9170, -205.0772,  ...,  207.0416,  -13.8627,\n",
            "        -295.5858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -34.5294,  -580.5665, -1420.4402,  ...,   251.6778,   -29.9017,\n",
            "          103.8458])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -9.0432,  203.4424,  169.2666,  ..., -120.6597,   33.1912,\n",
            "         -97.6259])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  46.9615,  261.8416,  147.8052,  ...,   67.8207,  229.4621,\n",
            "        -472.5428])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 133.7676, -398.9947, -240.9957,  ...,    6.8761,  -51.8255,\n",
            "        -127.0893])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-382.0052,  -56.8623,  139.8151,  ...,   66.1816,   36.7875,\n",
            "         115.0184])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -3.5258,  48.3041,  33.6672,  ..., 226.4536, -25.6003, -65.2385])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  81.9113,  -30.3604,  -75.5793,  ...,  203.0424, -315.2423,\n",
            "        -184.3377])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 192.9780,   75.7856, -184.3377,  ...,  112.6817,  127.2268,\n",
            "        -173.6194])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -23.6717,  101.0095,   47.0526,  ..., -639.1730, -278.7523,\n",
            "          58.3277])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 120.0226,  101.3920,  -25.2919,  ..., -261.6236, -159.6237,\n",
            "        -116.1908])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -77.5733,  128.2519,  136.0037,  ...,   -1.8567, -437.4150,\n",
            "          71.6288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -36.1335, -136.5653,   65.3670,  ...,   37.3137,   44.3309,\n",
            "          56.8418])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  164.0762,  -310.0758, -2442.5439,  ..., -1748.4731,    40.5749,\n",
            "           57.0471])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   63.1083,   -54.1840,    95.5484,  ...,    64.2345,   156.0390,\n",
            "        -1382.3199])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -20.7534,   93.5523, -148.9709,  ...,  -17.3002,   -5.7474,\n",
            "          20.8055])\n",
            "actor loss: 2031.0943016660713, critic loss: 2328627.3359375, entropy: 58510.28564453125, KL divergence: 0.0021226589877833607\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[44.56766218472804, 61.507945327770635, 0, 0, 2.4790280720236866], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "112エピソード目の累積報酬：-22526.73372793888, 一つ保全の回数：6699, 二つ保全の回数：1473, 三つ保全の回数：20, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([119.2485,  76.3165, 203.5821,  ..., -78.6979,  79.4722, -79.8675])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   12.3731,    92.9279,     3.8987,  ...,   110.0228, -2511.0994,\n",
            "           42.3507])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 210.2975, -192.4455,  129.8468,  ...,  120.9584,   19.2709,\n",
            "         282.1725])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -147.3522,   -94.4907,   125.2353,  ...,    89.4322, -1517.1594,\n",
            "          333.8214])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  70.5011,   13.1814,  -77.5783,  ..., -243.6637, -666.5953,\n",
            "        -387.0793])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -44.1580,  -77.7659, -468.9881,  ...,   59.8194, -124.4489,\n",
            "           2.5844])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-53.7754, -37.9152,  84.9276,  ...,  81.3065, 296.0370,  89.6211])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -68.5395, -364.5011,  126.1983,  ...,  115.4365,  -30.7426,\n",
            "         200.5927])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 108.9361, -415.2320,  144.0414,  ...,  149.3031,   88.9480,\n",
            "        -160.6694])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -60.9519,  -17.1825,  107.1170,  ...,  204.5314, -172.0670,\n",
            "         -29.8518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -20.3159, -112.5875,   87.9783,  ...,    6.8254,  182.4016,\n",
            "         171.9882])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-185.9200,   31.7977,   23.6434,  ...,  -60.5019,   51.8125,\n",
            "          76.2082])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  92.8172, -162.6602,  119.4341,  ...,  -64.8059, -171.2078,\n",
            "         159.2615])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -80.4075,   90.6639,  316.2677,  ..., -167.6979, -635.1204,\n",
            "          -2.7193])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-113.6458, -742.0453,   61.4696,  ...,   12.0106,  146.6109,\n",
            "        -445.8416])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 172.1949,  144.2577,  -26.5890,  ...,  120.9578, -146.9830,\n",
            "         264.5415])\n",
            "actor loss: 1751.444492684416, critic loss: 2616502.0390625, entropy: 58646.470947265625, KL divergence: 0.0006694639991948392\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[45.22243859428498, 0, 0, 0, 2.1572373101597613], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "113エピソード目の累積報酬：-19914.0970414167, 一つ保全の回数：6740, 二つ保全の回数：1440, 三つ保全の回数：12, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -179.3139,   110.2163,   -33.7184,  ..., -1878.9922,   145.9473,\n",
            "          110.6270])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  85.2771, -433.9494,  398.8137,  ...,  349.1690,   28.9310,\n",
            "        -184.2880])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-296.6282, -126.7205,   28.0929,  ...,  -98.1425,   35.9678,\n",
            "         150.0001])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   96.9111,   -46.4744, -2352.2808,  ...,  -122.6989, -1391.3226,\n",
            "          -89.5357])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([115.9276, 230.3092, 198.1858,  ..., -35.7933,  79.3008,  39.5175])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 105.2511, -129.6078, -440.9074,  ..., -114.7624, -153.5276,\n",
            "        -227.2734])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 30.4146,  -0.8067, 193.0993,  ..., 185.5156,  51.6852,  29.0397])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  22.0514,  -21.0205,  -70.4424,  ...,   22.6510, -163.3565,\n",
            "         100.0079])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-219.6175, -111.1813, -142.5020,  ..., -686.7396,   99.2412,\n",
            "         117.9876])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   7.1110,   49.6860, -167.0169,  ...,   55.0680,   32.9440,\n",
            "         -29.6464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -165.9921,    91.4535,  -117.1171,  ..., -1481.8496,   236.0587,\n",
            "           79.6433])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([169.4888, 143.2928,  14.3935,  ..., 147.0607, -28.9549,  28.2655])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -26.6517,  -17.0078, -126.7205,  ...,   68.0773,  -58.2051,\n",
            "           1.8693])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 121.5171, -171.4434, -643.4880,  ...,   -4.9342,  144.7958,\n",
            "         118.2504])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 103.2283,  -52.0909,  156.0586,  ...,   31.8666,  142.1864,\n",
            "        -196.5199])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 119.2451,  -38.3340, -426.4272,  ...,  -68.7878, -177.1658,\n",
            "          92.0392])\n",
            "actor loss: 2019.7014754984784, critic loss: 2461511.1953125, entropy: 58750.951416015625, KL divergence: 0.00234818408096587\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[28.784119379157858, 0.7089912065772549, 0, 0, 2.3349566361031666], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "114エピソード目の累積報酬：-21163.894961020746, 一つ保全の回数：6719, 二つ保全の回数：1455, 三つ保全の回数：18, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  75.3205,  -73.3957, -278.2430,  ...,  -25.4928,   40.6683,\n",
            "        -233.8405])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -1.2669, -185.6374,   93.8939,  ..., -148.8244,  323.2051,\n",
            "        -204.7617])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([146.8171,   5.6764, 114.1562,  ...,  75.3302, -40.3050,  35.1083])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   39.8859,   141.9119,  -532.1121,  ..., -1789.8926,   230.8465,\n",
            "         -563.2314])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-155.5617, -178.7159,   96.3536,  ..., -118.4733,  235.6566,\n",
            "         175.7898])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  16.9663, -238.4970,  187.5667,  ...,  114.8169,  214.2105,\n",
            "           9.6479])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -7.7786, -86.7375, 241.6140,  ...,  15.5112,  29.9701, 112.3200])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 196.2446,  118.2007, -235.7884,  ...,  -76.7955,   60.1880,\n",
            "          14.5383])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  163.4041, -2398.0144,  -110.4746,  ...,  -208.7868,   117.9989,\n",
            "           61.7399])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-155.9506,  -80.7674,  -95.6263,  ...,  -82.9445,  -16.8208,\n",
            "          77.5453])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-113.5939,   68.2053,  214.1383,  ...,  167.8871,  215.2194,\n",
            "         107.9645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1452.5768,   170.2113,   684.6171,  ..., -1467.2992,    49.9633,\n",
            "         -177.4305])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 176.9082, -242.8605, -173.7096,  ...,  311.9739,   77.1543,\n",
            "         175.7598])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-110.3739,  -27.2707, -288.9943,  ..., -171.9343, -228.9889,\n",
            "        -304.8557])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -39.0981,   23.3666,  255.1698,  ..., -566.6976,   71.6797,\n",
            "          38.5695])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  74.8157,  277.6951,  -43.4228,  ...,  554.6814, -117.3742,\n",
            "          62.2183])\n",
            "actor loss: 1622.2339841296512, critic loss: 2399122.5859375, entropy: 58759.039306640625, KL divergence: 0.0016036793778548214\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[89.37150583099964, 43.181238315258426, 0, 0, 1.958712783386567], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "115エピソード目の累積報酬：-23215.26207723602, 一つ保全の回数：6725, 二つ保全の回数：1452, 三つ保全の回数：15, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.8467,   18.5995, -162.9396,  ...,  188.9557, -136.5455,\n",
            "         201.5531])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-336.9482, -138.1582,  266.2782,  ...,   70.1207,   25.6818,\n",
            "         -28.4195])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  49.3095,  -47.1290,  -19.5334,  ...,  -96.6039, -164.0566,\n",
            "        -108.6256])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  18.0611, -129.0020,   76.3831,  ...,  207.3537, -325.4954,\n",
            "        -778.7144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  95.9722,   30.8666, -127.1399,  ...,  224.4046,   -4.1508,\n",
            "           0.5002])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -37.5625, -136.5820,   84.2568,  ..., -274.3484,  257.5720,\n",
            "        -155.1837])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([174.5725,  90.9085, -19.7146,  ...,  46.8520,  41.3056, 187.4033])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   18.3232,   -19.7282,   156.6271,  ...,  -116.6427, -2272.0129,\n",
            "         -130.7100])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-130.5116,    0.5754,  147.2971,  ...,  -26.3277, -102.5698,\n",
            "        -149.2715])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  90.8258,  141.0242,   88.4293,  ..., -178.3807,   58.5876,\n",
            "          -8.1840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-198.7885,  135.2130,   93.0732,  ...,   57.2275,  -48.4493,\n",
            "         -70.3103])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  16.5851,  -16.3455,  -40.4135,  ...,  -99.0935,  -32.6373,\n",
            "        -253.4192])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  32.8440,  -43.1514,   26.4177,  ...,  181.2963,  117.9140,\n",
            "        -168.2034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-158.3377, -321.3650,   25.6104,  ..., -199.9635,  227.3908,\n",
            "        -204.9105])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-281.2917,   84.4107,  157.1425,  ...,  -57.8918,   64.4821,\n",
            "         -91.9927])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -96.5113,  -60.0016,  -76.6590,  ...,  125.0380, -196.7272,\n",
            "         129.1558])\n",
            "actor loss: 2047.855626832465, critic loss: 2703773.9765625, entropy: 58598.643310546875, KL divergence: 0.0008752368359846199\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[66.06152675289835, 0.013634069682953163, 0, 0, 2.8739115401098125], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "116エピソード目の累積報酬：-20343.25058887725, 一つ保全の回数：6658, 二つ保全の回数：1514, 三つ保全の回数：20, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-196.9641,  168.0965, -359.2650,  ...,  178.5194, -377.0076,\n",
            "         -11.5882])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 70.0725, -90.1911, 257.8984,  ...,  28.2421,  47.9125, -31.2636])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  12.5589,   -2.3987, -236.9893,  ..., -448.3439,  -71.2148,\n",
            "        -163.1606])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   16.0451, -1302.6180,    55.7278,  ...,   104.3187,   -46.8993,\n",
            "          -99.2098])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 167.9669, -260.4707,   52.4828,  ...,  152.3423, -199.3307,\n",
            "         -19.5878])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 343.6675, -113.0458,   54.0417,  ...,   26.6779,   43.9972,\n",
            "        -109.7350])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  66.3633,    1.0110,    7.6674,  ...,  -53.9793, -314.2744,\n",
            "        -176.7219])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  35.3992,   89.5820,  253.7937,  ...,   62.0191, -125.8227,\n",
            "          62.3618])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 102.1860, -251.8684,  215.8697,  ..., -241.2053,  -96.4308,\n",
            "          80.7288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -48.8311, -399.4608,  137.5425,  ..., -110.6726,   74.5460,\n",
            "         -37.0747])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  20.1194,  208.7393,   37.4018,  ..., -548.5706,  243.0706,\n",
            "          17.2071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -20.0336,   80.8062,  128.9244,  ..., -126.7109, -297.7247,\n",
            "          29.8055])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-576.9174,   98.3173, -162.6270,  ...,  116.7102,   56.7639,\n",
            "         162.9750])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 112.2593,  106.4054,  104.1572,  ..., -173.7558, -226.6662,\n",
            "          63.3844])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-261.7659,   55.0159,    6.8422,  ...,   32.7934,  196.9817,\n",
            "          55.9623])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -23.2401, -283.6082,  237.6156,  ...,   86.6819,  -80.0136,\n",
            "         -12.0961])\n",
            "actor loss: 1837.0144694130097, critic loss: 2153154.2890625, entropy: 58886.135498046875, KL divergence: 0.0031310276340345425\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[44.28550384531267, 13.456336799194464, 0, 0, 1.0419651837372443], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "117エピソード目の累積報酬：-21078.768500155864, 一つ保全の回数：6671, 二つ保全の回数：1510, 三つ保全の回数：11, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -91.4097,  114.0695,  -51.3459,  ..., -123.7051,   46.2862,\n",
            "         -71.1943])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  83.1416,   47.8880,  -95.2662,  ...,  -47.7805, -177.3419,\n",
            "        -246.1596])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-179.2761,  -54.5070, -142.4167,  ..., -282.4111, -103.5965,\n",
            "        -203.1888])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-106.3537, -163.4707, -191.5071,  ...,  -13.5140,   -4.4466,\n",
            "         -48.5590])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-323.2320,    1.3723,   25.4918,  ...,  -88.9424,  229.4303,\n",
            "        -199.7190])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  49.0700, -294.6024,  125.4598,  ...,   93.1458,  -21.6282,\n",
            "         -50.2643])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 559.0426,   62.2972, -101.0893,  ..., -129.5409,   23.5726,\n",
            "        -165.9269])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 173.7336,   99.0208, -235.4127,  ..., -283.7635, -136.5041,\n",
            "         368.9906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -3.4542,   71.0417,  -85.8415,  ..., -150.7209, -367.8321,\n",
            "         -87.1587])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  34.8871,   31.8976, -391.9264,  ..., -443.3842,  135.2177,\n",
            "         112.5518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  52.9480,   34.2483,  -35.2024,  ..., -394.5337,   -8.0532,\n",
            "        -367.7288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 12.7246, -89.4393, 178.0738,  ..., 189.4265,  -7.8889, 100.8847])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 277.8973, -150.2732, -160.9366,  ...,  132.3265, -120.2610,\n",
            "        -263.1279])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  115.4284,     6.0042,    66.5988,  ..., -1443.0629,  -225.9644,\n",
            "          115.6663])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -32.8638, -229.3781, -766.0552,  ...,  124.8967,  -10.2161,\n",
            "        -253.2166])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-118.1326,   28.3524,  173.7956,  ...,   38.2174, -237.6954,\n",
            "        -264.8046])\n",
            "actor loss: 1832.9240960800225, critic loss: 2081014.375, entropy: 58961.111328125, KL divergence: 0.0011788981317050339\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[38.11332602440795, 52.711309230009135, 0, 0, 1.1719535684768883], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "118エピソード目の累積報酬：-18967.707207176787, 一つ保全の回数：6646, 二つ保全の回数：1521, 三つ保全の回数：25, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-348.6232, -207.2449,   82.3365,  ...,  132.8278,  -61.0552,\n",
            "           6.9627])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 180.6462,  -84.0766,  -75.6675,  ..., -137.6882,  -43.8991,\n",
            "          78.0138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  15.6902,   92.7693,  293.2223,  ..., -182.9848,  -79.5087,\n",
            "           7.6650])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -241.7836,   212.2224,   -44.4735,  ..., -1275.1112,   275.8118,\n",
            "          187.8508])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-604.3416, -223.0506,  -32.3765,  ..., -400.0323, -585.3850,\n",
            "         -58.4497])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  92.5498,   97.7478, -109.4117,  ..., -185.2148,   61.9001,\n",
            "        -193.2732])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-112.2745,  153.1787, -169.4202,  ...,   -5.0473, -475.1197,\n",
            "         -16.6817])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -78.6450, -156.1301, -284.8504,  ...,  165.5886, -261.7107,\n",
            "         -63.8610])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 131.7281,  155.7113,   85.5277,  ...,  155.9077,  -18.1495,\n",
            "        -214.3965])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-995.8141,  121.2614,  -33.5192,  ..., -382.1262,    5.2601,\n",
            "         335.1136])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3.5523e+01, -1.6823e+02, -6.4895e-01,  ..., -1.5119e+03,\n",
            "        -2.6103e+02, -1.9150e+02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  91.3411,  -75.8562,  184.5167,  ...,  116.4030, -131.6457,\n",
            "         168.8460])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 286.0382,  -18.1495,   90.1307,  ...,  -28.6746, -106.1882,\n",
            "          35.2174])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -92.2566,   43.2304,   57.7407,  ...,  -48.3042, -202.8736,\n",
            "        -524.2314])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 220.4357,  234.1156, -120.3363,  ..., -163.6179,   22.9881,\n",
            "          20.3106])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-908.5796, -214.3965,   91.6290,  ...,  526.2601, -212.9851,\n",
            "          44.6711])\n",
            "actor loss: 2096.7782157585375, critic loss: 2155670.6484375, entropy: 58888.790283203125, KL divergence: 0.0033675861498932216\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0.30369341113451015, 10.424242493186709, 0, 0, 2.2419218368238645], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "119エピソード目の累積報酬：-26661.04138752194, 一つ保全の回数：6595, 二つ保全の回数：1574, 三つ保全の回数：23, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-86.7307,  29.2743,  69.3940,  ...,  86.2714, 148.4133, 203.9284])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 159.6287,   96.8206, -359.0813,  ...,  303.0427,   34.3341,\n",
            "        -274.4872])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -75.2932, -274.0618,  792.4096,  ...,  -54.5463,  -79.8677,\n",
            "        -177.5635])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-495.9039, -267.5478, -265.4557,  ..., -778.8290,  -20.4780,\n",
            "          89.6518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -61.0210,  108.4966,  133.2410,  ..., -299.4062,  198.1054,\n",
            "         -68.3658])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 255.6678,   52.4010, -117.3307,  ..., -203.5115, -135.5351,\n",
            "         -11.6091])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-267.8856, -351.0247,  285.0569,  ...,  -29.5224,   49.2105,\n",
            "        -254.8499])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 305.7675,  135.3077, -104.3163,  ...,  -28.3838,  -23.1176,\n",
            "         164.0481])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  160.1771,   280.6383,   -90.1551,  ...,   -31.0808, -1467.5760,\n",
            "          -50.0299])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  65.8546,   87.8927,  -19.6718,  ...,  -90.1863,   85.9024,\n",
            "        -187.5424])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -16.3678,  -97.7645, -342.6773,  ...,  149.8976, -201.7943,\n",
            "         116.4074])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 158.3587,   26.1984,   73.7611,  ..., -127.8599,  232.8850,\n",
            "          55.2810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -15.8065,   50.3702,   -7.0558,  ...,   28.1969,    6.2990,\n",
            "        -264.8636])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 347.6375, -291.7538, -314.0026,  ...,  -53.0535, -145.2620,\n",
            "         139.9221])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-173.3611,  295.0269,   26.4462,  ...,    3.8229,  -71.9929,\n",
            "          81.2841])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -12.1951, -102.2593,    6.6623,  ...,   12.6933, -378.5554,\n",
            "          38.8838])\n",
            "actor loss: 1733.944679025377, critic loss: 2071869.5234375, entropy: 58736.501953125, KL divergence: 0.0008170963860575519\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[11.60009046110243, 81.43278228205065, 0, 0, 1.55795589898527], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "120エピソード目の累積報酬：-18127.98781620343, 一つ保全の回数：6582, 二つ保全の回数：1590, 三つ保全の回数：20, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-158.6452,  136.9800,  -82.0928,  ...,   95.2212,   35.7701,\n",
            "          72.7479])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-169.6432,  -32.0725, -340.6241,  ...,  -56.0725,  144.7836,\n",
            "          25.5608])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 557.0660, -423.3716, -556.9574,  ...,  -43.6285,  -70.9738,\n",
            "         -95.0939])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -269.8008,    23.7721,   -58.1724,  ..., -1997.8691,   -88.4645,\n",
            "         -141.6431])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -1.3149,  203.1672, -416.2431,  ...,   28.4435, -367.2766,\n",
            "        -165.2716])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 357.5776, -106.9341,   -0.8891,  ..., -135.7026,   -5.3604,\n",
            "        -311.1407])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-455.7183,   90.7562, -105.2041,  ...,  162.0602,  355.2362,\n",
            "         -30.5031])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 192.5106, -286.5070,  -28.2836,  ..., -721.8201,  150.3873,\n",
            "          83.9685])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  41.2370,  146.0726,  -25.9867,  ...,   40.0587,   57.0754,\n",
            "        -200.8035])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-187.8055, -188.1147,   65.5111,  ...,    1.8921, -237.0438,\n",
            "          64.9770])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 64.3991, -12.8691,  90.9132,  ..., -57.1132,   3.6176, 390.3301])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-102.0614,  176.3797,  -17.8242,  ...,  -13.5986,    6.6616,\n",
            "         114.9813])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 350.2948,  243.6959,  -51.1530,  ...,   89.8473,  380.2993,\n",
            "        -179.6505])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1404.1582,    34.0675,  -243.2147,  ...,    -6.4545,   134.9561,\n",
            "            4.1082])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-168.5743, -132.5318,  -79.8479,  ...,  -29.9418,  -92.6258,\n",
            "         167.5001])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 119.0566,  121.8118,  235.3265,  ...,  130.8628,   66.0639,\n",
            "        -122.9855])\n",
            "actor loss: 1697.9420634989187, critic loss: 2517878.1484375, entropy: 59068.0068359375, KL divergence: 0.0037501549436929843\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[38.40611939578205, 80.52082708083621, 0, 0, 1.6199710841157908], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "121エピソード目の累積報酬：-20539.914473335582, 一つ保全の回数：6575, 二つ保全の回数：1601, 三つ保全の回数：16, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-157.5615,  288.5975,  174.0832,  ..., -153.2807,  143.2323,\n",
            "         -83.5708])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -18.3040,   34.9460, -869.1584,  ...,   34.3545,    6.5751,\n",
            "        -575.2491])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -45.9062,  352.5842, -138.9868,  ..., -254.2148,  193.4160,\n",
            "          10.2866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  62.4501, -232.4255,  109.4178,  ...,  299.8860,  -87.8348,\n",
            "         162.0477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-256.0940,  102.8295,  -28.1616,  ...,  173.1117,  -85.9743,\n",
            "         -81.2823])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-167.7291, -178.7646, -283.2358,  ...,   95.7260,  -97.6532,\n",
            "         -97.8512])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-476.3520,  114.0846,  288.5975,  ..., -259.2926, -132.6960,\n",
            "           5.1804])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 289.5425, -109.0346,  -88.1560,  ...,  280.1183,  205.7162,\n",
            "         127.2954])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  98.3857,   84.1641,  358.5425,  ..., -304.5762,  -37.0788,\n",
            "        -172.5186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-413.7781, -267.9405,  117.3003,  ...,   82.7896, -275.2220,\n",
            "          30.1644])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1785.1775,  -333.8561,   119.5061,  ...,   100.3556,   -75.3068,\n",
            "           35.4247])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 157.4382, -292.0205,  232.0089,  ..., -240.4572,   17.5821,\n",
            "         -49.0539])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 53.5390, -41.6071, -26.4893,  ..., -63.7509,  13.6964, -22.1959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-63.4902, 151.3803, -84.3870,  ..., 104.9607, -51.0090, -22.5193])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-36.5339, -60.0658, 290.2022,  ..., -63.7127, -97.7156,  57.2041])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  57.9242, -222.9919, -140.0994,  ...,   87.5932,  140.4228,\n",
            "          12.6707])\n",
            "actor loss: 1566.2012156134188, critic loss: 2111692.3515625, entropy: 59271.529541015625, KL divergence: 0.009077735983373037\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[25.52480623692437, 132.3256056125756, 0, 1, 1.4417683974857334], 離散行動：[0, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "122エピソード目の累積報酬：-22942.74560023096, 一つ保全の回数：6587, 二つ保全の回数：1582, 三つ保全の回数：23, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-105.9612, -304.8124, -223.4898,  ...,   23.2145,  -88.7953,\n",
            "          91.6208])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 246.5845,  427.3588, -159.6597,  ...,   71.4528,  -85.9586,\n",
            "         304.6474])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -60.5846, -158.7738,   44.5522,  ...,  322.8228,  -83.0877,\n",
            "          36.1837])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-264.6801, -562.6019, -733.3256,  ...,  -88.4751,  146.4363,\n",
            "        -177.5330])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-147.4335, -774.3488,  -57.5115,  ...,  177.2196,  133.0654,\n",
            "         323.7609])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-351.3212, -248.0467, -489.5958,  ...,   91.3012,  113.0692,\n",
            "         286.9254])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 124.1768,  365.9391, -382.3186,  ..., -173.6252, -225.8231,\n",
            "        -289.3088])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  11.3987, -136.5921,  -73.2470,  ...,  -46.2397, -133.6958,\n",
            "        -354.7072])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  66.5963, -104.3371,  -86.9915,  ...,   10.7983,  444.9129,\n",
            "          78.8457])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-177.2157,  113.7908, -552.9573,  ...,  127.6507,   -3.9386,\n",
            "        -571.8766])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-296.7977,  327.3025, -131.1224,  ..., -171.1553,   78.0775,\n",
            "         -57.2316])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-110.0519,  -53.8874,   65.0929,  ...,   32.7907, -350.8698,\n",
            "        -107.4864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  62.2315,   47.9290,  -19.2149,  ...,  -44.3783, -521.4785,\n",
            "        -631.9567])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-186.5349,   20.1145,   -1.5239,  ...,    2.3547,   12.5773,\n",
            "         207.7588])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-155.6202, -563.0750,   57.3464,  ...,  -54.8835,  695.8506,\n",
            "        -197.6275])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-103.5388,  327.0011,  322.8228,  ...,  204.5582,   81.1781,\n",
            "        -189.4323])\n",
            "actor loss: 1810.6056870829739, critic loss: 1847433.71875, entropy: 59277.238037109375, KL divergence: 0.006213919611399057\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 1.614163448042231, 0, 0, 1.0604043893027941], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "123エピソード目の累積報酬：-20797.13752101116, 一つ保全の回数：6515, 二つ保全の回数：1660, 三つ保全の回数：17, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096, -20797.13752101116]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-367.1746,  -74.3892, -254.8200,  ..., -213.7045,   17.2714,\n",
            "          78.4651])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 245.7001,   69.8279, -302.0724,  ...,  146.1358, -206.8006,\n",
            "          73.5965])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -92.8635, -1369.6713,   102.5429,  ...,  -380.2778,  -245.2840,\n",
            "         -129.6964])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 52.4033, 140.6790,   6.8411,  ...,  58.4754, 212.6913,  93.1877])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -17.7375, -213.9937,   67.5587,  ...,  180.8560,  -14.3540,\n",
            "         408.1891])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-268.7940,  219.5587, -171.5576,  ...,  244.2022, -102.5395,\n",
            "         -80.3956])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  90.7106,   -5.0898, -152.8803,  ...,  391.6035, -269.3322,\n",
            "         -83.6011])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 249.0330,  -79.7591, -121.2825,  ..., -334.8414,  127.3008,\n",
            "        -218.5446])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 227.4514, -402.8243, -208.4732,  ...,  371.2278,  -60.4945,\n",
            "        -139.5589])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-482.3535,   39.7041,  135.7809,  ..., -100.6060,  103.8151,\n",
            "         -61.8144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  577.0834,   119.4538,   -92.3994,  ..., -1786.2439,   197.0224,\n",
            "         -169.3096])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  43.6544,   21.8459,  -13.7677,  ...,  147.0139,   60.2389,\n",
            "        -455.7764])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-276.4144, -152.0055,   -6.8187,  ..., -368.0669, -523.4924,\n",
            "         -74.7169])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([236.6461, -25.7399, -13.1360,  ..., 113.6689,  49.5099, 148.2040])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 161.5805,  -31.5897,  -13.0751,  ..., -488.8229, -235.5721,\n",
            "        -175.1772])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  47.2215, -286.8547,  301.6853,  ..., -543.1691,  176.3027,\n",
            "          13.1592])\n",
            "actor loss: 1957.2123726817774, critic loss: 1709590.62109375, entropy: 59158.24462890625, KL divergence: 0.002231298693849498\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[27.964260395971763, 26.28670725913977, 0, 0, 1.617864425641387], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "124エピソード目の累積報酬：-20982.11917246192, 一つ保全の回数：6482, 二つ保全の回数：1675, 三つ保全の回数：35, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096, -20797.13752101116, -20982.11917246192]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -22.7328, -284.5522,   59.3609,  ...,  -18.1136,   68.5121,\n",
            "        -189.4252])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 135.2733,  150.8244,  -39.1366,  ...,   92.6040, -197.2152,\n",
            "         -35.3668])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-112.6082, -510.1967,   10.5325,  ...,  206.9344,  238.3396,\n",
            "          71.6478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  23.0256, -355.7731, -534.2783,  ...,   20.2703,  -20.6203,\n",
            "          38.7773])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -554.1724,   300.5822, -1486.1205,  ...,   142.9532,  -262.3363,\n",
            "          -25.9838])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-131.6523,  174.5186, -171.2695,  ..., -201.1338, -334.8138,\n",
            "        -102.6509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 195.9229,   61.4239,   98.0875,  ..., -209.0854, -314.0521,\n",
            "         -70.6730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 60.8489,  19.1572,  77.4164,  ...,  46.9308,  65.7233, 170.3904])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  150.9376, -1372.6276,  -152.3382,  ...,  -267.5752,  -182.9397,\n",
            "         -166.8773])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-246.0188, -440.9179,  182.6058,  ..., -480.7344,   52.8970,\n",
            "         178.1693])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-250.4415, -579.3040,   55.6016,  ...,  168.7589,  101.6064,\n",
            "        -263.5393])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 104.7052,   66.5277, -223.5273,  ..., -192.8891, -346.6391,\n",
            "         -87.7115])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  79.4371,  288.6395,  199.2075,  ..., -184.0063,   67.0634,\n",
            "         -80.5084])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-163.6573,  -46.2527, -788.1758,  ...,  272.4689,  106.1004,\n",
            "          33.9669])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.6710,  195.1571,  149.6781,  ...,  433.5088, -102.0848,\n",
            "         145.7363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 104.0959, -111.5137,  -76.3824,  ...,    3.5538, -141.0726,\n",
            "         131.1548])\n",
            "actor loss: 1823.2579642263943, critic loss: 1485805.99609375, entropy: 58989.2236328125, KL divergence: 0.0019346219382112358\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[12.658006207289823, 47.35134771334505, 0, 0, 1.0708689902266595], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "125エピソード目の累積報酬：-20752.765926810433, 一つ保全の回数：6484, 二つ保全の回数：1685, 三つ保全の回数：23, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096, -20797.13752101116, -20982.11917246192, -20752.765926810433]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  149.4650,   -17.5262,   116.9806,  ...,    -1.8144, -1047.3389,\n",
            "           92.7519])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-171.4607,  113.3978,  -81.9417,  ...,  -26.0777, -165.2515,\n",
            "         119.9966])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-502.2072, -108.4481,   82.8391,  ...,  103.8086,  139.0178,\n",
            "          52.4366])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-506.0323,   41.4323,   60.4586,  ...,   66.0979, -127.5639,\n",
            "        -286.0492])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-150.6662, -108.9435, -402.9577,  ..., -147.7861,   81.6237,\n",
            "          -3.9336])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-245.2181, -271.3171,  173.6873,  ...,  -32.6085, -141.7108,\n",
            "          33.1909])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 12.1869, 278.7330, 221.1320,  ..., 176.2469,  55.1002,  56.8687])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 237.2915,    4.3278,  -41.8778,  ..., -150.4066,   75.7603,\n",
            "        -209.2122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -25.9728,   83.2588, -267.1849,  ...,  175.2537,  -62.0013,\n",
            "        -326.6843])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-122.9102,  -52.6787, -112.1348,  ...,  -83.6604, -301.5565,\n",
            "          59.8598])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 133.7349,   82.0588, -269.1380,  ...,  -15.3209,  105.1290,\n",
            "        -100.6029])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 188.9176, -278.3288,   58.9257,  ..., -129.3131,  -66.0349,\n",
            "          78.4574])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-245.6210,   79.0532,  112.4665,  ...,   56.9742,   48.2399,\n",
            "        -298.4839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -81.0119,  139.4830,  116.1808,  ...,  170.5533,   48.7088,\n",
            "        -215.3072])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 128.6247, -146.1694, -198.7561,  ..., -248.0152,  -50.0756,\n",
            "         112.3026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -34.9945, -356.4012, -312.0790,  ..., -827.4920,   59.7949,\n",
            "         145.5035])\n",
            "actor loss: 1720.648904947246, critic loss: 1420333.52734375, entropy: 58738.637939453125, KL divergence: 0.012739648954724854\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[46.65379343554609, 0, 0, 0, 1.4036765931266424], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "126エピソード目の累積報酬：-25319.370869404233, 一つ保全の回数：6512, 二つ保全の回数：1649, 三つ保全の回数：31, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096, -20797.13752101116, -20982.11917246192, -20752.765926810433, -25319.370869404233]\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  29.9900, -144.4601,   98.1015,  ..., -154.9518,  260.8858,\n",
            "        -254.1280])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  58.2048, -189.5753,   82.6840,  ...,   56.7137, -317.2855,\n",
            "         -23.3601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  117.3201,  -253.7970,    60.6562,  ...,    25.2189,    69.6598,\n",
            "        -1653.1110])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 294.1940,  179.9563,  137.2847,  ...,  -67.5820,   75.2026,\n",
            "        -183.6217])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  78.2241,   46.9189,  107.3843,  ...,    5.3104,  148.6563,\n",
            "        -175.0600])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -36.2352, -246.9850,   84.0070,  ...,  -16.0470,   72.7024,\n",
            "         229.9201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 177.5095,   44.1580, -389.2747,  ...,   -4.9536,  153.3732,\n",
            "         192.0692])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  134.7921,  -492.2549, -1125.6482,  ...,    -4.8524,    -6.1497,\n",
            "          137.5268])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-47.4335, -34.7637,  95.1105,  ..., -21.6746,  10.8453, -62.8172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-210.2055,    1.4768,  112.2713,  ...,   61.5061,  151.3421,\n",
            "          48.9669])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 142.0399,  -74.8669,  127.8275,  ...,   -2.1721, -575.9941,\n",
            "        -102.5057])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-138.6944,  -12.7043,  -22.7439,  ..., -224.2521, -484.4674,\n",
            "          -8.6093])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-114.5258, -576.2936,    3.7396,  ..., -248.8076, -299.9616,\n",
            "         200.1776])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  27.9795,  183.2319,  232.9013,  ...,  -75.8048,  -73.4287,\n",
            "        -460.8447])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-388.6115, -250.5466,   22.3939,  ...,   28.3263,  176.6542,\n",
            "        -183.4696])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  77.1295,  -40.5315,  241.3252,  ..., -156.1744,  153.6297,\n",
            "        -166.6081])\n",
            "actor loss: 1495.9097779155124, critic loss: 1491343.61328125, entropy: 58712.61328125, KL divergence: 0.008076816123849504\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[23.85296026988586, 40.98195314183396, 0, 0, 1.6653010726815307], 離散行動：[1, 1], 連続行動：0.5\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "127エピソード目の累積報酬：-21091.96162347202, 一つ保全の回数：6580, 二つ保全の回数：1592, 三つ保全の回数：20, 違反回数：0,episode_reward_history：[-282953.0525161782, -292983.54766904365, -303485.816303672, -121852.26453490621, -176901.69677113762, -141413.43904974934, -117561.83075308768, -75351.20625236488, -47587.63026002425, -57347.58907877218, -50028.14086910474, -28125.335748895734, -34549.52526561325, -21299.30732337322, -21915.051717157094, -34016.82841458073, -25720.087656240237, -29567.732244862367, -24511.193313977394, -28081.712111364395, -26944.805572337827, -27397.4139926584, -28293.47757100034, -21980.363874436527, -27357.00298884954, -24690.383995709977, -23735.597581978844, -31083.157157373604, -27499.614414734326, -22129.569179865564, -23211.755916974114, -25273.596170518336, -30075.730853954607, -22662.414940247687, -24247.125952661954, -23949.52811975116, -29844.04606342391, -27243.823311605564, -25882.76114575636, -26038.788905050624, -22603.141609311006, -24734.17237726064, -26393.076645984667, -22565.415971282062, -23044.47659577465, -23814.767979638615, -28551.9301261481, -24917.749755662404, -25337.94575587208, -23805.951472199493, -28174.350037956385, -23493.119403083107, -22342.707117480757, -25292.67491216678, -21887.574031240696, -22896.30519222649, -25056.148256185123, -20358.181621124313, -20749.60500177104, -22765.788548306482, -25510.229374526705, -24956.92435889103, -26463.25925935395, -22577.33289287758, -19039.549942147765, -22809.712104322745, -22378.541843758092, -22523.68077286674, -21744.472410006434, -19888.058218363374, -27681.57467485969, -20857.365750052602, -24582.618135710025, -20345.258218218372, -22860.54932644582, -24866.534585932255, -22525.938654018817, -22624.91653168549, -24660.538244237076, -20997.22990589221, -20773.190783154812, -23738.015460517923, -23564.70506220424, -20962.77881779239, -21173.65785882577, -26564.886787827723, -21535.354368824905, -24252.37141700874, -22932.701936714555, -27148.416610185417, -21778.892758673097, -23187.850027415872, -22462.70531714696, -22000.13585923402, -23657.69727516782, -21876.503286638057, -24957.38447292349, -19528.92923377098, -22644.19589263626, -21245.86149471463, -17141.777692249514, -18384.641118152456, -21087.017058786143, -25188.57646721826, -22382.593746753602, -23312.39406798249, -20655.326372754014, -19324.757056728507, -22654.18442604987, -20081.608527653665, -21435.34747184617, -20331.809050030355, -22526.73372793888, -19914.0970414167, -21163.894961020746, -23215.26207723602, -20343.25058887725, -21078.768500155864, -18967.707207176787, -26661.04138752194, -18127.98781620343, -20539.914473335582, -22942.74560023096, -20797.13752101116, -20982.11917246192, -20752.765926810433, -25319.370869404233, -21091.96162347202]\n"
          ]
        }
      ],
      "source": [
        "num_episode =128#8*16*4で90分\n",
        "threshold = 0 #cnt学習を開始するエピソード\n",
        "best_reward = -np.inf\n",
        "episode_reward_history = []\n",
        "avg_cost = 0\n",
        "critic_value0_history=[]\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    episode_reward = 0\n",
        "    operation_time = 0\n",
        "    one_action = 0\n",
        "    two_action = 0\n",
        "    three_action = 0\n",
        "    penalty_action = 0\n",
        "    #level_ohe, load_total = env.init_random()\n",
        "    levels, flags, load_total = env.init_random()\n",
        "    if episode % 100 == 0:\n",
        "        interval_time_episode = time.time()\n",
        "    interval_time_episode = time.time()\n",
        "    for _ in range(1024*8):#1024*8\n",
        "        #state = level_ohe + list([inventory,demand])\n",
        "        #print(levels,\"levels\")\n",
        "        state = levels + flags + list([load_total])\n",
        "        #print(state)\n",
        "        act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action(state)\n",
        "        act_dsc_list = [int(bit) for bit in format(act_dsc.item(), f'0{env.n_units}b')]\n",
        "        if sum(act_dsc_list) == 2:\n",
        "            one_action += 1\n",
        "        elif sum(act_dsc_list) == 1:\n",
        "            two_action += 1\n",
        "        elif sum(act_dsc_list) == 0:\n",
        "            three_action += 1\n",
        "        act_cnt_np = act_cnt.squeeze().cpu().numpy().copy()\n",
        "        act_cnt_np = 0.5 #補正\n",
        "\n",
        "        if act_cnt_np<0.5:\n",
        "          env.cntCount[0]+=1\n",
        "        else:\n",
        "          env.cntCount[1]+=1\n",
        "        #print(act_dsc_list,act_cnt_np)\n",
        "        #reward, level_ohe_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "        reward, levels_next, flags_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "\n",
        "        episode_reward = episode_reward *0.99 + reward\n",
        "        #penalty_action += flag\n",
        "        #if remain_interval > remain_interval_next:\n",
        "            #operation_time += (remain_interval+1)/2*interval - (remain_interval_next+1)/2*interval\n",
        "        #else:\n",
        "            #operation_time += (remain_interval_next + 1) / 2 * interval\n",
        "        agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob_dsc, log_prob_cnt, val, reward, operation_time)\n",
        "        levels = levels_next\n",
        "        flags = flags_next\n",
        "        #mstatus_ohe = mstatus_ohe_next\n",
        "        #inventory = inventory_next\n",
        "        #demand = demand_next\n",
        "        #remain_interval = remain_interval_next\n",
        "        load_total = load_total_next\n",
        "    #print(f'{episode}エピソード目の時間：{time.time()-interval_time_episode}')\n",
        "    interval_time_episode = time.time()\n",
        "    agent.learn(episode, threshold)\n",
        "\n",
        "    old_agent = Agent(n_units=n_units,\n",
        "                        input_dims=input_size,\n",
        "                        n_states=n_states,\n",
        "                        MAX_maintenance_time=MAX_maintenance_time,\n",
        "                        beta=beta,\n",
        "                        interval=interval,\n",
        "                        alpha_actor=alpha_actor,\n",
        "                        alpha_critic=alpha_critic,\n",
        "                        policy_clip=policy_clip,\n",
        "                        batch_size=batch_size,\n",
        "                        n_epochs=n_epochs)\n",
        "    if episode != 0:\n",
        "        old_agent.load_models()\n",
        "\n",
        "    #if Check_convergence(agent, old_agent, n_units, n_states, MAX_maintenance_time):\n",
        "    #    break\n",
        "\n",
        "\n",
        "\n",
        "    agent.save_models()\n",
        "    print(f'状態{state}, 離散行動：{act_dsc_list}, 連続行動：{act_cnt_np}')\n",
        "    print(f'[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [{env.replace_chance}, {env.failure_keep1}, {env.failure_keep2}, {env.failure_keep3}]')\n",
        "    env.replace_chance = 0\n",
        "    env.failure_keep1 = 0\n",
        "    env.failure_keep2 = 0\n",
        "    env.failure_keep3 = 0\n",
        "    #print(f'{episode}エピソード目の学習時間：{time.time()-interval_time_episode}')\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    print(f'{episode}エピソード目の累積報酬：{episode_reward}, 一つ保全の回数：{one_action}, 二つ保全の回数：{two_action}, 三つ保全の回数：{three_action}, 違反回数：{penalty_action},episode_reward_history：{episode_reward_history}')\n",
        "\n",
        "    critic_value0_history.append(float(agent.critic(state0)[0][0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncPGOL29TYPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2ad53f0-b867-4d36-a838-d67d646011d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHLLAogeTYPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee3a4c85-fb07-4add-a3a1-97a546807492"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_learning_curve(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# エージェントの学習\n",
        "# (agent.train()の呼び出しなど)\n",
        "\n",
        "# 学習後のエージェントの評価\n",
        "#evaluate_agent(agent, env, num_episodes=10)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plot_learning_curve(episode_reward_history, title=\"PPO Learning Curve\", label=\"Total reward\")\n",
        "plot_learning_curve(agent.loss_history_detail, title=\"loss curve\", label=\"Total loss (actor loss +  critic loss) \")\n",
        "plot_learning_curve(agent.actor_loss_history, title=\"actor loss curve\", label=\"actor loss\")\n",
        "plot_learning_curve(agent.critic_loss_history, title=\"critic loss curve\", label=\"critic loss\")\n",
        "plot_learning_curve(agent.entropy_history, title=\"entropy curve\", label=\"entropy\")\n",
        "plot_learning_curve(agent.kl_divergence_history, title=\"kl divergence curve\", label=\"kl divergence\")\n",
        "plot_learning_curve(critic_value0_history, title=\"PPO Learning Curve\", label=\"critic_value0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3bLWs2RTYPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b43cfc8a-76d9-47bb-fdf4-7e94efdad92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[0.0075, 0.0093, 0.0074, 0.9759]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[0.0000, 5.0000, 0.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[0.0064, 0.0123, 0.0058, 0.9756]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0050, 0.0168, 0.0041, 0.9741]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0040, 0.0229, 0.0029, 0.9703]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0031, 0.0312, 0.0020, 0.9637]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0425, 0.0014, 0.9537]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0019, 0.0575, 0.0010, 0.9396]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4710e-03, 7.7526e-02, 6.8674e-04, 9.2032e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1320e-03, 1.0375e-01, 4.7314e-04, 8.9464e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.6274e-04, 1.3751e-01, 3.2283e-04, 8.6131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4951e-04, 1.8003e-01, 2.1759e-04, 8.1910e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8161e-04, 2.3215e-01, 1.4444e-04, 7.6723e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.5065e-04, 2.9393e-01, 9.4152e-05, 7.0563e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4994e-04, 3.6434e-01, 6.0082e-05, 6.3535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7399e-04, 4.4108e-01, 3.7446e-05, 5.5871e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1812e-04, 5.2073e-01, 2.2759e-05, 4.7913e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.8177e-05, 5.9934e-01, 1.3485e-05, 4.0057e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.0490e-05, 6.7314e-01, 7.7971e-06, 3.2680e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1885e-05, 7.3926e-01, 4.4083e-06, 2.6070e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.9744e-05, 7.9606e-01, 2.4438e-06, 2.0392e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.2084e-05, 8.4641e-01, 1.3349e-06, 1.5358e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[7.2535e-06, 8.8354e-01, 7.1735e-07, 1.1645e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[5.0000, 0.0000, 0.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[0.0064, 0.0123, 0.0058, 0.9756]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[5.0000, 5.0000, 0.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[0.0071, 0.0129, 0.0063, 0.9737]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0061, 0.0173, 0.0049, 0.9716]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0050, 0.0239, 0.0036, 0.9675]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0040, 0.0325, 0.0026, 0.9609]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0031, 0.0442, 0.0018, 0.9509]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0600, 0.0012, 0.9364]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8699e-03, 8.0793e-02, 8.6542e-04, 9.1647e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4373e-03, 1.0801e-01, 5.9556e-04, 8.8996e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0937e-03, 1.4294e-01, 4.0575e-04, 8.5556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.2181e-04, 1.8678e-01, 2.7294e-04, 8.1212e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.0790e-04, 2.4029e-01, 1.8076e-04, 7.5892e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.4133e-04, 3.0339e-01, 1.1749e-04, 6.9605e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1355e-04, 3.7487e-01, 7.4730e-05, 6.2475e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1751e-04, 4.5224e-01, 4.6410e-05, 5.4749e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4712e-04, 5.3199e-01, 2.8104e-05, 4.6783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.7019e-05, 6.1013e-01, 1.6593e-05, 3.8975e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.2447e-05, 6.8300e-01, 9.5619e-06, 3.1693e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9318e-05, 7.4787e-01, 5.3898e-06, 2.5209e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4283e-05, 8.0328e-01, 2.9802e-06, 1.9669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.4814e-05, 8.5214e-01, 1.6230e-06, 1.4784e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[8.8773e-06, 8.8807e-01, 8.7072e-07, 1.1192e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0050, 0.0168, 0.0041, 0.9741]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0061, 0.0173, 0.0049, 0.9716]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0060, 0.0187, 0.0046, 0.9707]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0054, 0.0246, 0.0038, 0.9663]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0046, 0.0339, 0.0029, 0.9587]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0038, 0.0463, 0.0021, 0.9478]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0030, 0.0629, 0.0015, 0.9326]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0845, 0.0011, 0.9121]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8247e-03, 1.1265e-01, 7.4679e-04, 8.8478e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3870e-03, 1.4875e-01, 5.0879e-04, 8.4935e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0406e-03, 1.9388e-01, 3.4213e-04, 8.0474e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.6862e-04, 2.4873e-01, 2.2647e-04, 7.5028e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.5666e-04, 3.1318e-01, 1.4684e-04, 6.8612e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9419e-04, 3.8570e-01, 9.3099e-05, 6.1381e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7248e-04, 4.6367e-01, 5.7613e-05, 5.3600e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8363e-04, 5.4344e-01, 3.4761e-05, 4.5634e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2066e-04, 6.2103e-01, 2.0450e-05, 3.7883e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.7408e-05, 6.9289e-01, 1.1745e-05, 3.0702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8593e-05, 7.5646e-01, 6.6010e-06, 2.4348e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.9936e-05, 8.1047e-01, 3.6407e-06, 1.8950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.8223e-05, 8.5786e-01, 1.9783e-06, 1.4212e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.0902e-05, 8.9257e-01, 1.0596e-06, 1.0742e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0040, 0.0229, 0.0029, 0.9703]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0050, 0.0239, 0.0036, 0.9675]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0054, 0.0246, 0.0038, 0.9663]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0050, 0.0270, 0.0033, 0.9647]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0046, 0.0347, 0.0028, 0.9579]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0039, 0.0473, 0.0021, 0.9467]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0034, 0.0650, 0.0017, 0.9300]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0028, 0.0878, 0.0012, 0.9082]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0022, 0.1178, 0.0009, 0.8791]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7034e-03, 1.5515e-01, 6.1342e-04, 8.4253e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2906e-03, 2.0155e-01, 4.1708e-04, 7.9674e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6805e-04, 2.5816e-01, 2.8082e-04, 7.4059e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9957e-04, 3.2385e-01, 1.8191e-04, 6.7527e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9395e-04, 3.9725e-01, 1.1512e-04, 6.0215e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.4038e-04, 4.7556e-01, 7.1105e-05, 5.2403e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2869e-04, 5.5508e-01, 4.2820e-05, 4.4464e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4985e-04, 6.3187e-01, 2.5149e-05, 3.6795e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.5909e-05, 7.0260e-01, 1.4421e-05, 2.9728e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.0033e-05, 7.6485e-01, 8.0812e-06, 2.3508e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6893e-05, 8.1745e-01, 4.4462e-06, 1.8251e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.2409e-05, 8.6339e-01, 2.4108e-06, 1.3659e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.3386e-05, 8.9691e-01, 1.2892e-06, 1.0307e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0031, 0.0312, 0.0020, 0.9637]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0040, 0.0325, 0.0026, 0.9609]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0046, 0.0339, 0.0029, 0.9587]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0046, 0.0347, 0.0028, 0.9579]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0041, 0.0388, 0.0024, 0.9547]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0038, 0.0491, 0.0020, 0.9450]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0033, 0.0658, 0.0016, 0.9293]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0028, 0.0890, 0.0012, 0.9070]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0023, 0.1206, 0.0009, 0.8761]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.9237e-03, 1.6022e-01, 6.7683e-04, 8.3718e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5140e-03, 2.0929e-01, 4.7885e-04, 7.8872e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1462e-03, 2.6761e-01, 3.2557e-04, 7.3092e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.3506e-04, 3.3430e-01, 2.1287e-04, 6.6465e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.9561e-04, 4.0842e-01, 1.3626e-04, 5.9084e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.1730e-04, 4.8752e-01, 8.5696e-05, 5.1198e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.8402e-04, 5.6752e-01, 5.2350e-05, 4.3214e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8539e-04, 6.4355e-01, 3.0628e-05, 3.5623e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1822e-04, 7.1297e-01, 1.7506e-05, 2.8690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.3841e-05, 7.7361e-01, 9.8005e-06, 2.2630e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.5305e-05, 8.2460e-01, 5.3896e-06, 1.7535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7424e-05, 8.6874e-01, 2.9171e-06, 1.3123e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.6373e-05, 9.0104e-01, 1.5610e-06, 9.8943e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0425, 0.0014, 0.9537]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0031, 0.0442, 0.0018, 0.9509]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0038, 0.0463, 0.0021, 0.9478]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0039, 0.0473, 0.0021, 0.9467]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0038, 0.0491, 0.0020, 0.9450]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0034, 0.0555, 0.0017, 0.9394]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0032, 0.0695, 0.0015, 0.9258]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0027, 0.0912, 0.0012, 0.9049]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2796e-03, 1.2167e-01, 8.6518e-04, 8.7518e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8828e-03, 1.6120e-01, 6.4433e-04, 8.3627e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5614e-03, 2.1261e-01, 4.8108e-04, 7.8535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2514e-03, 2.7378e-01, 3.4708e-04, 7.2462e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.5337e-04, 3.4391e-01, 2.3771e-04, 6.5490e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0266e-04, 4.2090e-01, 1.5748e-04, 5.7824e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8697e-04, 4.9971e-01, 9.7911e-05, 4.9970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.2879e-04, 5.7851e-01, 5.9326e-05, 4.2110e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1725e-04, 6.5374e-01, 3.5184e-05, 3.4601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4089e-04, 7.2231e-01, 2.0482e-05, 2.7753e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.9381e-05, 7.8192e-01, 1.1663e-05, 2.1798e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.5513e-05, 8.3179e-01, 6.5010e-06, 1.6814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.3529e-05, 8.7441e-01, 3.5109e-06, 1.2556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.9986e-05, 9.0546e-01, 1.8757e-06, 9.4516e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0019, 0.0575, 0.0010, 0.9396]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0600, 0.0012, 0.9364]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0030, 0.0629, 0.0015, 0.9326]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0034, 0.0650, 0.0017, 0.9300]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0033, 0.0658, 0.0016, 0.9293]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0032, 0.0695, 0.0015, 0.9258]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0028, 0.0788, 0.0012, 0.9172]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0026, 0.0977, 0.0010, 0.8987]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2482e-03, 1.2576e-01, 8.2378e-04, 8.7117e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8597e-03, 1.6431e-01, 6.1723e-04, 8.3322e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5085e-03, 2.1394e-01, 4.5144e-04, 7.8410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2060e-03, 2.7425e-01, 3.2544e-04, 7.2422e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6051e-04, 3.4704e-01, 2.3330e-04, 6.5176e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.4299e-04, 4.2762e-01, 1.6233e-04, 5.7147e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.4023e-04, 5.0917e-01, 1.0619e-04, 4.9018e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.7767e-04, 5.9005e-01, 6.6735e-05, 4.0950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.5277e-04, 6.6516e-01, 4.0093e-05, 3.3455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6227e-04, 7.3187e-01, 2.3095e-05, 2.6794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0209e-04, 7.8949e-01, 1.3039e-05, 2.1040e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.3440e-05, 8.3771e-01, 7.2730e-06, 1.6222e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.9553e-05, 8.7930e-01, 4.0646e-06, 1.2066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.3956e-05, 9.0945e-01, 2.2097e-06, 9.0525e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4710e-03, 7.7526e-02, 6.8674e-04, 9.2032e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8699e-03, 8.0793e-02, 8.6542e-04, 9.1647e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0024, 0.0845, 0.0011, 0.9121]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0028, 0.0878, 0.0012, 0.9082]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0028, 0.0890, 0.0012, 0.9070]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0027, 0.0912, 0.0012, 0.9049]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0026, 0.0977, 0.0010, 0.8987]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2549e-03, 1.1074e-01, 8.4707e-04, 8.8616e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0866e-03, 1.3556e-01, 7.2671e-04, 8.6162e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8124e-03, 1.7119e-01, 5.7857e-04, 8.2642e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4954e-03, 2.1868e-01, 4.3347e-04, 7.7939e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1866e-03, 2.7856e-01, 3.1056e-04, 7.1994e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.2441e-04, 3.4834e-01, 2.1813e-04, 6.5051e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0564e-04, 4.2622e-01, 1.5016e-04, 5.7293e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.3324e-04, 5.1122e-01, 1.0211e-04, 4.8814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9259e-04, 5.9629e-01, 6.7585e-05, 4.0325e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7092e-04, 6.7182e-01, 4.1980e-05, 3.2787e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8156e-04, 7.4021e-01, 2.5292e-05, 2.5958e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1769e-04, 7.9787e-01, 1.4733e-05, 2.0199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.3039e-05, 8.4443e-01, 8.1993e-06, 1.5549e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.5204e-05, 8.8431e-01, 4.5479e-06, 1.1564e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7220e-05, 9.1306e-01, 2.4577e-06, 8.6914e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1320e-03, 1.0375e-01, 4.7314e-04, 8.9464e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4373e-03, 1.0801e-01, 5.9556e-04, 8.8996e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8247e-03, 1.1265e-01, 7.4679e-04, 8.8478e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0022, 0.1178, 0.0009, 0.8791]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[0.0023, 0.1206, 0.0009, 0.8761]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2796e-03, 1.2167e-01, 8.6518e-04, 8.7518e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2482e-03, 1.2576e-01, 8.2378e-04, 8.7117e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0866e-03, 1.3556e-01, 7.2671e-04, 8.6162e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7989e-03, 1.5356e-01, 5.8608e-04, 8.4405e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6498e-03, 1.8539e-01, 4.9887e-04, 8.1247e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4315e-03, 2.2945e-01, 3.9765e-04, 7.6873e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1669e-03, 2.8588e-01, 2.9498e-04, 7.1265e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.1097e-04, 3.5399e-01, 2.0848e-04, 6.4489e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9137e-04, 4.3125e-01, 1.4268e-04, 5.6791e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.1147e-04, 5.1211e-01, 9.5160e-05, 4.8728e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6993e-04, 5.9348e-01, 6.2073e-05, 4.0609e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.6455e-04, 6.7298e-01, 3.9934e-05, 3.2672e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8461e-04, 7.4399e-01, 2.5053e-05, 2.5580e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2288e-04, 8.0172e-01, 1.5007e-05, 1.9814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.9489e-05, 8.4919e-01, 8.7294e-06, 1.5072e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[5.0141e-05, 8.8847e-01, 4.9365e-06, 1.1147e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1309e-05, 9.1710e-01, 2.7702e-06, 8.2870e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.6274e-04, 1.3751e-01, 3.2283e-04, 8.6131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0937e-03, 1.4294e-01, 4.0575e-04, 8.5556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3870e-03, 1.4875e-01, 5.0879e-04, 8.4935e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7034e-03, 1.5515e-01, 6.1342e-04, 8.4253e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.9237e-03, 1.6022e-01, 6.7683e-04, 8.3718e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8828e-03, 1.6120e-01, 6.4433e-04, 8.3627e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8597e-03, 1.6431e-01, 6.1723e-04, 8.3322e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8124e-03, 1.7119e-01, 5.7857e-04, 8.2642e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6498e-03, 1.8539e-01, 4.9887e-04, 8.1247e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4086e-03, 2.0904e-01, 3.9804e-04, 7.8916e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2772e-03, 2.4846e-01, 3.3527e-04, 7.4992e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1010e-03, 3.0130e-01, 2.6578e-04, 6.9733e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.8200e-04, 3.6481e-01, 1.9426e-04, 6.3412e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8129e-04, 4.3817e-01, 1.3617e-04, 5.6101e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.0187e-04, 5.1819e-01, 9.0579e-05, 4.8122e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6093e-04, 5.9819e-01, 5.8729e-05, 4.0139e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.5308e-04, 6.7328e-01, 3.7126e-05, 3.2643e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7410e-04, 7.4160e-01, 2.3033e-05, 2.5820e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1881e-04, 8.0193e-01, 1.4138e-05, 1.9794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.9594e-05, 8.5113e-01, 8.5150e-06, 1.4878e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[5.1287e-05, 8.9141e-01, 4.9115e-06, 1.0854e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.2273e-05, 9.1882e-01, 2.7841e-06, 8.1150e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4951e-04, 1.8003e-01, 2.1759e-04, 8.1910e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.2181e-04, 1.8678e-01, 2.7294e-04, 8.1212e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0406e-03, 1.9388e-01, 3.4213e-04, 8.0474e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2906e-03, 2.0155e-01, 4.1708e-04, 7.9674e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5140e-03, 2.0929e-01, 4.7885e-04, 7.8872e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5614e-03, 2.1261e-01, 4.8108e-04, 7.8535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5085e-03, 2.1394e-01, 4.5144e-04, 7.8410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4954e-03, 2.1868e-01, 4.3347e-04, 7.7939e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4315e-03, 2.2945e-01, 3.9765e-04, 7.6873e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2772e-03, 2.4846e-01, 3.3527e-04, 7.4992e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0775e-03, 2.7795e-01, 2.6406e-04, 7.2071e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6210e-04, 3.2433e-01, 2.1922e-04, 6.7448e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.2270e-04, 3.8442e-01, 1.7259e-04, 6.1458e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4698e-04, 4.5187e-01, 1.2414e-04, 5.4736e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9101e-04, 5.2684e-01, 8.5625e-05, 4.7258e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.5351e-04, 6.0418e-01, 5.5786e-05, 3.9541e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4736e-04, 6.7861e-01, 3.5201e-05, 3.2111e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6919e-04, 7.4507e-01, 2.1707e-05, 2.5474e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1344e-04, 8.0182e-01, 1.3122e-05, 1.9805e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.5162e-05, 8.4954e-01, 7.8392e-06, 1.5038e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.9028e-05, 8.9064e-01, 4.5797e-06, 1.0930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1971e-05, 9.2004e-01, 2.6843e-06, 7.9926e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8161e-04, 2.3215e-01, 1.4444e-04, 7.6723e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.0790e-04, 2.4029e-01, 1.8076e-04, 7.5892e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.6862e-04, 2.4873e-01, 2.2647e-04, 7.5028e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6805e-04, 2.5816e-01, 2.8082e-04, 7.4059e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1462e-03, 2.6761e-01, 3.2557e-04, 7.3092e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2514e-03, 2.7378e-01, 3.4708e-04, 7.2462e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2060e-03, 2.7425e-01, 3.2544e-04, 7.2422e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1866e-03, 2.7856e-01, 3.1056e-04, 7.1994e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1669e-03, 2.8588e-01, 2.9498e-04, 7.1265e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1010e-03, 3.0130e-01, 2.6578e-04, 6.9733e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6210e-04, 3.2433e-01, 2.1922e-04, 6.7448e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.0111e-04, 3.5924e-01, 1.7028e-04, 6.3979e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0262e-04, 4.1109e-01, 1.3890e-04, 5.8807e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.9521e-04, 4.7486e-01, 1.0851e-04, 5.2443e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.5975e-04, 5.4249e-01, 7.6805e-05, 4.5698e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3947e-04, 6.1496e-01, 5.1580e-05, 3.8465e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4208e-04, 6.8476e-01, 3.3362e-05, 3.1496e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6508e-04, 7.4967e-01, 2.0546e-05, 2.5015e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1052e-04, 8.0564e-01, 1.2401e-05, 1.9424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.2779e-05, 8.5157e-01, 7.3625e-06, 1.4835e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.6813e-05, 8.9057e-01, 4.2506e-06, 1.0938e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.0214e-05, 9.1908e-01, 2.4736e-06, 8.0891e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.5065e-04, 2.9393e-01, 9.4152e-05, 7.0563e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.4133e-04, 3.0339e-01, 1.1749e-04, 6.9605e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.5666e-04, 3.1318e-01, 1.4684e-04, 6.8612e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9957e-04, 3.2385e-01, 1.8191e-04, 6.7527e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.3506e-04, 3.3430e-01, 2.1287e-04, 6.6465e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.5337e-04, 3.4391e-01, 2.3771e-04, 6.5490e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.6051e-04, 3.4704e-01, 2.3330e-04, 6.5176e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.2441e-04, 3.4834e-01, 2.1813e-04, 6.5051e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.1097e-04, 3.5399e-01, 2.0848e-04, 6.4489e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.8200e-04, 3.6481e-01, 1.9426e-04, 6.3412e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.2270e-04, 3.8442e-01, 1.7259e-04, 6.1458e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0262e-04, 4.1109e-01, 1.3890e-04, 5.8807e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.7663e-04, 4.4948e-01, 1.0630e-04, 5.4983e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9606e-04, 5.0373e-01, 8.5088e-05, 4.9569e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.1315e-04, 5.6699e-01, 6.5424e-05, 4.3253e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1379e-04, 6.3185e-01, 4.5560e-05, 3.6779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2761e-04, 6.9613e-01, 3.0133e-05, 3.0362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6084e-04, 7.5584e-01, 1.9350e-05, 2.4398e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0770e-04, 8.0955e-01, 1.1719e-05, 1.9033e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0828e-05, 8.5462e-01, 6.9506e-06, 1.4530e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.5466e-05, 8.9270e-01, 4.0041e-06, 1.0725e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.9194e-05, 9.2010e-01, 2.3179e-06, 7.9869e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4994e-04, 3.6434e-01, 6.0082e-05, 6.3535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1355e-04, 3.7487e-01, 7.4730e-05, 6.2475e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9419e-04, 3.8570e-01, 9.3099e-05, 6.1381e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9395e-04, 3.9725e-01, 1.1512e-04, 6.0215e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.9561e-04, 4.0842e-01, 1.3626e-04, 5.9084e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0266e-04, 4.2090e-01, 1.5748e-04, 5.7824e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.4299e-04, 4.2762e-01, 1.6233e-04, 5.7147e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0564e-04, 4.2622e-01, 1.5016e-04, 5.7293e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9137e-04, 4.3125e-01, 1.4268e-04, 5.6791e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8129e-04, 4.3817e-01, 1.3617e-04, 5.6101e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4698e-04, 4.5187e-01, 1.2414e-04, 5.4736e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.9521e-04, 4.7486e-01, 1.0851e-04, 5.2443e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9606e-04, 5.0373e-01, 8.5088e-05, 4.9569e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[65.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.0085e-04, 5.4317e-01, 6.4090e-05, 4.5636e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[65.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3823e-04, 5.9610e-01, 5.0337e-05, 4.0351e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7606e-04, 6.5470e-01, 3.7959e-05, 3.4499e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0740e-04, 7.1305e-01, 2.6169e-05, 2.8672e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4847e-04, 7.6687e-01, 1.7121e-05, 2.3296e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0287e-04, 8.1618e-01, 1.0783e-05, 1.8370e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8977e-05, 8.5813e-01, 6.5546e-06, 1.4179e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.4204e-05, 8.9501e-01, 3.7764e-06, 1.0494e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.8365e-05, 9.2187e-01, 2.1847e-06, 7.8100e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7399e-04, 4.4108e-01, 3.7446e-05, 5.5871e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1751e-04, 4.5224e-01, 4.6410e-05, 5.4749e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7248e-04, 4.6367e-01, 5.7613e-05, 5.3600e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.4038e-04, 4.7556e-01, 7.1105e-05, 5.2403e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.1730e-04, 4.8752e-01, 8.5696e-05, 5.1198e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8697e-04, 4.9971e-01, 9.7911e-05, 4.9970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.4023e-04, 5.0917e-01, 1.0619e-04, 4.9018e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.3324e-04, 5.1122e-01, 1.0211e-04, 4.8814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.1147e-04, 5.1211e-01, 9.5160e-05, 4.8728e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.0187e-04, 5.1819e-01, 9.0579e-05, 4.8122e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.9101e-04, 5.2684e-01, 8.5625e-05, 4.7258e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.5975e-04, 5.4249e-01, 7.6805e-05, 4.5698e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.1315e-04, 5.6699e-01, 6.5424e-05, 4.3253e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3823e-04, 5.9610e-01, 5.0337e-05, 4.0351e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.6911e-04, 6.3389e-01, 3.7317e-05, 3.6580e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2301e-04, 6.8212e-01, 2.8795e-05, 3.1762e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7885e-04, 7.3300e-01, 2.1354e-05, 2.6680e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3330e-04, 7.8250e-01, 1.4617e-05, 2.1735e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.4599e-05, 8.2554e-01, 9.4968e-06, 1.7435e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4524e-05, 8.6428e-01, 5.8929e-06, 1.3565e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[4.3013e-05, 8.9788e-01, 3.5548e-06, 1.0208e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7557e-05, 9.2360e-01, 2.0590e-06, 7.6365e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1812e-04, 5.2073e-01, 2.2759e-05, 4.7913e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4712e-04, 5.3199e-01, 2.8104e-05, 4.6783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8363e-04, 5.4344e-01, 3.4761e-05, 4.5634e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2869e-04, 5.5508e-01, 4.2820e-05, 4.4464e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.8402e-04, 5.6752e-01, 5.2350e-05, 4.3214e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.2879e-04, 5.7851e-01, 5.9326e-05, 4.2110e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.7767e-04, 5.9005e-01, 6.6735e-05, 4.0950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9259e-04, 5.9629e-01, 6.7585e-05, 4.0325e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6993e-04, 5.9348e-01, 6.2073e-05, 4.0609e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6093e-04, 5.9819e-01, 5.8729e-05, 4.0139e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.5351e-04, 6.0418e-01, 5.5786e-05, 3.9541e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3947e-04, 6.1496e-01, 5.1580e-05, 3.8465e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1379e-04, 6.3185e-01, 4.5560e-05, 3.6779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7606e-04, 6.5470e-01, 3.7959e-05, 3.4499e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2301e-04, 6.8212e-01, 2.8795e-05, 3.1762e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7486e-04, 7.1601e-01, 2.1030e-05, 2.8380e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4265e-04, 7.5728e-01, 1.5981e-05, 2.4256e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1277e-04, 7.9907e-01, 1.1691e-05, 2.0081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.3704e-05, 8.3894e-01, 7.9761e-06, 1.6097e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.8873e-05, 8.7224e-01, 5.1411e-06, 1.2770e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.9388e-05, 9.0320e-01, 3.1212e-06, 9.6755e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.6296e-05, 9.2652e-01, 1.8936e-06, 7.3454e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.8177e-05, 5.9934e-01, 1.3485e-05, 4.0057e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.7019e-05, 6.1013e-01, 1.6593e-05, 3.8975e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2066e-04, 6.2103e-01, 2.0450e-05, 3.7883e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4985e-04, 6.3187e-01, 2.5149e-05, 3.6795e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8539e-04, 6.4355e-01, 3.0628e-05, 3.5623e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1725e-04, 6.5374e-01, 3.5184e-05, 3.4601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.5277e-04, 6.6516e-01, 4.0093e-05, 3.3455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.7092e-04, 6.7182e-01, 4.1980e-05, 3.2787e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.6455e-04, 6.7298e-01, 3.9934e-05, 3.2672e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.5308e-04, 6.7328e-01, 3.7126e-05, 3.2643e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4736e-04, 6.7861e-01, 3.5201e-05, 3.2111e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4208e-04, 6.8476e-01, 3.3362e-05, 3.1496e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.2761e-04, 6.9613e-01, 3.0133e-05, 3.0362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0740e-04, 7.1305e-01, 2.6169e-05, 2.8672e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7885e-04, 7.3300e-01, 2.1354e-05, 2.6680e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4265e-04, 7.5728e-01, 1.5981e-05, 2.4256e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1042e-04, 7.8592e-01, 1.1517e-05, 2.1396e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.8929e-05, 8.1937e-01, 8.6440e-06, 1.8054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9540e-05, 8.5213e-01, 6.2596e-06, 1.4779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.1594e-05, 8.8292e-01, 4.2724e-06, 1.1703e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.5643e-05, 9.0961e-01, 2.6960e-06, 9.0351e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.4027e-05, 9.3057e-01, 1.6581e-06, 6.9403e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.0490e-05, 6.7314e-01, 7.7971e-06, 3.2680e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.2447e-05, 6.8300e-01, 9.5619e-06, 3.1693e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.7408e-05, 6.9289e-01, 1.1745e-05, 3.0702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.5909e-05, 7.0260e-01, 1.4421e-05, 2.9728e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1822e-04, 7.1297e-01, 1.7506e-05, 2.8690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4089e-04, 7.2231e-01, 2.0482e-05, 2.7753e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6227e-04, 7.3187e-01, 2.3095e-05, 2.6794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8156e-04, 7.4021e-01, 2.5292e-05, 2.5958e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8461e-04, 7.4399e-01, 2.5053e-05, 2.5580e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7410e-04, 7.4160e-01, 2.3033e-05, 2.5820e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6919e-04, 7.4507e-01, 2.1707e-05, 2.5474e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6508e-04, 7.4967e-01, 2.0546e-05, 2.5015e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6084e-04, 7.5584e-01, 1.9350e-05, 2.4398e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4847e-04, 7.6687e-01, 1.7121e-05, 2.3296e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3330e-04, 7.8250e-01, 1.4617e-05, 2.1735e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1277e-04, 7.9907e-01, 1.1691e-05, 2.0081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.8929e-05, 8.1937e-01, 8.6440e-06, 1.8054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8083e-05, 8.4241e-01, 6.1593e-06, 1.5752e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.4301e-05, 8.6833e-01, 4.5795e-06, 1.3161e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.2141e-05, 8.9305e-01, 3.2936e-06, 1.0690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1124e-05, 9.1746e-01, 2.2325e-06, 8.2505e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.1476e-05, 9.3577e-01, 1.4117e-06, 6.4208e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[90.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.1885e-05, 7.3926e-01, 4.4083e-06, 2.6070e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9318e-05, 7.4787e-01, 5.3898e-06, 2.5209e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.8593e-05, 7.5646e-01, 6.6010e-06, 2.4348e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.0033e-05, 7.6485e-01, 8.0812e-06, 2.3508e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.3841e-05, 7.7361e-01, 9.8005e-06, 2.2630e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.9381e-05, 7.8192e-01, 1.1663e-05, 2.1798e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0209e-04, 7.8949e-01, 1.3039e-05, 2.1040e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1769e-04, 7.9787e-01, 1.4733e-05, 2.0199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2288e-04, 8.0172e-01, 1.5007e-05, 1.9814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1881e-04, 8.0193e-01, 1.4138e-05, 1.9794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1344e-04, 8.0182e-01, 1.3122e-05, 1.9805e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.1052e-04, 8.0564e-01, 1.2401e-05, 1.9424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0770e-04, 8.0955e-01, 1.1719e-05, 1.9033e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0287e-04, 8.1618e-01, 1.0783e-05, 1.8370e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[9.4599e-05, 8.2554e-01, 9.4968e-06, 1.7435e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[8.3704e-05, 8.3894e-01, 7.9761e-06, 1.6097e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9540e-05, 8.5213e-01, 6.2596e-06, 1.4779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.4301e-05, 8.6833e-01, 4.5795e-06, 1.3161e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.1199e-05, 8.8615e-01, 3.2326e-06, 1.1381e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.2628e-05, 9.0555e-01, 2.3875e-06, 9.4414e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[2.5110e-05, 9.2490e-01, 1.6987e-06, 7.5074e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.8707e-05, 9.4149e-01, 1.1661e-06, 5.8486e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[95.0000,  0.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.9744e-05, 7.9606e-01, 2.4438e-06, 2.0392e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000,  5.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4283e-05, 8.0328e-01, 2.9802e-06, 1.9669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 10.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.9936e-05, 8.1047e-01, 3.6407e-06, 1.8950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 15.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6893e-05, 8.1745e-01, 4.4462e-06, 1.8251e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 20.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.5305e-05, 8.2460e-01, 5.3896e-06, 1.7535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 25.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.5513e-05, 8.3179e-01, 6.5010e-06, 1.6814e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 30.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.3440e-05, 8.3771e-01, 7.2730e-06, 1.6222e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 35.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.3039e-05, 8.4443e-01, 8.1993e-06, 1.5549e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 40.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.9489e-05, 8.4919e-01, 8.7294e-06, 1.5072e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 45.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.9594e-05, 8.5113e-01, 8.5150e-06, 1.4878e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 50.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.5162e-05, 8.4954e-01, 7.8392e-06, 1.5038e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 55.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.2779e-05, 8.5157e-01, 7.3625e-06, 1.4835e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 60.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[7.0828e-05, 8.5462e-01, 6.9506e-06, 1.4530e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 65.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8977e-05, 8.5813e-01, 6.5546e-06, 1.4179e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 70.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.4524e-05, 8.6428e-01, 5.8929e-06, 1.3565e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 75.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.8873e-05, 8.7224e-01, 5.1411e-06, 1.2770e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 80.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[5.1594e-05, 8.8292e-01, 4.2724e-06, 1.1703e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 85.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[4.2141e-05, 8.9305e-01, 3.2936e-06, 1.0690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 90.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[3.2628e-05, 9.0555e-01, 2.3875e-06, 9.4414e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 95.0000,  0.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4577e-05, 9.1891e-01, 1.6725e-06, 8.1059e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[ 95.0000, 100.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.9345e-05, 9.3414e-01, 1.2253e-06, 6.5838e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 95.0000, 105.0000,   0.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.4839e-05, 9.4701e-01, 8.7137e-07, 5.2970e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[100.0000,   0.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.2084e-05, 8.4641e-01, 1.3349e-06, 1.5358e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,   5.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.4814e-05, 8.5214e-01, 1.6230e-06, 1.4784e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  10.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.8223e-05, 8.5786e-01, 1.9783e-06, 1.4212e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  15.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.2409e-05, 8.6339e-01, 2.4108e-06, 1.3659e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  20.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7424e-05, 8.6874e-01, 2.9171e-06, 1.3123e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  25.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.3529e-05, 8.7441e-01, 3.5109e-06, 1.2556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  30.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.9553e-05, 8.7930e-01, 4.0646e-06, 1.2066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  35.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.5204e-05, 8.8431e-01, 4.5479e-06, 1.1564e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  40.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[5.0141e-05, 8.8847e-01, 4.9365e-06, 1.1147e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  45.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[5.1287e-05, 8.9141e-01, 4.9115e-06, 1.0854e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  50.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.9028e-05, 8.9064e-01, 4.5797e-06, 1.0930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  55.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.6813e-05, 8.9057e-01, 4.2506e-06, 1.0938e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  60.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.5466e-05, 8.9270e-01, 4.0041e-06, 1.0725e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  65.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.4204e-05, 8.9501e-01, 3.7764e-06, 1.0494e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  70.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[4.3013e-05, 8.9788e-01, 3.5548e-06, 1.0208e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  75.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.9388e-05, 9.0320e-01, 3.1212e-06, 9.6755e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  80.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.5643e-05, 9.0961e-01, 2.6960e-06, 9.0351e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  85.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1124e-05, 9.1746e-01, 2.2325e-06, 8.2505e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  90.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.5110e-05, 9.2490e-01, 1.6987e-06, 7.5074e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  95.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.9345e-05, 9.3414e-01, 1.2253e-06, 6.5838e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000, 100.0000,   1.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.4255e-05, 9.4485e-01, 8.3230e-07, 5.5133e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000, 105.0000,   1.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.1195e-05, 9.5465e-01, 6.0996e-07, 4.5341e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[105.0000,   0.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[7.2535e-06, 8.8354e-01, 7.1735e-07, 1.1645e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,   5.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[8.8773e-06, 8.8807e-01, 8.7072e-07, 1.1192e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  10.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.0902e-05, 8.9257e-01, 1.0596e-06, 1.0742e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  15.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.3386e-05, 8.9691e-01, 1.2892e-06, 1.0307e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  20.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.6373e-05, 9.0104e-01, 1.5610e-06, 9.8943e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  25.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.9986e-05, 9.0546e-01, 1.8757e-06, 9.4516e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  30.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.3956e-05, 9.0945e-01, 2.2097e-06, 9.0525e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  35.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7220e-05, 9.1306e-01, 2.4577e-06, 8.6914e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  40.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1309e-05, 9.1710e-01, 2.7702e-06, 8.2870e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  45.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.2273e-05, 9.1882e-01, 2.7841e-06, 8.1150e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  50.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.1971e-05, 9.2004e-01, 2.6843e-06, 7.9926e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  55.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[3.0214e-05, 9.1908e-01, 2.4736e-06, 8.0891e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  60.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.9194e-05, 9.2010e-01, 2.3179e-06, 7.9869e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  65.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.8365e-05, 9.2187e-01, 2.1847e-06, 7.8100e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  70.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.7557e-05, 9.2360e-01, 2.0590e-06, 7.6365e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  75.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.6296e-05, 9.2652e-01, 1.8936e-06, 7.3454e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  80.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.4027e-05, 9.3057e-01, 1.6581e-06, 6.9403e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  85.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[2.1476e-05, 9.3577e-01, 1.4117e-06, 6.4208e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  90.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.8707e-05, 9.4149e-01, 1.1661e-06, 5.8486e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  95.0000,   1.0000,   0.0000,   0.2000]]) dist_dsc.probs: tensor([[1.4839e-05, 9.4701e-01, 8.7137e-07, 5.2970e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000, 100.0000,   1.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[1.1195e-05, 9.5465e-01, 6.0996e-07, 4.5341e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000, 105.0000,   1.0000,   1.0000,   0.2000]]) dist_dsc.probs: tensor([[8.3446e-06, 9.6146e-01, 4.2256e-07, 3.8533e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0066, 0.0090, 0.0065, 0.9779]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[0., 5., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0059, 0.0116, 0.0053, 0.9772]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0159, 0.0038, 0.9757]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0037, 0.0217, 0.0027, 0.9719]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0299, 0.0019, 0.9652]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0407, 0.0013, 0.9556]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8006e-03, 5.5243e-02, 9.4137e-04, 9.4202e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3962e-03, 7.4475e-02, 6.5381e-04, 9.2348e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0748e-03, 9.9681e-02, 4.5083e-04, 8.9879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1978e-04, 1.3218e-01, 3.0799e-04, 8.6669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.1811e-04, 1.7330e-01, 2.0796e-04, 8.2587e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.5941e-04, 2.2400e-01, 1.3838e-04, 7.7540e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3543e-04, 2.8441e-01, 9.0455e-05, 7.1516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.3987e-04, 3.5369e-01, 5.7910e-05, 6.4602e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6757e-04, 4.2969e-01, 3.6219e-05, 5.7010e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1418e-04, 5.0916e-01, 2.2094e-05, 4.9070e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5846e-05, 5.8816e-01, 1.3139e-05, 4.1175e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.9153e-05, 6.6287e-01, 7.6234e-06, 3.3708e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1137e-05, 7.3023e-01, 4.3235e-06, 2.6973e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.9332e-05, 7.8843e-01, 2.4032e-06, 2.1154e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.1860e-05, 8.4029e-01, 1.3158e-06, 1.5970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[7.1316e-06, 8.7869e-01, 7.0833e-07, 1.2130e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[5., 0., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0059, 0.0116, 0.0053, 0.9772]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[5., 5., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0066, 0.0123, 0.0058, 0.9754]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0058, 0.0165, 0.0046, 0.9731]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0227, 0.0034, 0.9691]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0312, 0.0025, 0.9625]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0030, 0.0424, 0.0017, 0.9529]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0574, 0.0012, 0.9391]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7735e-03, 7.7451e-02, 8.2419e-04, 9.1995e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3648e-03, 1.0366e-01, 5.6786e-04, 8.9441e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0402e-03, 1.3740e-01, 3.8748e-04, 8.6117e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.8311e-04, 1.7990e-01, 2.6116e-04, 8.1905e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8068e-04, 2.3200e-01, 1.7338e-04, 7.6725e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.2277e-04, 2.9376e-01, 1.1301e-04, 7.0571e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.0135e-04, 3.6415e-01, 7.2118e-05, 6.3548e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0978e-04, 4.4087e-01, 4.4947e-05, 5.5887e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4242e-04, 5.2052e-01, 2.7318e-05, 4.7931e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.4256e-05, 5.9913e-01, 1.6187e-05, 4.0076e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.0874e-05, 6.7295e-01, 9.3595e-06, 3.2698e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.8443e-05, 7.3909e-01, 5.2917e-06, 2.6087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.3804e-05, 7.9591e-01, 2.9335e-06, 2.0406e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.4555e-05, 8.4625e-01, 1.6012e-06, 1.5374e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[8.7368e-06, 8.8341e-01, 8.6048e-07, 1.1658e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0159, 0.0038, 0.9757]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0058, 0.0165, 0.0046, 0.9731]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0057, 0.0181, 0.0044, 0.9718]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0052, 0.0236, 0.0037, 0.9675]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0044, 0.0325, 0.0028, 0.9603]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0036, 0.0444, 0.0021, 0.9499]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0604, 0.0015, 0.9353]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0812, 0.0010, 0.9155]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7449e-03, 1.0827e-01, 7.1784e-04, 8.8926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3196e-03, 1.4295e-01, 4.8652e-04, 8.5525e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.9207e-04, 1.8669e-01, 3.2783e-04, 8.1199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.3482e-04, 2.4027e-01, 2.1741e-04, 7.5878e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3349e-04, 3.0338e-01, 1.4132e-04, 6.9595e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7904e-04, 3.7486e-01, 8.9889e-05, 6.2467e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6294e-04, 4.5225e-01, 5.5826e-05, 5.4743e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7785e-04, 5.3200e-01, 3.3807e-05, 4.6779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1729e-04, 6.1015e-01, 1.9959e-05, 3.8972e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5493e-05, 6.8301e-01, 1.1502e-05, 3.1690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7531e-05, 7.4788e-01, 6.4834e-06, 2.5206e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.9355e-05, 8.0330e-01, 3.5849e-06, 1.9667e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.7909e-05, 8.5215e-01, 1.9523e-06, 1.4783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.0732e-05, 8.8808e-01, 1.0474e-06, 1.1191e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0037, 0.0217, 0.0027, 0.9719]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0227, 0.0034, 0.9691]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0052, 0.0236, 0.0037, 0.9675]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0048, 0.0263, 0.0032, 0.9657]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0045, 0.0335, 0.0028, 0.9593]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0455, 0.0021, 0.9486]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0623, 0.0016, 0.9328]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0846, 0.0012, 0.9116]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1121e-03, 1.1321e-01, 8.5112e-04, 8.8383e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6357e-03, 1.4961e-01, 5.9206e-04, 8.4816e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2417e-03, 1.9473e-01, 4.0334e-04, 8.0362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.2698e-04, 2.4951e-01, 2.7023e-04, 7.4930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7095e-04, 3.1372e-01, 1.7535e-04, 6.8544e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7536e-04, 3.8613e-01, 1.1135e-04, 6.1329e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2876e-04, 4.6393e-01, 6.9025e-05, 5.3567e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2171e-04, 5.4353e-01, 4.1722e-05, 4.5621e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4586e-04, 6.2104e-01, 2.4597e-05, 3.7879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3578e-05, 6.9290e-01, 1.4128e-05, 3.0699e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8744e-05, 7.5647e-01, 7.9403e-06, 2.4346e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.6189e-05, 8.1048e-01, 4.3794e-06, 1.8948e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.2029e-05, 8.5787e-01, 2.3797e-06, 1.4211e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.3179e-05, 8.9258e-01, 1.2746e-06, 1.0741e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0299, 0.0019, 0.9652]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0312, 0.0025, 0.9625]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0044, 0.0325, 0.0028, 0.9603]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0045, 0.0335, 0.0028, 0.9593]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0040, 0.0378, 0.0023, 0.9559]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0475, 0.0020, 0.9468]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0634, 0.0016, 0.9318]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0858, 0.0012, 0.9103]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.1164, 0.0009, 0.8803]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8736e-03, 1.5473e-01, 6.6262e-04, 8.4273e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4645e-03, 2.0226e-01, 4.6543e-04, 7.9581e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1059e-03, 2.5928e-01, 3.1573e-04, 7.3930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0796e-04, 3.2481e-01, 2.0702e-04, 6.7418e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.7736e-04, 3.9798e-01, 1.3276e-04, 6.0131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0273e-04, 4.7600e-01, 8.3106e-05, 5.2352e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7483e-04, 5.5593e-01, 5.0903e-05, 4.4374e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8047e-04, 6.3277e-01, 2.9966e-05, 3.6702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1546e-04, 7.0331e-01, 1.7184e-05, 2.9655e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2324e-05, 7.6533e-01, 9.6477e-06, 2.3459e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.4482e-05, 8.1774e-01, 5.3184e-06, 1.8221e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.6982e-05, 8.6332e-01, 2.8846e-06, 1.3665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.6134e-05, 8.9680e-01, 1.5459e-06, 1.0319e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0407, 0.0013, 0.9556]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0030, 0.0424, 0.0017, 0.9529]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0036, 0.0444, 0.0021, 0.9499]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0455, 0.0021, 0.9486]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0038, 0.0475, 0.0020, 0.9468]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0033, 0.0541, 0.0017, 0.9409]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0031, 0.0673, 0.0014, 0.9281]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0880, 0.0011, 0.9082]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2442e-03, 1.1740e-01, 8.5698e-04, 8.7950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8571e-03, 1.5582e-01, 6.3945e-04, 8.4169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.5455e-03, 2.0608e-01, 4.7903e-04, 7.9190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2347e-03, 2.6610e-01, 3.4426e-04, 7.3233e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3554e-04, 3.3463e-01, 2.3448e-04, 6.6420e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7930e-04, 4.1005e-01, 1.5293e-04, 5.8912e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7451e-04, 4.8897e-01, 9.5895e-05, 5.1046e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2146e-04, 5.6799e-01, 5.8302e-05, 4.3163e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1246e-04, 6.4369e-01, 3.4581e-05, 3.5607e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3722e-04, 7.1282e-01, 2.0043e-05, 2.8703e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7351e-05, 7.7378e-01, 1.1453e-05, 2.2613e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.4454e-05, 8.2513e-01, 6.4086e-06, 1.7481e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.2998e-05, 8.6919e-01, 3.4727e-06, 1.3078e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.9698e-05, 9.0139e-01, 1.8581e-06, 9.8591e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8006e-03, 5.5243e-02, 9.4137e-04, 9.4202e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0574, 0.0012, 0.9391]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0604, 0.0015, 0.9353]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0623, 0.0016, 0.9328]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0634, 0.0016, 0.9318]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0031, 0.0673, 0.0014, 0.9281]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0768, 0.0012, 0.9193]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0025, 0.0949, 0.0010, 0.9016]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2096e-03, 1.2165e-01, 8.1382e-04, 8.7532e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8356e-03, 1.5894e-01, 6.1271e-04, 8.3861e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4906e-03, 2.0719e-01, 4.4881e-04, 7.9087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1949e-03, 2.6629e-01, 3.2443e-04, 7.3219e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5575e-04, 3.3815e-01, 2.3353e-04, 6.6066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.4046e-04, 4.1759e-01, 1.6278e-04, 5.8151e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3454e-04, 4.9893e-01, 1.0562e-04, 5.0043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7258e-04, 5.7977e-01, 6.6163e-05, 4.1980e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4804e-04, 6.5552e-01, 3.9546e-05, 3.4419e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.5971e-04, 7.2335e-01, 2.2847e-05, 2.7647e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0073e-04, 7.8225e-01, 1.2931e-05, 2.1764e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.2445e-05, 8.3153e-01, 7.1946e-06, 1.6840e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.8806e-05, 8.7423e-01, 4.0071e-06, 1.2573e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.3555e-05, 9.0549e-01, 2.1833e-06, 9.4484e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3962e-03, 7.4475e-02, 6.5381e-04, 9.2348e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7735e-03, 7.7451e-02, 8.2419e-04, 9.1995e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.0812, 0.0010, 0.9155]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0846, 0.0012, 0.9116]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0858, 0.0012, 0.9103]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0880, 0.0011, 0.9082]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0025, 0.0949, 0.0010, 0.9016]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1872e-03, 1.0817e-01, 8.2383e-04, 8.8882e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0264e-03, 1.3213e-01, 7.0810e-04, 8.6513e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7837e-03, 1.6580e-01, 5.7240e-04, 8.3184e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4792e-03, 2.1199e-01, 4.3121e-04, 7.8610e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1755e-03, 2.7047e-01, 3.0955e-04, 7.2805e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.1838e-04, 3.3917e-01, 2.1803e-04, 6.5969e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.0342e-04, 4.1637e-01, 1.5061e-04, 5.8278e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3404e-04, 5.0135e-01, 1.0287e-04, 4.9801e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.9347e-04, 5.8620e-01, 6.8166e-05, 4.1334e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7238e-04, 6.6344e-01, 4.2429e-05, 3.3624e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8111e-04, 7.3216e-01, 2.5360e-05, 2.6763e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1627e-04, 7.9079e-01, 1.4627e-05, 2.0908e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2239e-05, 8.3870e-01, 8.1510e-06, 1.6122e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.4787e-05, 8.7984e-01, 4.5291e-06, 1.2011e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.7003e-05, 9.0958e-01, 2.4506e-06, 9.0389e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0748e-03, 9.9681e-02, 4.5083e-04, 8.9879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3648e-03, 1.0366e-01, 5.6786e-04, 8.9441e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7449e-03, 1.0827e-01, 7.1784e-04, 8.8926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1121e-03, 1.1321e-01, 8.5112e-04, 8.8383e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.1164, 0.0009, 0.8803]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2442e-03, 1.1740e-01, 8.5698e-04, 8.7950e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2096e-03, 1.2165e-01, 8.1382e-04, 8.7532e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0264e-03, 1.3213e-01, 7.0810e-04, 8.6513e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7465e-03, 1.5016e-01, 5.7055e-04, 8.4753e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6049e-03, 1.8115e-01, 4.8686e-04, 8.1676e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4083e-03, 2.2314e-01, 3.9301e-04, 7.7506e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1538e-03, 2.7821e-01, 2.9316e-04, 7.2034e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.0589e-04, 3.4499e-01, 2.0850e-04, 6.5390e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.8916e-04, 4.2131e-01, 1.4309e-04, 5.7786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1151e-04, 5.0195e-01, 9.5752e-05, 4.9745e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7132e-04, 5.8373e-01, 6.2688e-05, 4.1584e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6665e-04, 6.6423e-01, 4.0490e-05, 3.3547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8655e-04, 7.3627e-01, 2.5473e-05, 2.6351e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2471e-04, 7.9576e-01, 1.5314e-05, 2.0410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0005e-05, 8.4402e-01, 8.8319e-06, 1.5589e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[5.0636e-05, 8.8453e-01, 5.0113e-06, 1.1542e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.0985e-05, 9.1367e-01, 2.7545e-06, 8.6296e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1978e-04, 1.3218e-01, 3.0799e-04, 8.6669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0402e-03, 1.3740e-01, 3.8748e-04, 8.6117e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3196e-03, 1.4295e-01, 4.8652e-04, 8.5525e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6357e-03, 1.4961e-01, 5.9206e-04, 8.4816e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8736e-03, 1.5473e-01, 6.6262e-04, 8.4273e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8571e-03, 1.5582e-01, 6.3945e-04, 8.4169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8356e-03, 1.5894e-01, 6.1271e-04, 8.3861e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7837e-03, 1.6580e-01, 5.7240e-04, 8.3184e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6049e-03, 1.8115e-01, 4.8686e-04, 8.1676e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3696e-03, 2.0469e-01, 3.8805e-04, 7.9355e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2438e-03, 2.4316e-01, 3.2756e-04, 7.5527e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0859e-03, 2.9377e-01, 2.6335e-04, 7.0488e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7464e-04, 3.5607e-01, 1.9363e-04, 6.4286e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7974e-04, 4.2844e-01, 1.3663e-04, 5.7074e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.0204e-04, 5.0803e-01, 9.1165e-05, 4.9138e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.6223e-04, 5.8838e-01, 5.9302e-05, 4.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.5477e-04, 6.6427e-01, 3.7604e-05, 3.3544e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7596e-04, 7.3396e-01, 2.3418e-05, 2.6584e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2037e-04, 7.9558e-01, 1.4408e-05, 2.0429e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0781e-05, 8.4598e-01, 8.6956e-06, 1.5393e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[5.2196e-05, 8.8739e-01, 5.0294e-06, 1.1255e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.2887e-05, 9.1579e-01, 2.8541e-06, 8.4172e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.1811e-04, 1.7330e-01, 2.0796e-04, 8.2587e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.8311e-04, 1.7990e-01, 2.6116e-04, 8.1905e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.9207e-04, 1.8669e-01, 3.2783e-04, 8.1199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2417e-03, 1.9473e-01, 4.0334e-04, 8.0362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4645e-03, 2.0226e-01, 4.6543e-04, 7.9581e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.5455e-03, 2.0608e-01, 4.7903e-04, 7.9190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4906e-03, 2.0719e-01, 4.4881e-04, 7.9087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4792e-03, 2.1199e-01, 4.3121e-04, 7.8610e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4083e-03, 2.2314e-01, 3.9301e-04, 7.7506e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2438e-03, 2.4316e-01, 3.2756e-04, 7.5527e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0495e-03, 2.7266e-01, 2.5790e-04, 7.2603e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3881e-04, 3.1823e-01, 2.1456e-04, 6.8062e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1386e-04, 3.7592e-01, 1.7152e-04, 6.2309e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.4358e-04, 4.4235e-01, 1.2414e-04, 5.5688e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8990e-04, 5.1739e-01, 8.5874e-05, 4.8204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.5504e-04, 5.9467e-01, 5.6345e-05, 4.0492e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4907e-04, 6.6968e-01, 3.5662e-05, 3.3004e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7083e-04, 7.3727e-01, 2.2051e-05, 2.6254e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1482e-04, 7.9532e-01, 1.3363e-05, 2.0455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.6294e-05, 8.4442e-01, 8.0046e-06, 1.5550e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.9848e-05, 8.8673e-01, 4.6840e-06, 1.1321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.2539e-05, 9.1704e-01, 2.7490e-06, 8.2927e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.5941e-04, 2.2400e-01, 1.3838e-04, 7.7540e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8068e-04, 2.3200e-01, 1.7338e-04, 7.6725e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.3482e-04, 2.4027e-01, 2.1741e-04, 7.5878e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.2698e-04, 2.4951e-01, 2.7023e-04, 7.4930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1059e-03, 2.5928e-01, 3.1573e-04, 7.3930e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2347e-03, 2.6610e-01, 3.4426e-04, 7.3233e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1949e-03, 2.6629e-01, 3.2443e-04, 7.3219e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1755e-03, 2.7047e-01, 3.0955e-04, 7.2805e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1538e-03, 2.7821e-01, 2.9316e-04, 7.2034e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0859e-03, 2.9377e-01, 2.6335e-04, 7.0488e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3881e-04, 3.1823e-01, 2.1456e-04, 6.8062e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.8200e-04, 3.5316e-01, 1.6666e-04, 6.4589e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.8726e-04, 4.0433e-01, 1.3628e-04, 5.9485e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8619e-04, 4.6589e-01, 1.0731e-04, 5.3341e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.5757e-04, 5.3344e-01, 7.6801e-05, 4.6602e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3984e-04, 6.0595e-01, 5.1903e-05, 3.9366e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4390e-04, 6.7615e-01, 3.3805e-05, 3.2357e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6671e-04, 7.4195e-01, 2.0875e-05, 2.5786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1187e-04, 7.9919e-01, 1.2629e-05, 2.0069e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.3804e-05, 8.4635e-01, 7.5120e-06, 1.5357e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.7555e-05, 8.8657e-01, 4.3446e-06, 1.1338e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.0754e-05, 9.1609e-01, 2.5328e-06, 8.3878e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3543e-04, 2.8441e-01, 9.0455e-05, 7.1516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.2277e-04, 2.9376e-01, 1.1301e-04, 7.0571e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3349e-04, 3.0338e-01, 1.4132e-04, 6.9595e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7095e-04, 3.1372e-01, 1.7535e-04, 6.8544e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0796e-04, 3.2481e-01, 2.0702e-04, 6.7418e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3554e-04, 3.3463e-01, 2.3448e-04, 6.6420e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5575e-04, 3.3815e-01, 2.3353e-04, 6.6066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.1838e-04, 3.3917e-01, 2.1803e-04, 6.5969e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.0589e-04, 3.4499e-01, 2.0850e-04, 6.5390e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7464e-04, 3.5607e-01, 1.9363e-04, 6.4286e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1386e-04, 3.7592e-01, 1.7152e-04, 6.2309e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.8726e-04, 4.0433e-01, 1.3628e-04, 5.9485e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.6421e-04, 4.4293e-01, 1.0429e-04, 5.5640e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8648e-04, 4.9673e-01, 8.3698e-05, 5.0270e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0636e-04, 5.5813e-01, 6.4592e-05, 4.4140e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1320e-04, 6.2342e-01, 4.5683e-05, 3.7622e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2856e-04, 6.8805e-01, 3.0415e-05, 3.1169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6188e-04, 7.4878e-01, 1.9576e-05, 2.5104e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0905e-04, 8.0337e-01, 1.1934e-05, 1.9651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.1835e-05, 8.4949e-01, 7.0927e-06, 1.5043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.6185e-05, 8.8874e-01, 4.0923e-06, 1.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.9689e-05, 9.1706e-01, 2.3717e-06, 8.2911e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.3987e-04, 3.5369e-01, 5.7910e-05, 6.4602e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.0135e-04, 3.6415e-01, 7.2118e-05, 6.3548e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7904e-04, 3.7486e-01, 8.9889e-05, 6.2467e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7536e-04, 3.8613e-01, 1.1135e-04, 6.1329e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.7736e-04, 3.9798e-01, 1.3276e-04, 6.0131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7930e-04, 4.1005e-01, 1.5293e-04, 5.8912e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.4046e-04, 4.1759e-01, 1.6278e-04, 5.8151e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.0342e-04, 4.1637e-01, 1.5061e-04, 5.8278e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.8916e-04, 4.2131e-01, 1.4309e-04, 5.7786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7974e-04, 4.2844e-01, 1.3663e-04, 5.7074e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.4358e-04, 4.4235e-01, 1.2414e-04, 5.5688e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8619e-04, 4.6589e-01, 1.0731e-04, 5.3341e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8648e-04, 4.9673e-01, 8.3698e-05, 5.0270e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.9320e-04, 5.3658e-01, 6.3034e-05, 4.6296e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[65., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3256e-04, 5.8934e-01, 4.9643e-05, 4.1028e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7238e-04, 6.4651e-01, 3.7595e-05, 3.5318e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0762e-04, 7.0561e-01, 2.6317e-05, 2.9415e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4950e-04, 7.5987e-01, 1.7332e-05, 2.3997e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0378e-04, 8.1043e-01, 1.0934e-05, 1.8946e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9981e-05, 8.5324e-01, 6.6878e-06, 1.4669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.4907e-05, 8.9113e-01, 3.8600e-06, 1.0882e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.8848e-05, 9.1889e-01, 2.2355e-06, 8.1080e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6757e-04, 4.2969e-01, 3.6219e-05, 5.7010e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0978e-04, 4.4087e-01, 4.4947e-05, 5.5887e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6294e-04, 4.5225e-01, 5.5826e-05, 5.4743e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2876e-04, 4.6393e-01, 6.9025e-05, 5.3567e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0273e-04, 4.7600e-01, 8.3106e-05, 5.2352e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7451e-04, 4.8897e-01, 9.5895e-05, 5.1046e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3454e-04, 4.9893e-01, 1.0562e-04, 5.0043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3404e-04, 5.0135e-01, 1.0287e-04, 4.9801e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1151e-04, 5.0195e-01, 9.5752e-05, 4.9745e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.0204e-04, 5.0803e-01, 9.1165e-05, 4.9138e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8990e-04, 5.1739e-01, 8.5874e-05, 4.8204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.5757e-04, 5.3344e-01, 7.6801e-05, 4.6602e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0636e-04, 5.5813e-01, 6.4592e-05, 4.4140e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3256e-04, 5.8934e-01, 4.9643e-05, 4.1028e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6461e-04, 6.2771e-01, 3.6790e-05, 3.7198e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1980e-04, 6.7602e-01, 2.8467e-05, 3.2373e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7691e-04, 7.2600e-01, 2.1201e-05, 2.7380e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3378e-04, 7.7630e-01, 1.4737e-05, 2.2355e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5475e-05, 8.1987e-01, 9.6363e-06, 1.8002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.5212e-05, 8.5978e-01, 5.9866e-06, 1.4015e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.3538e-05, 8.9434e-01, 3.6169e-06, 1.0561e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.8032e-05, 9.2076e-01, 2.1064e-06, 7.9208e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1418e-04, 5.0916e-01, 2.2094e-05, 4.9070e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4242e-04, 5.2052e-01, 2.7318e-05, 4.7931e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7785e-04, 5.3200e-01, 3.3807e-05, 4.6779e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2171e-04, 5.4353e-01, 4.1722e-05, 4.5621e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7483e-04, 5.5593e-01, 5.0903e-05, 4.4374e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2146e-04, 5.6799e-01, 5.8302e-05, 4.3163e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7258e-04, 5.7977e-01, 6.6163e-05, 4.1980e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.9347e-04, 5.8620e-01, 6.8166e-05, 4.1334e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.7132e-04, 5.8373e-01, 6.2688e-05, 4.1584e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.6223e-04, 5.8838e-01, 5.9302e-05, 4.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.5504e-04, 5.9467e-01, 5.6345e-05, 4.0492e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3984e-04, 6.0595e-01, 5.1903e-05, 3.9366e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1320e-04, 6.2342e-01, 4.5683e-05, 3.7622e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7238e-04, 6.4651e-01, 3.7595e-05, 3.5318e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1980e-04, 6.7602e-01, 2.8467e-05, 3.2373e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7231e-04, 7.1058e-01, 2.0779e-05, 2.8922e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4089e-04, 7.5209e-01, 1.5832e-05, 2.4775e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1179e-04, 7.9337e-01, 1.1632e-05, 2.0650e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.4177e-05, 8.3400e-01, 8.0581e-06, 1.6590e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.9277e-05, 8.6816e-01, 5.2003e-06, 1.3177e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.9868e-05, 8.9984e-01, 3.1755e-06, 1.0012e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.6640e-05, 9.2389e-01, 1.9283e-06, 7.6077e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5846e-05, 5.8816e-01, 1.3139e-05, 4.1175e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.4256e-05, 5.9913e-01, 1.6187e-05, 4.0076e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1729e-04, 6.1015e-01, 1.9959e-05, 3.8972e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4586e-04, 6.2104e-01, 2.4597e-05, 3.7879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8047e-04, 6.3277e-01, 2.9966e-05, 3.6702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.1246e-04, 6.4369e-01, 3.4581e-05, 3.5607e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4804e-04, 6.5552e-01, 3.9546e-05, 3.4419e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.7238e-04, 6.6344e-01, 4.2429e-05, 3.3624e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6665e-04, 6.6423e-01, 4.0490e-05, 3.3547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.5477e-04, 6.6427e-01, 3.7604e-05, 3.3544e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4907e-04, 6.6968e-01, 3.5662e-05, 3.3004e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4390e-04, 6.7615e-01, 3.3805e-05, 3.2357e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2856e-04, 6.8805e-01, 3.0415e-05, 3.1169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.0762e-04, 7.0561e-01, 2.6317e-05, 2.9415e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7691e-04, 7.2600e-01, 2.1201e-05, 2.7380e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4089e-04, 7.5209e-01, 1.5832e-05, 2.4775e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0901e-04, 7.8142e-01, 1.1401e-05, 2.1846e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7989e-05, 8.1518e-01, 8.5786e-06, 1.8472e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9061e-05, 8.4765e-01, 6.2396e-06, 1.5227e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1969e-05, 8.7914e-01, 4.3233e-06, 1.2081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.5937e-05, 9.0660e-01, 2.7308e-06, 9.3361e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.4349e-05, 9.2804e-01, 1.6893e-06, 7.1934e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.9153e-05, 6.6287e-01, 7.6234e-06, 3.3708e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.0874e-05, 6.7295e-01, 9.3595e-06, 3.2698e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5493e-05, 6.8301e-01, 1.1502e-05, 3.1690e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.3578e-05, 6.9290e-01, 1.4128e-05, 3.0699e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1546e-04, 7.0331e-01, 1.7184e-05, 2.9655e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3722e-04, 7.1282e-01, 2.0043e-05, 2.8703e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.5971e-04, 7.2335e-01, 2.2847e-05, 2.7647e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8111e-04, 7.3216e-01, 2.5360e-05, 2.6763e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8655e-04, 7.3627e-01, 2.5473e-05, 2.6351e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7596e-04, 7.3396e-01, 2.3418e-05, 2.6584e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7083e-04, 7.3727e-01, 2.2051e-05, 2.6254e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6671e-04, 7.4195e-01, 2.0875e-05, 2.5786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6188e-04, 7.4878e-01, 1.9576e-05, 2.5104e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4950e-04, 7.5987e-01, 1.7332e-05, 2.3997e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.3378e-04, 7.7630e-01, 1.4737e-05, 2.2355e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1179e-04, 7.9337e-01, 1.1632e-05, 2.0650e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7989e-05, 8.1518e-01, 8.5786e-06, 1.8472e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.7317e-05, 8.3885e-01, 6.1062e-06, 1.6107e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3801e-05, 8.6509e-01, 4.5511e-06, 1.3485e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.1881e-05, 8.8985e-01, 3.2849e-06, 1.1011e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.1390e-05, 9.1469e-01, 2.2620e-06, 8.5278e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.1674e-05, 9.3357e-01, 1.4313e-06, 6.6411e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[90.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1137e-05, 7.3023e-01, 4.3235e-06, 2.6973e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.8443e-05, 7.3909e-01, 5.2917e-06, 2.6087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7531e-05, 7.4788e-01, 6.4834e-06, 2.5206e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.8744e-05, 7.5647e-01, 7.9403e-06, 2.4346e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2324e-05, 7.6533e-01, 9.6477e-06, 2.3459e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7351e-05, 7.7378e-01, 1.1453e-05, 2.2613e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0073e-04, 7.8225e-01, 1.2931e-05, 2.1764e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1627e-04, 7.9079e-01, 1.4627e-05, 2.0908e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2471e-04, 7.9576e-01, 1.5314e-05, 2.0410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2037e-04, 7.9558e-01, 1.4408e-05, 2.0429e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1482e-04, 7.9532e-01, 1.3363e-05, 2.0455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1187e-04, 7.9919e-01, 1.2629e-05, 2.0069e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0905e-04, 8.0337e-01, 1.1934e-05, 1.9651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0378e-04, 8.1043e-01, 1.0934e-05, 1.8946e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5475e-05, 8.1987e-01, 9.6363e-06, 1.8002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.4177e-05, 8.3400e-01, 8.0581e-06, 1.6590e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9061e-05, 8.4765e-01, 6.2396e-06, 1.5227e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3801e-05, 8.6509e-01, 4.5511e-06, 1.3485e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0784e-05, 8.8344e-01, 3.2085e-06, 1.1651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2362e-05, 9.0313e-01, 2.3752e-06, 9.6839e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.4933e-05, 9.2288e-01, 1.6922e-06, 7.7090e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.8675e-05, 9.3948e-01, 1.1686e-06, 6.0502e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[95.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.9332e-05, 7.8843e-01, 2.4032e-06, 2.1154e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.,  5.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.3804e-05, 7.9591e-01, 2.9335e-06, 2.0406e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.9355e-05, 8.0330e-01, 3.5849e-06, 1.9667e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 15.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.6189e-05, 8.1048e-01, 4.3794e-06, 1.8948e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.4482e-05, 8.1774e-01, 5.3184e-06, 1.8221e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 25.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.4454e-05, 8.2513e-01, 6.4086e-06, 1.7481e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.2445e-05, 8.3153e-01, 7.1946e-06, 1.6840e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 35.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2239e-05, 8.3870e-01, 8.1510e-06, 1.6122e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0005e-05, 8.4402e-01, 8.8319e-06, 1.5589e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 45.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0781e-05, 8.4598e-01, 8.6956e-06, 1.5393e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.6294e-05, 8.4442e-01, 8.0046e-06, 1.5550e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 55.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.3804e-05, 8.4635e-01, 7.5120e-06, 1.5357e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.1835e-05, 8.4949e-01, 7.0927e-06, 1.5043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 65.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9981e-05, 8.5324e-01, 6.6878e-06, 1.4669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.5212e-05, 8.5978e-01, 5.9866e-06, 1.4015e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 75.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.9277e-05, 8.6816e-01, 5.2003e-06, 1.3177e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1969e-05, 8.7914e-01, 4.3233e-06, 1.2081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 85.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.1881e-05, 8.8985e-01, 3.2849e-06, 1.1011e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.2362e-05, 9.0313e-01, 2.3752e-06, 9.6839e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95., 95.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4350e-05, 9.1692e-01, 1.6615e-06, 8.3059e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[ 95., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.9203e-05, 9.3240e-01, 1.2200e-06, 6.7584e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 95., 105.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.4743e-05, 9.4556e-01, 8.6856e-07, 5.4427e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[100.,   0.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.1860e-05, 8.4029e-01, 1.3158e-06, 1.5970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,   5.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.4555e-05, 8.4625e-01, 1.6012e-06, 1.5374e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  10.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.7909e-05, 8.5215e-01, 1.9523e-06, 1.4783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  15.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.2029e-05, 8.5787e-01, 2.3797e-06, 1.4211e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  20.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.6982e-05, 8.6332e-01, 2.8846e-06, 1.3665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  25.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.2998e-05, 8.6919e-01, 3.4727e-06, 1.3078e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  30.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.8806e-05, 8.7423e-01, 4.0071e-06, 1.2573e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  35.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.4787e-05, 8.7984e-01, 4.5291e-06, 1.2011e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  40.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[5.0636e-05, 8.8453e-01, 5.0113e-06, 1.1542e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  45.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[5.2196e-05, 8.8739e-01, 5.0294e-06, 1.1255e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  50.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.9848e-05, 8.8673e-01, 4.6840e-06, 1.1321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  55.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.7555e-05, 8.8657e-01, 4.3446e-06, 1.1338e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  60.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.6185e-05, 8.8874e-01, 4.0923e-06, 1.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  65.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.4907e-05, 8.9113e-01, 3.8600e-06, 1.0882e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  70.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.3538e-05, 8.9434e-01, 3.6169e-06, 1.0561e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  75.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.9868e-05, 8.9984e-01, 3.1755e-06, 1.0012e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  80.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.5937e-05, 9.0660e-01, 2.7308e-06, 9.3361e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  85.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.1390e-05, 9.1469e-01, 2.2620e-06, 8.5278e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  90.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.4933e-05, 9.2288e-01, 1.6922e-06, 7.7090e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.,  95.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.9203e-05, 9.3240e-01, 1.2200e-06, 6.7584e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100., 100.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[1.4134e-05, 9.4345e-01, 8.2740e-07, 5.6532e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100., 105.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[1.1119e-05, 9.5342e-01, 6.0767e-07, 4.6571e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[105.,   0.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[7.1316e-06, 8.7869e-01, 7.0833e-07, 1.2130e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,   5.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[8.7368e-06, 8.8341e-01, 8.6048e-07, 1.1658e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  10.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.0732e-05, 8.8808e-01, 1.0474e-06, 1.1191e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  15.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.3179e-05, 8.9258e-01, 1.2746e-06, 1.0741e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  20.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.6134e-05, 8.9680e-01, 1.5459e-06, 1.0319e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  25.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.9698e-05, 9.0139e-01, 1.8581e-06, 9.8591e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  30.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.3555e-05, 9.0549e-01, 2.1833e-06, 9.4484e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  35.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.7003e-05, 9.0958e-01, 2.4506e-06, 9.0389e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  40.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.0985e-05, 9.1367e-01, 2.7545e-06, 8.6296e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  45.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.2887e-05, 9.1579e-01, 2.8541e-06, 8.4172e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  50.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.2539e-05, 9.1704e-01, 2.7490e-06, 8.2927e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  55.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.0754e-05, 9.1609e-01, 2.5328e-06, 8.3878e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  60.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.9689e-05, 9.1706e-01, 2.3717e-06, 8.2911e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  65.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.8848e-05, 9.1889e-01, 2.2355e-06, 8.1080e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  70.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.8032e-05, 9.2076e-01, 2.1064e-06, 7.9208e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  75.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.6640e-05, 9.2389e-01, 1.9283e-06, 7.6077e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  80.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.4349e-05, 9.2804e-01, 1.6893e-06, 7.1934e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  85.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.1674e-05, 9.3357e-01, 1.4313e-06, 6.6411e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  90.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.8675e-05, 9.3948e-01, 1.1686e-06, 6.0502e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.,  95.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.4743e-05, 9.4556e-01, 8.6856e-07, 5.4427e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105., 100.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[1.1119e-05, 9.5342e-01, 6.0767e-07, 4.6571e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105., 105.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[8.2771e-06, 9.6046e-01, 4.2026e-07, 3.9529e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[0.0057, 0.0087, 0.0056, 0.9800]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[0.0000, 5.0000, 0.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[0.0055, 0.0110, 0.0050, 0.9786]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0150, 0.0036, 0.9770]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0034, 0.0206, 0.0025, 0.9734]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0027, 0.0283, 0.0018, 0.9672]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0388, 0.0013, 0.9578]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7041e-03, 5.2990e-02, 8.9345e-04, 9.4441e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3256e-03, 7.1575e-02, 6.2247e-04, 9.2648e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0215e-03, 9.5899e-02, 4.2966e-04, 9.0265e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.8020e-04, 1.2734e-01, 2.9393e-04, 8.7158e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.8911e-04, 1.6717e-01, 1.9879e-04, 8.3204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.3854e-04, 2.1637e-01, 1.3255e-04, 7.8306e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2086e-04, 2.7524e-01, 8.6868e-05, 7.2435e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.3009e-04, 3.4317e-01, 5.5788e-05, 6.5654e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6130e-04, 4.1838e-01, 3.5015e-05, 5.8142e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1031e-04, 4.9758e-01, 2.1438e-05, 5.0229e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 0.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.3546e-05, 5.7690e-01, 1.2796e-05, 4.2301e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.7829e-05, 6.5244e-01, 7.4500e-06, 3.4751e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.0394e-05, 7.2101e-01, 4.2385e-06, 2.7895e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 0.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8922e-05, 7.8060e-01, 2.3624e-06, 2.1938e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.1637e-05, 8.3398e-01, 1.2966e-06, 1.6601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  0.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[7.0100e-06, 8.7366e-01, 6.9926e-07, 1.2633e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[5.0000, 0.0000, 0.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[0.0055, 0.0110, 0.0050, 0.9786]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[5.0000, 5.0000, 0.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[0.0060, 0.0118, 0.0053, 0.9769]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0053, 0.0155, 0.0043, 0.9749]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0214, 0.0032, 0.9710]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0036, 0.0296, 0.0023, 0.9645]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0028, 0.0406, 0.0016, 0.9550]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0552, 0.0011, 0.9415]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6880e-03, 7.4314e-02, 7.8780e-04, 9.2321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2957e-03, 9.9473e-02, 5.4134e-04, 8.9869e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.8902e-04, 1.3205e-01, 3.6993e-04, 8.6659e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.4601e-04, 1.7322e-01, 2.4982e-04, 8.2578e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.5446e-04, 2.2390e-01, 1.6623e-04, 7.7537e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.0482e-04, 2.8431e-01, 1.0866e-04, 7.1518e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.8948e-04, 3.5357e-01, 6.9563e-05, 6.4607e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0223e-04, 4.2957e-01, 4.3507e-05, 5.7019e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[ 5.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3779e-04, 5.0903e-01, 2.6540e-05, 4.9081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1525e-05, 5.8803e-01, 1.5783e-05, 4.1186e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.9313e-05, 6.6274e-01, 9.1571e-06, 3.3719e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.7572e-05, 7.3012e-01, 5.1932e-06, 2.6984e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 5.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.3326e-05, 7.8833e-01, 2.8865e-06, 2.1164e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.4297e-05, 8.4016e-01, 1.5793e-06, 1.5982e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[  5.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[8.5966e-06, 8.7858e-01, 8.5017e-07, 1.2141e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0150, 0.0036, 0.9770]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0053, 0.0155, 0.0043, 0.9749]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0054, 0.0174, 0.0042, 0.9730]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0049, 0.0224, 0.0035, 0.9692]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0042, 0.0308, 0.0026, 0.9624]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0034, 0.0425, 0.0020, 0.9522]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0028, 0.0579, 0.0014, 0.9380]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0781, 0.0010, 0.9187]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6780e-03, 1.0422e-01, 6.9402e-04, 8.9341e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2681e-03, 1.3771e-01, 4.7002e-04, 8.6055e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.4795e-04, 1.7995e-01, 3.1458e-04, 8.1879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0192e-04, 2.3198e-01, 2.0853e-04, 7.6711e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1106e-04, 2.9375e-01, 1.3593e-04, 7.0560e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6429e-04, 3.6415e-01, 8.6748e-05, 6.3540e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5360e-04, 4.4088e-01, 5.4066e-05, 5.5881e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[10.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7217e-04, 5.2053e-01, 3.2861e-05, 4.7926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1395e-04, 5.9915e-01, 1.9471e-05, 4.0072e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.3591e-05, 6.7297e-01, 1.1258e-05, 3.2695e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.6473e-05, 7.3911e-01, 6.3653e-06, 2.6084e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[10.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.8776e-05, 7.9593e-01, 3.5287e-06, 2.0404e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.7596e-05, 8.4626e-01, 1.9261e-06, 1.5372e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 10.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.0562e-05, 8.8342e-01, 1.0351e-06, 1.1657e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0034, 0.0206, 0.0025, 0.9734]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0214, 0.0032, 0.9710]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0049, 0.0224, 0.0035, 0.9692]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0046, 0.0253, 0.0031, 0.9670]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0322, 0.0027, 0.9608]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0037, 0.0437, 0.0021, 0.9505]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0031, 0.0599, 0.0016, 0.9354]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0025, 0.0812, 0.0011, 0.9151]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0130e-03, 1.0878e-01, 8.1477e-04, 8.8839e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5703e-03, 1.4423e-01, 5.7130e-04, 8.5363e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1943e-03, 1.8809e-01, 3.8994e-04, 8.1033e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.9419e-04, 2.4150e-01, 2.6201e-04, 7.5734e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.4814e-04, 3.0429e-01, 1.7028e-04, 6.9489e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.5722e-04, 3.7512e-01, 1.0764e-04, 6.2431e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1736e-04, 4.5235e-01, 6.6969e-05, 5.4727e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[15.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1482e-04, 5.3192e-01, 4.0630e-05, 4.6782e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4179e-04, 6.1016e-01, 2.4009e-05, 3.8968e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1262e-05, 6.8303e-01, 1.3836e-05, 3.1687e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7460e-05, 7.4790e-01, 7.7988e-06, 2.5204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[15.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.5487e-05, 8.0331e-01, 4.3122e-06, 1.9665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.1650e-05, 8.5216e-01, 2.3484e-06, 1.4781e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 15.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.2973e-05, 8.8809e-01, 1.2599e-06, 1.1190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0027, 0.0283, 0.0018, 0.9672]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0036, 0.0296, 0.0023, 0.9645]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0042, 0.0308, 0.0026, 0.9624]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0044, 0.0322, 0.0027, 0.9608]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0039, 0.0368, 0.0022, 0.9571]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0036, 0.0462, 0.0020, 0.9482]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0032, 0.0611, 0.0015, 0.9342]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0027, 0.0826, 0.0012, 0.9135]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2332e-03, 1.1187e-01, 8.8296e-04, 8.8502e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8185e-03, 1.4935e-01, 6.4633e-04, 8.4819e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4012e-03, 1.9511e-01, 4.4728e-04, 8.0304e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0613e-03, 2.5077e-01, 3.0434e-04, 7.4786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.8142e-04, 3.1546e-01, 2.0124e-04, 6.8356e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.6014e-04, 3.8773e-01, 1.2946e-04, 6.1158e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9202e-04, 4.6529e-01, 8.1312e-05, 5.3424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[20.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6709e-04, 5.4472e-01, 4.9716e-05, 4.5496e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7560e-04, 6.2185e-01, 2.9304e-05, 3.7794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1272e-04, 6.9348e-01, 1.6860e-05, 3.0639e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0811e-05, 7.5684e-01, 9.4936e-06, 2.4308e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[20.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.3660e-05, 8.1067e-01, 5.2465e-06, 1.8928e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6541e-05, 8.5772e-01, 2.8517e-06, 1.4225e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 20.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.5915e-05, 8.9254e-01, 1.5315e-06, 1.0744e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0388, 0.0013, 0.9578]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0028, 0.0406, 0.0016, 0.9550]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0034, 0.0425, 0.0020, 0.9522]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0037, 0.0437, 0.0021, 0.9505]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0036, 0.0462, 0.0020, 0.9482]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0032, 0.0528, 0.0016, 0.9424]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0030, 0.0656, 0.0014, 0.9300]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0026, 0.0850, 0.0011, 0.9112]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2090e-03, 1.1325e-01, 8.4871e-04, 8.8369e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8313e-03, 1.5058e-01, 6.3447e-04, 8.4695e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5190e-03, 1.9950e-01, 4.7350e-04, 7.9851e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2057e-03, 2.5799e-01, 3.3790e-04, 7.4046e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.0512e-04, 3.2491e-01, 2.2791e-04, 6.7396e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.5594e-04, 3.9921e-01, 1.4833e-04, 5.9998e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.6215e-04, 4.7824e-01, 9.3877e-05, 5.2120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[25.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1415e-04, 5.5742e-01, 5.7269e-05, 4.4221e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0831e-04, 6.3377e-01, 3.4080e-05, 3.6599e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3494e-04, 7.0393e-01, 1.9812e-05, 2.9591e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.5589e-05, 7.6563e-01, 1.1277e-05, 2.3428e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[25.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3327e-05, 8.1822e-01, 6.3063e-06, 1.8172e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.2467e-05, 8.6378e-01, 3.4341e-06, 1.3618e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 25.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.9411e-05, 8.9716e-01, 1.8402e-06, 1.0282e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7041e-03, 5.2990e-02, 8.9345e-04, 9.4441e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0552, 0.0011, 0.9415]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0028, 0.0579, 0.0014, 0.9380]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0031, 0.0599, 0.0016, 0.9354]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0032, 0.0611, 0.0015, 0.9342]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0030, 0.0656, 0.0014, 0.9300]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0026, 0.0751, 0.0011, 0.9211]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0024, 0.0924, 0.0010, 0.9041]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1713e-03, 1.1767e-01, 8.0387e-04, 8.7936e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8115e-03, 1.5372e-01, 6.0810e-04, 8.4386e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4725e-03, 2.0059e-01, 4.4608e-04, 7.9749e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1835e-03, 2.5847e-01, 3.2332e-04, 7.4002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.5068e-04, 3.2937e-01, 2.3367e-04, 6.6945e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.2622e-04, 4.0722e-01, 1.6051e-04, 5.9190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.2790e-04, 4.8858e-01, 1.0485e-04, 5.1079e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[30.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6252e-04, 5.6877e-01, 6.4662e-05, 4.3081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4266e-04, 6.4548e-01, 3.8870e-05, 3.5424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5713e-04, 7.1466e-01, 2.2593e-05, 2.8516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.9355e-05, 7.7484e-01, 1.2821e-05, 2.2505e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[30.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.1727e-05, 8.2541e-01, 7.1483e-06, 1.7452e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.8424e-05, 8.6942e-01, 3.9880e-06, 1.3054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 30.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.3217e-05, 9.0147e-01, 2.1625e-06, 9.8503e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3256e-03, 7.1575e-02, 6.2247e-04, 9.2648e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6880e-03, 7.4314e-02, 7.8780e-04, 9.2321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0022, 0.0781, 0.0010, 0.9187]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0025, 0.0812, 0.0011, 0.9151]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0027, 0.0826, 0.0012, 0.9135]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0026, 0.0850, 0.0011, 0.9112]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[0.0024, 0.0924, 0.0010, 0.9041]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1286e-03, 1.0579e-01, 8.0385e-04, 8.9128e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.9682e-03, 1.2892e-01, 6.9003e-04, 8.6842e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7529e-03, 1.6090e-01, 5.6512e-04, 8.3679e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4619e-03, 2.0552e-01, 4.2853e-04, 7.9259e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1642e-03, 2.6252e-01, 3.0845e-04, 7.3601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1204e-04, 3.3012e-01, 2.1786e-04, 6.6875e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0093e-04, 4.0659e-01, 1.5100e-04, 5.9256e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3463e-04, 4.9148e-01, 1.0359e-04, 5.0789e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[35.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9183e-04, 5.7573e-01, 6.8294e-05, 4.2381e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.7084e-04, 6.5413e-01, 4.2408e-05, 3.4556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7890e-04, 7.2358e-01, 2.5171e-05, 2.7622e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1422e-04, 7.8324e-01, 1.4432e-05, 2.1663e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[35.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.1425e-05, 8.3279e-01, 8.1008e-06, 1.6713e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.4365e-05, 8.7522e-01, 4.5095e-06, 1.2473e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 35.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6784e-05, 9.0598e-01, 2.4432e-06, 9.3990e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0215e-03, 9.5899e-02, 4.2966e-04, 9.0265e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2957e-03, 9.9473e-02, 5.4134e-04, 8.9869e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6780e-03, 1.0422e-01, 6.9402e-04, 8.9341e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0130e-03, 1.0878e-01, 8.1477e-04, 8.8839e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2332e-03, 1.1187e-01, 8.8296e-04, 8.8502e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2090e-03, 1.1325e-01, 8.4871e-04, 8.8369e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1713e-03, 1.1767e-01, 8.0387e-04, 8.7936e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.9682e-03, 1.2892e-01, 6.9003e-04, 8.6842e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7014e-03, 1.4700e-01, 5.5726e-04, 8.5074e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5609e-03, 1.7698e-01, 4.7507e-04, 8.2098e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3851e-03, 2.1695e-01, 3.8833e-04, 7.8127e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1404e-03, 2.7067e-01, 2.9127e-04, 7.2789e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.0052e-04, 3.3610e-01, 2.0844e-04, 6.6279e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.8668e-04, 4.1143e-01, 1.4345e-04, 5.8774e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1134e-04, 4.9178e-01, 9.6307e-05, 5.0761e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[40.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.7294e-04, 5.7410e-01, 6.3337e-05, 4.2547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6867e-04, 6.5536e-01, 4.1040e-05, 3.4433e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8847e-04, 7.2845e-01, 2.5893e-05, 2.7134e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2522e-04, 7.8919e-01, 1.5457e-05, 2.1067e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[40.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.0157e-05, 8.3848e-01, 8.8946e-06, 1.6143e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[5.0181e-05, 8.7995e-01, 4.9893e-06, 1.2000e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 40.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.0610e-05, 9.1005e-01, 2.7331e-06, 8.9915e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.8020e-04, 1.2734e-01, 2.9393e-04, 8.7158e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.8902e-04, 1.3205e-01, 3.6993e-04, 8.6659e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2681e-03, 1.3771e-01, 4.7002e-04, 8.6055e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5703e-03, 1.4423e-01, 5.7130e-04, 8.5363e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8185e-03, 1.4935e-01, 6.4633e-04, 8.4819e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8313e-03, 1.5058e-01, 6.3447e-04, 8.4695e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8115e-03, 1.5372e-01, 6.0810e-04, 8.4386e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7529e-03, 1.6090e-01, 5.6512e-04, 8.3679e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5609e-03, 1.7698e-01, 4.7507e-04, 8.2098e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3358e-03, 2.0065e-01, 3.7946e-04, 7.9763e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2111e-03, 2.3793e-01, 3.1997e-04, 7.6054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0707e-03, 2.8635e-01, 2.6087e-04, 7.1232e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.6705e-04, 3.4744e-01, 1.9295e-04, 6.5150e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.7794e-04, 4.1878e-01, 1.3704e-04, 5.8041e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.0199e-04, 4.9786e-01, 9.1717e-05, 5.0154e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[45.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6338e-04, 5.7849e-01, 5.9856e-05, 4.2108e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5638e-04, 6.5514e-01, 3.8073e-05, 3.4456e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7779e-04, 7.2618e-01, 2.3802e-05, 2.7362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2192e-04, 7.8908e-01, 1.4681e-05, 2.1079e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[45.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.1972e-05, 8.4070e-01, 8.8781e-06, 1.5921e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[5.3113e-05, 8.8325e-01, 5.1493e-06, 1.1669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 45.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.3523e-05, 9.1297e-01, 2.9247e-06, 8.6995e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.8911e-04, 1.6717e-01, 1.9879e-04, 8.3204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.4601e-04, 1.7322e-01, 2.4982e-04, 8.2578e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.4795e-04, 1.7995e-01, 3.1458e-04, 8.1879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1943e-03, 1.8809e-01, 3.8994e-04, 8.1033e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4012e-03, 1.9511e-01, 4.4728e-04, 8.0304e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5190e-03, 1.9950e-01, 4.7350e-04, 7.9851e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4725e-03, 2.0059e-01, 4.4608e-04, 7.9749e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4619e-03, 2.0552e-01, 4.2853e-04, 7.9259e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3851e-03, 2.1695e-01, 3.8833e-04, 7.8127e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2111e-03, 2.3793e-01, 3.1997e-04, 7.6054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0252e-03, 2.6771e-01, 2.5257e-04, 7.3101e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1593e-04, 3.1219e-01, 2.0997e-04, 6.8668e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.9845e-04, 3.6752e-01, 1.6896e-04, 6.3152e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.4008e-04, 4.3303e-01, 1.2411e-04, 5.6621e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[50.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.8862e-04, 5.0792e-01, 8.6092e-05, 4.9151e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.5643e-04, 5.8509e-01, 5.6888e-05, 4.1450e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5069e-04, 6.6062e-01, 3.6115e-05, 3.3910e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7242e-04, 7.2931e-01, 2.2393e-05, 2.7049e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1618e-04, 7.8866e-01, 1.3605e-05, 2.1121e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[50.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.7427e-05, 8.3915e-01, 8.1717e-06, 1.6076e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[5.0675e-05, 8.8270e-01, 4.7899e-06, 1.1724e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 50.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.3113e-05, 9.1393e-01, 2.8148e-06, 8.6031e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.3854e-04, 2.1637e-01, 1.3255e-04, 7.8306e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.5446e-04, 2.2390e-01, 1.6623e-04, 7.7537e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0192e-04, 2.3198e-01, 2.0853e-04, 7.6711e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.9419e-04, 2.4150e-01, 2.6201e-04, 7.5734e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0613e-03, 2.5077e-01, 3.0434e-04, 7.4786e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2057e-03, 2.5799e-01, 3.3790e-04, 7.4046e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1835e-03, 2.5847e-01, 3.2332e-04, 7.4002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1642e-03, 2.6252e-01, 3.0845e-04, 7.3601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1404e-03, 2.7067e-01, 2.9127e-04, 7.2789e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0707e-03, 2.8635e-01, 2.6087e-04, 7.1232e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1593e-04, 3.1219e-01, 2.0997e-04, 6.8668e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.6527e-04, 3.4743e-01, 1.6352e-04, 6.5164e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.7212e-04, 3.9760e-01, 1.3368e-04, 6.0159e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7446e-04, 4.5695e-01, 1.0556e-04, 5.4237e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[55.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.5508e-04, 5.2446e-01, 7.6736e-05, 4.7501e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.4009e-04, 5.9686e-01, 5.2210e-05, 4.0274e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4566e-04, 6.6741e-01, 3.4241e-05, 3.3231e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6829e-04, 7.3409e-01, 2.1203e-05, 2.6572e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1320e-04, 7.9258e-01, 1.2858e-05, 2.0729e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[55.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.4827e-05, 8.4099e-01, 7.6628e-06, 1.5893e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.8301e-05, 8.8244e-01, 4.4398e-06, 1.1751e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 55.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.1301e-05, 9.1300e-01, 2.5932e-06, 8.6965e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2086e-04, 2.7524e-01, 8.6868e-05, 7.2435e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.0482e-04, 2.8431e-01, 1.0866e-04, 7.1518e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1106e-04, 2.9375e-01, 1.3593e-04, 7.0560e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.4814e-04, 3.0429e-01, 1.7028e-04, 6.9489e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.8142e-04, 3.1546e-01, 2.0124e-04, 6.8356e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.0512e-04, 3.2491e-01, 2.2791e-04, 6.7396e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.5068e-04, 3.2937e-01, 2.3367e-04, 6.6945e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1204e-04, 3.3012e-01, 2.1786e-04, 6.6875e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.0052e-04, 3.3610e-01, 2.0844e-04, 6.6279e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.6705e-04, 3.4744e-01, 1.9295e-04, 6.5150e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.9845e-04, 3.6752e-01, 1.6896e-04, 6.3152e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.7212e-04, 3.9760e-01, 1.3368e-04, 6.0159e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.5331e-04, 4.3671e-01, 1.0254e-04, 5.6264e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.7699e-04, 4.8973e-01, 8.2315e-05, 5.0971e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 0.0\n",
            "0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[60.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9955e-04, 5.4924e-01, 6.3750e-05, 4.5030e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1251e-04, 6.1492e-01, 4.5793e-05, 3.8473e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2945e-04, 6.7986e-01, 3.0691e-05, 3.1988e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6289e-04, 7.4159e-01, 1.9799e-05, 2.5823e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1040e-04, 7.9703e-01, 1.2149e-05, 2.0285e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[60.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.2840e-05, 8.4422e-01, 7.2361e-06, 1.5570e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.6907e-05, 8.8466e-01, 4.1818e-06, 1.1529e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 60.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.0189e-05, 9.1391e-01, 2.4264e-06, 8.6058e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.3009e-04, 3.4317e-01, 5.5788e-05, 6.5654e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.8948e-04, 3.5357e-01, 6.9563e-05, 6.4607e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6429e-04, 3.6415e-01, 8.6748e-05, 6.3540e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.5722e-04, 3.7512e-01, 1.0764e-04, 6.2431e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.6014e-04, 3.8773e-01, 1.2946e-04, 6.1158e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.5594e-04, 3.9921e-01, 1.4833e-04, 5.9998e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.2622e-04, 4.0722e-01, 1.6051e-04, 5.9190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0093e-04, 4.0659e-01, 1.5100e-04, 5.9256e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.8668e-04, 4.1143e-01, 1.4345e-04, 5.8774e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.7794e-04, 4.1878e-01, 1.3704e-04, 5.8041e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.4008e-04, 4.3303e-01, 1.2411e-04, 5.6621e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7446e-04, 4.5695e-01, 1.0556e-04, 5.4237e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.7699e-04, 4.8973e-01, 8.2315e-05, 5.0971e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[65.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.8645e-04, 5.3027e-01, 6.2116e-05, 4.6928e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[65.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2692e-04, 5.8254e-01, 4.8949e-05, 4.1708e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6858e-04, 6.3839e-01, 3.7209e-05, 3.6131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0778e-04, 6.9807e-01, 2.6459e-05, 3.0170e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5049e-04, 7.5284e-01, 1.7539e-05, 2.4700e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0467e-04, 8.0454e-01, 1.1085e-05, 1.9535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[65.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0985e-05, 8.4821e-01, 6.8224e-06, 1.5172e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.5613e-05, 8.8712e-01, 3.9448e-06, 1.1283e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 65.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.9335e-05, 9.1580e-01, 2.2873e-06, 8.4164e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6130e-04, 4.1838e-01, 3.5015e-05, 5.8142e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0223e-04, 4.2957e-01, 4.3507e-05, 5.7019e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5360e-04, 4.4088e-01, 5.4066e-05, 5.5881e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1736e-04, 4.5235e-01, 6.6969e-05, 5.4727e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9202e-04, 4.6529e-01, 8.1312e-05, 5.3424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.6215e-04, 4.7824e-01, 9.3877e-05, 5.2120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.2790e-04, 4.8858e-01, 1.0485e-04, 5.1079e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3463e-04, 4.9148e-01, 1.0359e-04, 5.0789e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1134e-04, 4.9178e-01, 9.6307e-05, 5.0761e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.0199e-04, 4.9786e-01, 9.1717e-05, 5.0154e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[70.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.8862e-04, 5.0792e-01, 8.6092e-05, 4.9151e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.5508e-04, 5.2446e-01, 7.6736e-05, 4.7501e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9955e-04, 5.4924e-01, 6.3750e-05, 4.5030e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2692e-04, 5.8254e-01, 4.8949e-05, 4.1708e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6064e-04, 6.2175e-01, 3.6334e-05, 3.7795e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[70.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1659e-04, 6.6985e-01, 2.8138e-05, 3.2990e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7485e-04, 7.1939e-01, 2.1030e-05, 2.8042e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3423e-04, 7.6997e-01, 1.4854e-05, 2.2988e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.6042e-05, 8.1438e-01, 9.7401e-06, 1.8551e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[70.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.5896e-05, 8.5514e-01, 6.0807e-06, 1.4478e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.4054e-05, 8.9070e-01, 3.6787e-06, 1.0925e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 70.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.8512e-05, 9.1782e-01, 2.1546e-06, 8.2147e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1031e-04, 4.9758e-01, 2.1438e-05, 5.0229e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "3 -0.0\n",
            "-0.0 <class 'numpy.ndarray'>\n",
            "state: tensor([[75.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3779e-04, 5.0903e-01, 2.6540e-05, 4.9081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7217e-04, 5.2053e-01, 3.2861e-05, 4.7926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1482e-04, 5.3192e-01, 4.0630e-05, 4.6782e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6709e-04, 5.4472e-01, 4.9716e-05, 4.5496e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1415e-04, 5.5742e-01, 5.7269e-05, 4.4221e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6252e-04, 5.6877e-01, 6.4662e-05, 4.3081e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.9183e-04, 5.7573e-01, 6.8294e-05, 4.2381e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.7294e-04, 5.7410e-01, 6.3337e-05, 4.2547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6338e-04, 5.7849e-01, 5.9856e-05, 4.2108e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.5643e-04, 5.8509e-01, 5.6888e-05, 4.1450e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.4009e-04, 5.9686e-01, 5.2210e-05, 4.0274e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.1251e-04, 6.1492e-01, 4.5793e-05, 3.8473e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6858e-04, 6.3839e-01, 3.7209e-05, 3.6131e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1659e-04, 6.6985e-01, 2.8138e-05, 3.2990e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7007e-04, 7.0531e-01, 2.0562e-05, 2.9450e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[75.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3914e-04, 7.4683e-01, 1.5683e-05, 2.5302e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1068e-04, 7.8828e-01, 1.1557e-05, 2.1160e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.4637e-05, 8.2895e-01, 8.1396e-06, 1.7096e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[75.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.9676e-05, 8.6398e-01, 5.2594e-06, 1.3596e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[4.0348e-05, 8.9637e-01, 3.2304e-06, 1.0359e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 75.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6986e-05, 9.2119e-01, 1.9634e-06, 7.8785e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.3546e-05, 5.7690e-01, 1.2796e-05, 4.2301e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1525e-05, 5.8803e-01, 1.5783e-05, 4.1186e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1395e-04, 5.9915e-01, 1.9471e-05, 4.0072e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.4179e-04, 6.1016e-01, 2.4009e-05, 3.8968e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7560e-04, 6.2185e-01, 2.9304e-05, 3.7794e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0831e-04, 6.3377e-01, 3.4080e-05, 3.6599e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4266e-04, 6.4548e-01, 3.8870e-05, 3.5424e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.7084e-04, 6.5413e-01, 4.2408e-05, 3.4556e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6867e-04, 6.5536e-01, 4.1040e-05, 3.4433e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5638e-04, 6.5514e-01, 3.8073e-05, 3.4456e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.5069e-04, 6.6062e-01, 3.6115e-05, 3.3910e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4566e-04, 6.6741e-01, 3.4241e-05, 3.3231e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.2945e-04, 6.7986e-01, 3.0691e-05, 3.1988e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0778e-04, 6.9807e-01, 2.6459e-05, 3.0170e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7485e-04, 7.1939e-01, 2.1030e-05, 2.8042e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3914e-04, 7.4683e-01, 1.5683e-05, 2.5302e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0777e-04, 7.7702e-01, 1.1301e-05, 2.2286e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[80.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.7048e-05, 8.1092e-01, 8.5128e-06, 1.8898e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.8454e-05, 8.4393e-01, 6.2050e-06, 1.5600e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[80.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.2091e-05, 8.7525e-01, 4.3521e-06, 1.2469e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.6229e-05, 9.0350e-01, 2.7657e-06, 9.6461e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 80.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.4670e-05, 9.2545e-01, 1.7206e-06, 7.4529e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.7829e-05, 6.5244e-01, 7.4500e-06, 3.4751e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.9313e-05, 6.6274e-01, 9.1571e-06, 3.3719e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.3591e-05, 6.7297e-01, 1.1258e-05, 3.2695e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1262e-05, 6.8303e-01, 1.3836e-05, 3.1687e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1272e-04, 6.9348e-01, 1.6860e-05, 3.0639e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3494e-04, 7.0393e-01, 1.9812e-05, 2.9591e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5713e-04, 7.1466e-01, 2.2593e-05, 2.8516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7890e-04, 7.2358e-01, 2.5171e-05, 2.7622e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8847e-04, 7.2845e-01, 2.5893e-05, 2.7134e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7779e-04, 7.2618e-01, 2.3802e-05, 2.7362e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7242e-04, 7.2931e-01, 2.2393e-05, 2.7049e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6829e-04, 7.3409e-01, 2.1203e-05, 2.6572e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6289e-04, 7.4159e-01, 1.9799e-05, 2.5823e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5049e-04, 7.5284e-01, 1.7539e-05, 2.4700e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.3423e-04, 7.6997e-01, 1.4854e-05, 2.2988e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1068e-04, 7.8828e-01, 1.1557e-05, 2.1160e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.7048e-05, 8.1092e-01, 8.5128e-06, 1.8898e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.6643e-05, 8.3535e-01, 6.0610e-06, 1.6457e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[85.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3301e-05, 8.6179e-01, 4.5226e-06, 1.3815e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[85.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.1543e-05, 8.8701e-01, 3.2689e-06, 1.1295e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[3.1297e-05, 9.1184e-01, 2.2639e-06, 8.8131e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 85.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.1871e-05, 9.3129e-01, 1.4510e-06, 6.8683e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[90.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.0394e-05, 7.2101e-01, 4.2385e-06, 2.7895e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.7572e-05, 7.3012e-01, 5.1932e-06, 2.6984e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.6473e-05, 7.3911e-01, 6.3653e-06, 2.6084e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7460e-05, 7.4790e-01, 7.7988e-06, 2.5204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0811e-05, 7.5684e-01, 9.4936e-06, 2.4308e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.5589e-05, 7.6563e-01, 1.1277e-05, 2.3428e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.9355e-05, 7.7484e-01, 1.2821e-05, 2.2505e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1422e-04, 7.8324e-01, 1.4432e-05, 2.1663e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2522e-04, 7.8919e-01, 1.5457e-05, 2.1067e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2192e-04, 7.8908e-01, 1.4681e-05, 2.1079e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1618e-04, 7.8866e-01, 1.3605e-05, 2.1121e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1320e-04, 7.9258e-01, 1.2858e-05, 2.0729e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1040e-04, 7.9703e-01, 1.2149e-05, 2.0285e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0467e-04, 8.0454e-01, 1.1085e-05, 1.9535e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[9.6042e-05, 8.1438e-01, 9.7401e-06, 1.8551e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.4637e-05, 8.2895e-01, 8.1396e-06, 1.7096e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.8454e-05, 8.4393e-01, 6.2050e-06, 1.5600e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3301e-05, 8.6179e-01, 4.5226e-06, 1.3815e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.0417e-05, 8.8077e-01, 3.1880e-06, 1.1919e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[90.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2096e-05, 9.0065e-01, 2.3628e-06, 9.9320e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[2.4756e-05, 9.2082e-01, 1.6855e-06, 7.9156e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 90.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.8618e-05, 9.3740e-01, 1.1694e-06, 6.2583e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[95.0000,  0.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8922e-05, 7.8060e-01, 2.3624e-06, 2.1938e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000,  5.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.3326e-05, 7.8833e-01, 2.8865e-06, 2.1164e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 10.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.8776e-05, 7.9593e-01, 3.5287e-06, 2.0404e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 15.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.5487e-05, 8.0331e-01, 4.3122e-06, 1.9665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 20.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.3660e-05, 8.1067e-01, 5.2465e-06, 1.8928e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 25.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3327e-05, 8.1822e-01, 6.3063e-06, 1.8172e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 30.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.1727e-05, 8.2541e-01, 7.1483e-06, 1.7452e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 35.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.1425e-05, 8.3279e-01, 8.1008e-06, 1.6713e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 40.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.0157e-05, 8.3848e-01, 8.8946e-06, 1.6143e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 45.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[8.1972e-05, 8.4070e-01, 8.8781e-06, 1.5921e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 50.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.7427e-05, 8.3915e-01, 8.1717e-06, 1.6076e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 55.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.4827e-05, 8.4099e-01, 7.6628e-06, 1.5893e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 60.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.2840e-05, 8.4422e-01, 7.2361e-06, 1.5570e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 65.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[7.0985e-05, 8.4821e-01, 6.8224e-06, 1.5172e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 70.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[6.5896e-05, 8.5514e-01, 6.0807e-06, 1.4478e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 75.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.9676e-05, 8.6398e-01, 5.2594e-06, 1.3596e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 80.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[5.2091e-05, 8.7525e-01, 4.3521e-06, 1.2469e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 85.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[4.1543e-05, 8.8701e-01, 3.2689e-06, 1.1295e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 90.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2096e-05, 9.0065e-01, 2.3628e-06, 9.9320e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[95.0000, 95.0000,  0.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4150e-05, 9.1493e-01, 1.6521e-06, 8.5047e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[ 95.0000, 100.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.9061e-05, 9.3061e-01, 1.2146e-06, 6.9373e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[ 95.0000, 105.0000,   0.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.4648e-05, 9.4406e-01, 8.6572e-07, 5.5923e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[100.0000,   0.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.1637e-05, 8.3398e-01, 1.2966e-06, 1.6601e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,   5.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.4297e-05, 8.4016e-01, 1.5793e-06, 1.5982e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  10.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.7596e-05, 8.4626e-01, 1.9261e-06, 1.5372e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  15.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.1650e-05, 8.5216e-01, 2.3484e-06, 1.4781e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  20.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6541e-05, 8.5772e-01, 2.8517e-06, 1.4225e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  25.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.2467e-05, 8.6378e-01, 3.4341e-06, 1.3618e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  30.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.8424e-05, 8.6942e-01, 3.9880e-06, 1.3054e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  35.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.4365e-05, 8.7522e-01, 4.5095e-06, 1.2473e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  40.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[5.0181e-05, 8.7995e-01, 4.9893e-06, 1.2000e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  45.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[5.3113e-05, 8.8325e-01, 5.1493e-06, 1.1669e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  50.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[5.0675e-05, 8.8270e-01, 4.7899e-06, 1.1724e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  55.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.8301e-05, 8.8244e-01, 4.4398e-06, 1.1751e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  60.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.6907e-05, 8.8466e-01, 4.1818e-06, 1.1529e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  65.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.5613e-05, 8.8712e-01, 3.9448e-06, 1.1283e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  70.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.4054e-05, 8.9070e-01, 3.6787e-06, 1.0925e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  75.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[4.0348e-05, 8.9637e-01, 3.2304e-06, 1.0359e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  80.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.6229e-05, 9.0350e-01, 2.7657e-06, 9.6461e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  85.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.1297e-05, 9.1184e-01, 2.2639e-06, 8.8131e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  90.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.4756e-05, 9.2082e-01, 1.6855e-06, 7.9156e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000,  95.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.9061e-05, 9.3061e-01, 1.2146e-06, 6.9373e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000, 100.0000,   1.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.4014e-05, 9.4203e-01, 8.2261e-07, 5.7959e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[100.0000, 105.0000,   1.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.1043e-05, 9.5216e-01, 6.0537e-07, 4.7832e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 0.0\n",
            "state: tensor([[105.0000,   0.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[7.0100e-06, 8.7366e-01, 6.9926e-07, 1.2633e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,   5.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[8.5966e-06, 8.7858e-01, 8.5017e-07, 1.2141e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  10.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.0562e-05, 8.8342e-01, 1.0351e-06, 1.1657e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  15.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.2973e-05, 8.8809e-01, 1.2599e-06, 1.1190e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  20.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.5915e-05, 8.9254e-01, 1.5315e-06, 1.0744e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  25.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.9411e-05, 8.9716e-01, 1.8402e-06, 1.0282e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  30.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.3217e-05, 9.0147e-01, 2.1625e-06, 9.8503e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  35.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6784e-05, 9.0598e-01, 2.4432e-06, 9.3990e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  40.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.0610e-05, 9.1005e-01, 2.7331e-06, 8.9915e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  45.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.3523e-05, 9.1297e-01, 2.9247e-06, 8.6995e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  50.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.3113e-05, 9.1393e-01, 2.8148e-06, 8.6031e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  55.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.1301e-05, 9.1300e-01, 2.5932e-06, 8.6965e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  60.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[3.0189e-05, 9.1391e-01, 2.4264e-06, 8.6058e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  65.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.9335e-05, 9.1580e-01, 2.2873e-06, 8.4164e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  70.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.8512e-05, 9.1782e-01, 2.1546e-06, 8.2147e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  75.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.6986e-05, 9.2119e-01, 1.9634e-06, 7.8785e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  80.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.4670e-05, 9.2545e-01, 1.7206e-06, 7.4529e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  85.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[2.1871e-05, 9.3129e-01, 1.4510e-06, 6.8683e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  90.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.8618e-05, 9.3740e-01, 1.1694e-06, 6.2583e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000,  95.0000,   1.0000,   0.0000,   1.8000]]) dist_dsc.probs: tensor([[1.4648e-05, 9.4406e-01, 8.6572e-07, 5.5923e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000, 100.0000,   1.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[1.1043e-05, 9.5216e-01, 6.0537e-07, 4.7832e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n",
            "state: tensor([[105.0000, 105.0000,   1.0000,   1.0000,   1.8000]]) dist_dsc.probs: tensor([[8.2102e-06, 9.5944e-01, 4.1797e-07, 4.0548e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "1 -0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 0., 0., 1.]]) dist_dsc.probs: tensor([[0.0066, 0.0090, 0.0065, 0.9779]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0159, 0.0038, 0.9757]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0299, 0.0019, 0.9652]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8006e-03, 5.5243e-02, 9.4137e-04, 9.4202e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0748e-03, 9.9681e-02, 4.5083e-04, 8.9879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.1811e-04, 1.7330e-01, 2.0796e-04, 8.2587e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3543e-04, 2.8441e-01, 9.0455e-05, 7.1516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6757e-04, 4.2969e-01, 3.6219e-05, 5.7010e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5846e-05, 5.8816e-01, 1.3139e-05, 4.1175e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1137e-05, 7.3023e-01, 4.3235e-06, 2.6973e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[  0., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.1860e-05, 8.4029e-01, 1.3158e-06, 1.5970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[  0., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.2417e-06, 9.0886e-01, 3.7718e-07, 9.1137e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0047, 0.0159, 0.0038, 0.9757]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0057, 0.0181, 0.0044, 0.9718]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0044, 0.0325, 0.0028, 0.9603]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0604, 0.0015, 0.9353]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7449e-03, 1.0827e-01, 7.1784e-04, 8.8926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.9207e-04, 1.8669e-01, 3.2783e-04, 8.1199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3349e-04, 3.0338e-01, 1.4132e-04, 6.9595e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6294e-04, 4.5225e-01, 5.5826e-05, 5.4743e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1729e-04, 6.1015e-01, 1.9959e-05, 3.8972e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7531e-05, 7.4788e-01, 6.4834e-06, 2.5206e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 10., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.7909e-05, 8.5215e-01, 1.9523e-06, 1.4783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 10., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[6.3656e-06, 9.1613e-01, 5.5621e-07, 8.3861e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0299, 0.0019, 0.9652]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0044, 0.0325, 0.0028, 0.9603]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0040, 0.0378, 0.0023, 0.9559]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0634, 0.0016, 0.9318]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.1164, 0.0009, 0.8803]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4645e-03, 2.0226e-01, 4.6543e-04, 7.9581e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0796e-04, 3.2481e-01, 2.0702e-04, 6.7418e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0273e-04, 4.7600e-01, 8.3106e-05, 5.2352e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8047e-04, 6.3277e-01, 2.9966e-05, 3.6702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2324e-05, 7.6533e-01, 9.6477e-06, 2.3459e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 20., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.6982e-05, 8.6332e-01, 2.8846e-06, 1.3665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 20., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[9.5707e-06, 9.2294e-01, 8.2117e-07, 7.7050e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8006e-03, 5.5243e-02, 9.4137e-04, 9.4202e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0029, 0.0604, 0.0015, 0.9353]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0032, 0.0634, 0.0016, 0.9318]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0027, 0.0768, 0.0012, 0.9193]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2096e-03, 1.2165e-01, 8.1382e-04, 8.7532e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4906e-03, 2.0719e-01, 4.4881e-04, 7.9087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5575e-04, 3.3815e-01, 2.3353e-04, 6.6066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3454e-04, 4.9893e-01, 1.0562e-04, 5.0043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4804e-04, 6.5552e-01, 3.9546e-05, 3.4419e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0073e-04, 7.8225e-01, 1.2931e-05, 2.1764e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 30., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.8806e-05, 8.7423e-01, 4.0071e-06, 1.2573e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 30., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.4138e-05, 9.2966e-01, 1.1762e-06, 7.0326e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0748e-03, 9.9681e-02, 4.5083e-04, 8.9879e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7449e-03, 1.0827e-01, 7.1784e-04, 8.8926e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[0.0023, 0.1164, 0.0009, 0.8803]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2096e-03, 1.2165e-01, 8.1382e-04, 8.7532e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7465e-03, 1.5016e-01, 5.7055e-04, 8.4753e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4083e-03, 2.2314e-01, 3.9301e-04, 7.7506e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.0589e-04, 3.4499e-01, 2.0850e-04, 6.5390e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1151e-04, 5.0195e-01, 9.5752e-05, 4.9745e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6665e-04, 6.6423e-01, 4.0490e-05, 3.3547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2471e-04, 7.9576e-01, 1.5314e-05, 2.0410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 40., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[5.0636e-05, 8.8453e-01, 5.0113e-06, 1.1542e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 40., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.8582e-05, 9.3573e-01, 1.4822e-06, 6.4249e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.1811e-04, 1.7330e-01, 2.0796e-04, 8.2587e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.9207e-04, 1.8669e-01, 3.2783e-04, 8.1199e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4645e-03, 2.0226e-01, 4.6543e-04, 7.9581e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4906e-03, 2.0719e-01, 4.4881e-04, 7.9087e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.4083e-03, 2.2314e-01, 3.9301e-04, 7.7506e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0495e-03, 2.7266e-01, 2.5790e-04, 7.2603e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1386e-04, 3.7592e-01, 1.7152e-04, 6.2309e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8990e-04, 5.1739e-01, 8.5874e-05, 4.8204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4907e-04, 6.6968e-01, 3.5662e-05, 3.3004e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1482e-04, 7.9532e-01, 1.3363e-05, 2.0455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 50., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.9848e-05, 8.8673e-01, 4.6840e-06, 1.1321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 50., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.1012e-05, 9.3979e-01, 1.5949e-06, 6.0184e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.3543e-04, 2.8441e-01, 9.0455e-05, 7.1516e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3349e-04, 3.0338e-01, 1.4132e-04, 6.9595e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0796e-04, 3.2481e-01, 2.0702e-04, 6.7418e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5575e-04, 3.3815e-01, 2.3353e-04, 6.6066e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.0589e-04, 3.4499e-01, 2.0850e-04, 6.5390e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[8.1386e-04, 3.7592e-01, 1.7152e-04, 6.2309e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.6421e-04, 4.4293e-01, 1.0429e-04, 5.5640e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0636e-04, 5.5813e-01, 6.4592e-05, 4.4140e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2856e-04, 6.8805e-01, 3.0415e-05, 3.1169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0905e-04, 8.0337e-01, 1.1934e-05, 1.9651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 60., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.6185e-05, 8.8874e-01, 4.0923e-06, 1.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 60., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.8960e-05, 9.3886e-01, 1.3659e-06, 6.1118e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.6757e-04, 4.2969e-01, 3.6219e-05, 5.7010e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6294e-04, 4.5225e-01, 5.5826e-05, 5.4743e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0273e-04, 4.7600e-01, 8.3106e-05, 5.2352e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.3454e-04, 4.9893e-01, 1.0562e-04, 5.0043e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[5.1151e-04, 5.0195e-01, 9.5752e-05, 4.9745e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.8990e-04, 5.1739e-01, 8.5874e-05, 4.8204e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0636e-04, 5.5813e-01, 6.4592e-05, 4.4140e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6461e-04, 6.2771e-01, 3.6790e-05, 3.7198e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7691e-04, 7.2600e-01, 2.1201e-05, 2.7380e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5475e-05, 8.1987e-01, 9.6363e-06, 1.8002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 70., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[4.3538e-05, 8.9434e-01, 3.6169e-06, 1.0561e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 70., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.7855e-05, 9.4140e-01, 1.2101e-06, 5.8577e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.5846e-05, 5.8816e-01, 1.3139e-05, 4.1175e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1729e-04, 6.1015e-01, 1.9959e-05, 3.8972e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.8047e-04, 6.3277e-01, 2.9966e-05, 3.6702e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4804e-04, 6.5552e-01, 3.9546e-05, 3.4419e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.6665e-04, 6.6423e-01, 4.0490e-05, 3.3547e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.4907e-04, 6.6968e-01, 3.5662e-05, 3.3004e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[2.2856e-04, 6.8805e-01, 3.0415e-05, 3.1169e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.7691e-04, 7.2600e-01, 2.1201e-05, 2.7380e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0901e-04, 7.8142e-01, 1.1401e-05, 2.1846e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9061e-05, 8.4765e-01, 6.2396e-06, 1.5227e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 80., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[3.5937e-05, 9.0660e-01, 2.7308e-06, 9.3361e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 80., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.6151e-05, 9.4568e-01, 1.0186e-06, 5.4299e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90.,  0.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[3.1137e-05, 7.3023e-01, 4.3235e-06, 2.6973e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 10.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.7531e-05, 7.4788e-01, 6.4834e-06, 2.5206e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 20.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[7.2324e-05, 7.6533e-01, 9.6477e-06, 2.3459e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 30.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0073e-04, 7.8225e-01, 1.2931e-05, 2.1764e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 40.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.2471e-04, 7.9576e-01, 1.5314e-05, 2.0410e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 50.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.1482e-04, 7.9532e-01, 1.3363e-05, 2.0455e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 60.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[1.0905e-04, 8.0337e-01, 1.1934e-05, 1.9651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 70.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[9.5475e-05, 8.1987e-01, 9.6363e-06, 1.8002e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 80.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[6.9061e-05, 8.4765e-01, 6.2396e-06, 1.5227e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 90.,  0.,  0.,  1.]]) dist_dsc.probs: tensor([[4.0784e-05, 8.8344e-01, 3.2085e-06, 1.1651e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 90., 100.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[2.4933e-05, 9.2288e-01, 1.6922e-06, 7.7090e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 90., 110.,   0.,   1.,   1.]]) dist_dsc.probs: tensor([[1.2960e-05, 9.5315e-01, 7.4375e-07, 4.6838e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,   0.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.1860e-05, 8.4029e-01, 1.3158e-06, 1.5970e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  10.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.7909e-05, 8.5215e-01, 1.9523e-06, 1.4783e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  20.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.6982e-05, 8.6332e-01, 2.8846e-06, 1.3665e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  30.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.8806e-05, 8.7423e-01, 4.0071e-06, 1.2573e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  40.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[5.0636e-05, 8.8453e-01, 5.0113e-06, 1.1542e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  50.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.9848e-05, 8.8673e-01, 4.6840e-06, 1.1321e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  60.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.6185e-05, 8.8874e-01, 4.0923e-06, 1.1120e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  70.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.3538e-05, 8.9434e-01, 3.6169e-06, 1.0561e-01]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  80.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[3.5937e-05, 9.0660e-01, 2.7308e-06, 9.3361e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  90.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.4933e-05, 9.2288e-01, 1.6922e-06, 7.7090e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100., 100.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[1.4134e-05, 9.4345e-01, 8.2740e-07, 5.6532e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100., 110.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[8.5211e-06, 9.6255e-01, 4.3218e-07, 3.7442e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[0.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,   0.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[4.2417e-06, 9.0886e-01, 3.7718e-07, 9.1137e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  10.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[6.3656e-06, 9.1613e-01, 5.5621e-07, 8.3861e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  20.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[9.5707e-06, 9.2294e-01, 8.2117e-07, 7.7050e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  30.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.4138e-05, 9.2966e-01, 1.1762e-06, 7.0326e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  40.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.8582e-05, 9.3573e-01, 1.4822e-06, 6.4249e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  50.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[2.1012e-05, 9.3979e-01, 1.5949e-06, 6.0184e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  60.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.8960e-05, 9.3886e-01, 1.3659e-06, 6.1118e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  70.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.7855e-05, 9.4140e-01, 1.2101e-06, 5.8577e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  80.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.6151e-05, 9.4568e-01, 1.0186e-06, 5.4299e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  90.,   1.,   0.,   1.]]) dist_dsc.probs: tensor([[1.2960e-05, 9.5315e-01, 7.4375e-07, 4.6838e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110., 100.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[8.5211e-06, 9.6255e-01, 4.3218e-07, 3.7442e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110., 110.,   1.,   1.,   1.]]) dist_dsc.probs: tensor([[4.8212e-06, 9.7250e-01, 2.1231e-07, 2.7490e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.]], grad_fn=<NegBackward0>) dist_cnt.scale tensor([[1.]], grad_fn=<ExpBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
            "[0, 1048576]\n"
          ]
        }
      ],
      "source": [
        "def one_hot_encoding(levels, n_units=2,n_states=5, MAX_maintenance_time=0):\n",
        "    level_ohe = []\n",
        "    #mstatus_ohe = []\n",
        "    for unit_idx in range(n_units):\n",
        "        l = [0] * n_states\n",
        "        #m = [0] * (MAX_maintenance_time + 1)\n",
        "        l[levels[unit_idx]] = 1\n",
        "        #m[maintenance_status[unit_idx]] = 1\n",
        "        level_ohe = level_ohe + l\n",
        "        #mstatus_ohe = mstatus_ohe + m\n",
        "    return level_ohe #, mstatus_ohe\n",
        "\n",
        "\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        print(action)\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"lightblue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, act_dsc, act_cnt, load_total):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "    if act_dsc==0:\n",
        "        ax.text(center_x,center_y,f'M12',ha='center', va='center')\n",
        "    elif act_dsc==1:\n",
        "        if center_x<center_y:\n",
        "          ax.text(center_x,center_y,f'M1',ha='center', va='center')\n",
        "        else:\n",
        "          ax.text(center_x,center_y,f'M2',ha='center', va='center')\n",
        "    elif act_dsc==2:\n",
        "        if center_x<center_y:\n",
        "          ax.text(center_x,center_y,f'M2',ha='center', va='center')\n",
        "        else:\n",
        "          ax.text(center_x,center_y,f'M1',ha='center', va='center')\n",
        "    else: #稼働継続\n",
        "        print(act_cnt, type(act_cnt))\n",
        "        act_cnt = act_cnt * 0.5 + 0.5\n",
        "        #ax.text(center_x, center_y,f'{(round(act_cnt[0],2),round(1-act_cnt[0],2))}',ha='center', va='center',fontsize=5)\n",
        "        ax.text(center_x, center_y, f'{(round(act_cnt, 2))}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "def plot_state_value(ax, center_x, center_y, val):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "\n",
        "    ax.text(center_x, center_y, f'{round(float(val), 0)}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 20\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a\n",
        "            x2 =s2*a\n",
        "            flag1=0\n",
        "            flag2=0\n",
        "            if s1>=x:\n",
        "              flag1=1\n",
        "            if s2>=x:\n",
        "              flag2=1\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] +[flag1,flag2] +list([load_total])\n",
        "            #print(state)\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "    # 軸のラベルを設定\n",
        "    ax.set_xticks(range(x+2))  # 目盛りの位置を設定\n",
        "    ax.set_xticklabels([f\"{int(label * a)}\" for label in ax.get_xticks()])  # X倍の値でラベルを整数で更新\n",
        "    ax.set_yticks(range(x+2))\n",
        "    ax.set_yticklabels([f\"{int(label * a)}\" for label in ax.get_yticks()])\n",
        "    plt.show()\n",
        "\n",
        "def state_value(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 10\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a\n",
        "            x2 =s2*a\n",
        "            flag1=0\n",
        "            flag2=0\n",
        "            if s1>=x:\n",
        "              flag1=1\n",
        "            if s2>=x:\n",
        "              flag2=1\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] + [flag1,flag2] + list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_state_value(ax, s1, s2, val)\n",
        "    # 軸の範囲と目盛りを設定\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1.0)\n",
        "optimal_policy(load_total=1.8)\n",
        "\n",
        "state_value(load_total=1)\n",
        "\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=0,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=1,m2=1,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=0.5)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "s1 = 4\n",
        "s2 = 4\n",
        "s3 = 4\n",
        "m1 = 0\n",
        "m2 = 0\n",
        "m3 = 0\n",
        "inventory = 0\n",
        "#demand = 0\n",
        "remain_interval = 1\n",
        "#level_ohe, mstatus_ohe = one_hot_encoding(levels=[s1,s2,s3],maintenance_status=[m1,m2,m3])\n",
        "#state = level_ohe + mstatus_ohe + list([inventory,demand,remain_interval])\n",
        "#act_dsc, act_cnt, log_prob, val = agent.choose_action_max_prob(state)\n",
        "#act_dsc = act_dsc.item()\n",
        "#act_dsc\n",
        "print(env.Visit)\n",
        "print(env.cntCount)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"blue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, size, action):\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        size,\n",
        "        size,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(action)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    if action >= 0:\n",
        "        r = list(map(int, bin(action)[2:].zfill(env.num_targets)))\n",
        "        #ax.text(center_x,center_y,f'({r[0]},{r[1]},{r[2]})',ha='center', va='center')\n",
        "        ax.text(center_x,center_y,f'{action}',ha='center', va='center')\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 26\n",
        "    for s1 in range(x):\n",
        "        for s2 in range(x):\n",
        "            state = [s1,s2] + list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "            plot_action(ax,cs2,cs3,size=1,action=action)\n",
        "    ax.set_xlim(-0.5,x-1+0.5)\n",
        "    ax.set_ylim(-0.5,x-1+0.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s2\")\n",
        "    ax.set_ylabel(\"s3\")\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"(s1=\"+str(cs1)+\", s2, s3, z1=\"+str(cz1)+\", z2=\"+str(cz2)+\", z3=\"+str(cz3)+\")\")\n",
        "    plt.show()\n",
        "\n",
        "optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1)\n",
        "optimal_policy(load_total=1.8)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qBPaE5n89aZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9357d832-bd8b-456d-daa9-da799f14f7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef get_color(action):\\n    if action < 0:\\n        return \"white\"\\n    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"blue\"]\\n    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\\n    return cmap[action]\\n\\ndef plot_action(ax, center_x, center_y, size, action):\\n    opt_action = patches.Rectangle(\\n        (center_x-size/2, center_y-size/2),\\n        size,\\n        size,\\n        linewidth = 0,\\n        facecolor = get_color(action)\\n    )\\n    ax.add_patch(opt_action)\\n    if action >= 0:\\n        r = list(map(int, bin(action)[2:].zfill(env.num_targets)))\\n        #ax.text(center_x,center_y,f\\'({r[0]},{r[1]},{r[2]})\\',ha=\\'center\\', va=\\'center\\')\\n        ax.text(center_x,center_y,f\\'{action}\\',ha=\\'center\\', va=\\'center\\')\\n\\ndef optimal_policy(load_total):\\n    fig, ax = plt.subplots()\\n    x = 26\\n    for s1 in range(x):\\n        for s2 in range(x):\\n            state = [s1,s2] + list([load_total])\\n            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\\n            act_dsc = act_dsc.item()\\n            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\\n            #print(act_dsc,act_cnt)\\n            print(\"val:\",val)\\n            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\\n            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\\n            plot_action(ax,cs2,cs3,size=1,action=action)\\n    ax.set_xlim(-0.5,x-1+0.5)\\n    ax.set_ylim(-0.5,x-1+0.5)\\n    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\\n    #cbar.ax.set_yticklabels([f\\'{i:b}\\' for i in range(8)])\\n    ax.set_aspect(\\'equal\\', adjustable=\\'box\\')  # アスペクト比を保持\\n    ax.set_xlabel(\"s2\")\\n    ax.set_ylabel(\"s3\")\\n    # グラフを表示\\n    ax.set_title(\"(s1=\"+str(cs1)+\", s2, s3, z1=\"+str(cz1)+\", z2=\"+str(cz2)+\", z3=\"+str(cz3)+\")\")\\n    plt.show()\\n\\noptimal_policy(load_total=0.2)\\noptimal_policy(load_total=1)\\noptimal_policy(load_total=1.8)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "DRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}