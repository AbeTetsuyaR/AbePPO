{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeTetsuyaR/AbePPO/blob/main/AbePPO0904_without_ohe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCrwylLHTYPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import cProfile\n",
        "import sys\n",
        "import copy\n",
        "from torch.distributions.categorical import Categorical\n",
        "import math\n",
        "import os\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.colors as mcolors\n",
        "from tqdm import tqdm  # tqdmをインポート\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import gamma, uniform, truncnorm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6hr_FE6TYPY"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, n_units=2):\n",
        "        self.n_units = n_units #number of unit\n",
        "        #self.n_states = 5 #number of state\n",
        "        self.inventory = 0\n",
        "        self.demand = 0\n",
        "        self.maintenance_status = [0] * self.n_units\n",
        "        self.interval = 24\n",
        "        self.remain_interval = 24\n",
        "        self.MAX_speed = 10/self.interval\n",
        "        self.MAX_inventory = 0\n",
        "        self.MAX_demand = 15\n",
        "        self.MAX_maintenance_time = 0\n",
        "\n",
        "        self.load_total=1\n",
        "\n",
        "        self.cp = 500#\n",
        "        self.cc = 1800#\n",
        "\n",
        "        self.cps = 0\n",
        "        self.co = 5\n",
        "        self.cs = 500#\n",
        "\n",
        "        self.levels = [0] * self.n_units\n",
        "        self.shape = 3\n",
        "        self.penalty = 1\n",
        "        self.L = 100#\n",
        "        #self.P_Cost =[[100,110,130,160,2540],\n",
        "                      #[110,120,140,170,2550],\n",
        "                      #[130,140,160,190,2570],\n",
        "                      #[160,170,190,220,2600],\n",
        "                      #[2540,2550,2570,2600,5000]]#convex化\n",
        "\n",
        "        self.P_Cost =[[0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [0,0,0,0,2500],\n",
        "                      [2500,2500,2500,2500,5000]]#平滑化\n",
        "        self.P_Cost_L = 2500\n",
        "\n",
        "        self.Visit =[[0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0],\n",
        "                      [0,0,0,0,0]]\n",
        "        self.cntCount=[0,0]\n",
        "\n",
        "        self.failure_keep1 = 0 #1つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep2 = 0 #2つ故障しているのに保全を選択しなかった回数\n",
        "        self.failure_keep3 = 0 #3つ故障しているのに保全を選択しなかった回数\n",
        "        self.replace_chance = 0 #保全を選択できた回数\n",
        "\n",
        "    def init_random(self):\n",
        "        levels=[0,0]\n",
        "        self.load_total=1\n",
        "        return levels, self.load_total\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.levels = np.zeros(self.n_units)\n",
        "\n",
        "    def complete_maintenance(self, unit_idx):\n",
        "        self.levels[unit_idx] = 0\n",
        "\n",
        "    def get_ability(self, level): #良品率\n",
        "        if level == 0:\n",
        "            return 1\n",
        "        elif level == 1:\n",
        "            return 0.8\n",
        "        elif level ==2:\n",
        "            return 0.5\n",
        "        elif level == 3:\n",
        "            return 0.1\n",
        "        return (self.n_states - 1 - level) / (self.n_states - 1)\n",
        "\n",
        "    def update_demand(self, speed, ability, time):\n",
        "        if self.demand >= self.inventory:\n",
        "            self.demand -= self.inventory\n",
        "            self.inventory = 0.0\n",
        "        else:\n",
        "            self.inventory -= self.demand\n",
        "            self.demand = 0.0\n",
        "        return max(0, self.demand-self.inventory-ability*speed*time)\n",
        "\n",
        "    def update_inventory(self, speed, ability, time):\n",
        "        if self.demand <= self.inventory + ability * speed * time:\n",
        "            return min(self.MAX_inventory, -self.demand+self.inventory+ability*speed*time), max(0, -self.MAX_inventory-self.demand+self.inventory+ability*speed*time)\n",
        "        else:\n",
        "            return 0.0, 0.0\n",
        "\n",
        "    def get_maintenance_time(self,level):\n",
        "        return 0\n",
        "\n",
        "    def update_maintenance_time(self, unit_idx):\n",
        "        return 0\n",
        "\n",
        "    def one_hot_encode(self):\n",
        "        level_ohe = []\n",
        "        #mstatus_ohe = []\n",
        "        for unit_idx in range(self.n_units):\n",
        "            l = [0] * self.n_states\n",
        "            #m = [0] * (self.MAX_maintenance_time + 1)\n",
        "            l[min(math.floor(self.levels[unit_idx]),self.n_states-1)] = 1\n",
        "            #m[self.maintenance_status[unit_idx]] = 1\n",
        "            level_ohe = level_ohe + l\n",
        "            #mstatus_ohe = mstatus_ohe + m\n",
        "        return level_ohe #mstatus_oheは削除\n",
        "\n",
        "\n",
        "\n",
        "    def operation(self, replacements, load_rate):\n",
        "        reward = 0\n",
        "        #print(load_rate)\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        #保全の意思決定\n",
        "        #print(replacements)\n",
        "\n",
        "        if replacements==[1,1]: #稼働継続\n",
        "\n",
        "          #reward -= self.P_Cost[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] #rewardを最初に計算\n",
        "          if self.levels[0]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          if self.levels[1]>self.L:\n",
        "            reward -= self.P_Cost_L\n",
        "          #print(reward)\n",
        "          # パラメータの設定\n",
        "          scales=[0,0]\n",
        "          shape0 = 0.69  # ガンマ分布のパラメータ v1 用 0.69 100倍にしてみる\n",
        "          shape1 = 0.69   # ガンマ分布のパラメータ v2 用\n",
        "          tau = 0.5  # ケンドールの順位相関係数\n",
        "\n",
        "          theta = 1 / (1 - tau)\n",
        "\n",
        "          #if load_rate<0:##\n",
        "            #load_rate=0\n",
        "          #if load_rate>1:\n",
        "            #load_rate=1\n",
        "          loads=[0,0]\n",
        "          loads[0]=load_total*load_rate\n",
        "          loads[1]=load_total*(1-load_rate)\n",
        "\n",
        "          #print(speeds, \"speeds\")\n",
        "          #尺度パラメータ計算\n",
        "          for i in range(self.n_units):\n",
        "            scales[i]=6.491*(loads[i]**2)+0.726\n",
        "            #scales[i]=6.491*(speeds[i]**0.5)+0.726\n",
        "            #scales[i]=scales[i] #1/10000倍にしてみる\n",
        "\n",
        "\n",
        "          # 一様分布から独立にサンプリング\n",
        "          u = uniform.rvs(size=1)\n",
        "          v = uniform.rvs(size=1)\n",
        "          #print(\"u,v:\",u,v)\n",
        "          # ガンベルコピュラの逆関数を適用\n",
        "          x = (-np.log(u)) ** theta\n",
        "          y = (-np.log(v)) ** theta\n",
        "          #print(\"x,y:\",x,y)\n",
        "\n",
        "          t = (x + y) ** (1/theta)\n",
        "          #print(\"t:\",t)\n",
        "\n",
        "          u_new = np.exp(-t * (x / (x + y)))\n",
        "          v_new = np.exp(-t * (y / (x + y)))\n",
        "\n",
        "          # ガンマ分布に変換\n",
        "          v1 = gamma.ppf(u_new, shape0, scale=scales[0])\n",
        "          v2 = gamma.ppf(v_new, shape1, scale=scales[1])\n",
        "          #print(v1,v2)\n",
        "          #v1 = gamma.rvs(shape0, scale=scales[0])\n",
        "          #v2 = gamma.rvs(shape1, scale=scales[1])\n",
        "\n",
        "          #print(\"稼働継続\")\n",
        "          #print(v1,v2, \"劣化増分\")\n",
        "          self.levels[0]+=v1\n",
        "          self.levels[1]+=v2\n",
        "          #print(self.levels, \"劣化\")\n",
        "\n",
        "\n",
        "        elif replacements==[0,1]: #1のみ取替\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[0]=0\n",
        "\n",
        "        elif replacements==[1,0]: #2のみ取替\n",
        "          reward -= self.cs\n",
        "          if self.levels[1]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels[1]=0\n",
        "        else: #両方取替\n",
        "          reward -= self.cs\n",
        "          if self.levels[0]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          if self.levels[1]<self.L:\n",
        "            reward -= self.cp\n",
        "          else:\n",
        "            reward -= self.cc\n",
        "          self.levels=[0,0]\n",
        "\n",
        "        #print(self.levels)\n",
        "        #print(reward)\n",
        "\n",
        "        #level_ohe= self.one_hot_encode()\n",
        "\n",
        "        #print(f'状態:{self.levels}, 保全状態:{self.maintenance_status}, 在庫:{self.inventory}, 需要:{self.demand}, 残り時間:{self.remain_interval}, 保全行動:{replacements}, {speeds}')\n",
        "        #print(\"#############\")\n",
        "\n",
        "        flag = 0\n",
        "\n",
        "        #切断正規分布\n",
        "        dist_N = truncnorm(0, 2, loc=1, scale=1)\n",
        "        self.load_total=float(dist_N.rvs(1))\n",
        "        #print(self.load_total)\n",
        "\n",
        "        #self.Visit[min(math.floor(self.levels[0]),self.n_states-1)][min(math.floor(self.levels[1]),self.n_states-1)] += 1\n",
        "\n",
        "        return reward, levels, self.load_total\n",
        "        #return reward, level_ohe, mstatus_ohe, \\\n",
        "        #       0, (self.demand-mean)/variance, self.remain_interval * 2 / self.interval - 1, flag\n",
        "\n",
        "\n",
        "    #劣化レベル順にすべき可能性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_7ruy0iTYPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3d5f71b9-de58-4ff2-bf80-935b15bd1336"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    def generate_advantage(self):\\n\\n        advantage = []\\n        gae = 0\\n        for t in reversed(range(len(self.rewards))):\\n            gamma = 1\\n            if t == len(self.rewards) - 1:\\n                delta = self.rewards[t] - self.vals[t]\\n            else:\\n                gamma = math.exp(-self.beta*1) #修正\\n                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\\n            gae = delta + gamma * gamma * gae\\n            advantage.insert(0, gae)\\n        self.advantage = np.array(advantage,dtype=np.float32)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self,batch_size, interval, beta, GAE_lam):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.vals = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.time = []\n",
        "        self.batch_size = batch_size\n",
        "        self.interval = interval\n",
        "        self.beta = beta\n",
        "        self.advantage = []\n",
        "        self.lam = GAE_lam\n",
        "\n",
        "\n",
        "\n",
        "    def generate_advantage(self):\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        gamma = math.exp(-self.beta)\n",
        "        lambd = 0.0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + lambd * gamma * gae  # Assuming lambda = 1 for simplicity, otherwise use gamma * lambda * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage, dtype=np.float32)\n",
        "\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "               np.array(self.acts_dsc),\\\n",
        "               np.array(self.acts_cnt),\\\n",
        "               np.array(self.probs_dsc),\\\n",
        "               np.array(self.probs_cnt),\\\n",
        "               np.array(self.vals),\\\n",
        "               np.array(self.rewards),\\\n",
        "               np.array(self.advantage),\\\n",
        "               batches\n",
        "\n",
        "\n",
        "\n",
        "    def store_memory(self, state, act_dsc, act_cnt, probs_dsc, probs_cnt, vals, reward, time):\n",
        "        self.states.append(state)\n",
        "        self.acts_dsc.append(act_dsc)\n",
        "        self.acts_cnt.append(act_cnt)\n",
        "        self.probs_dsc.append(probs_dsc)\n",
        "        self.probs_cnt.append(probs_cnt)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.time.append(time)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs_dsc = []\n",
        "        self.probs_cnt = []\n",
        "        self.acts_dsc = []\n",
        "        self.acts_cnt = []\n",
        "        self.rewards = []\n",
        "        self.vals = []\n",
        "        self.time = []\n",
        "\n",
        "\"\"\"\n",
        "    def generate_advantage(self):\n",
        "\n",
        "        advantage = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            gamma = 1\n",
        "            if t == len(self.rewards) - 1:\n",
        "                delta = self.rewards[t] - self.vals[t]\n",
        "            else:\n",
        "                gamma = math.exp(-self.beta*1) #修正\n",
        "                delta = self.rewards[t] + gamma * self.vals[t+1] - self.vals[t]\n",
        "            gae = delta + gamma * gamma * gae\n",
        "            advantage.insert(0, gae)\n",
        "        self.advantage = np.array(advantage,dtype=np.float32)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJC8xTojTYPa"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, alpha, fc1_dims=64, fc2_dims=64, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.n_units = n_units\n",
        "        self.n_states = n_states\n",
        "        self.MAX_maintenance_time = MAX_maintenance_time\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"actor_torch_ppo\")\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.fc3 = nn.Linear(fc1_dims, fc3_dims)\n",
        "\n",
        "        self.dsc = nn.Linear(fc2_dims, 2 ** n_units) #離散行動\n",
        "        # 以下に初期化コードを追加\n",
        "        self.init_dsc_weights()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.Tanh = nn.Tanh()\n",
        "\n",
        "        self.mean = nn.Linear(fc3_dims, n_units-1)\n",
        "        self.log_std = nn.Linear(fc3_dims, n_units-1)\n",
        "\n",
        "        # mean レイヤーの初期化\n",
        "        #self.init_mean_weights()\n",
        "        # std レイヤーの初期化\n",
        "        #self.init_std_weights()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "\n",
        "        #0824 optimizer追加\n",
        "        self.optimizer_discrete = optim.Adam([\n",
        "            {'params':self.fc1.parameters()},\n",
        "            {'params':self.fc2.parameters()},\n",
        "            {'params':self.dsc.parameters()},\n",
        "        ], lr=alpha)\n",
        "\n",
        "        self.optimizer_continuous = optim.Adam([\n",
        "            {'params':self.fc1.parameters()},\n",
        "            {'params':self.fc3.parameters()},\n",
        "            {'params':self.mean.parameters()},\n",
        "            {'params':self.log_std.parameters()},\n",
        "        ], lr=alpha)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "    def init_dsc_weights(self):\n",
        "        # ここで特定の出力確率を設定するための重みとバイアスを設定\n",
        "        with torch.no_grad():\n",
        "            # すべての出力がほぼ等しくなるように設定\n",
        "            self.dsc.weight.fill_(0.0)\n",
        "            # 特定の確率分布に調整\n",
        "            self.dsc.bias.data = torch.log(torch.tensor([0.004, 0.003, 0.003, 0.99]))  # logを取るのがポイント\n",
        "\n",
        "    def init_mean_weights(self):\n",
        "        # mean レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.mean.weight.fill_(0.0)\n",
        "            self.mean.bias.fill_(0.0)  # ここで固定出力0.5を設定#0に変更\n",
        "\n",
        "    def init_std_weights(self):\n",
        "        # std レイヤーの初期化\n",
        "        with torch.no_grad():\n",
        "            self.log_std.weight.fill_(0.0)\n",
        "            self.log_std.bias.fill_(1.0)  # ここで固定出力0.7を設定\n",
        "\n",
        "    #離散行動空間を制限するための関数, 返り値はmaskで制限されるところは-inf,されないところは1。返り値はバッチ数*2\n",
        "    def create_dsc_mask(self, state): #state=[s,m,b,d,t], action=[P(replace), P(keep)]\n",
        "        mask = torch.zeros(state.size(0),2**self.n_units)\n",
        "        #保全を選択できる時点にて、保全中のユニットは保全を選択できない\n",
        "        for unit_idx in range(self.n_units):\n",
        "            for a in range(2 ** self.n_units):\n",
        "                action_list = [int(bit) for bit in format(a, f'0{self.n_units}b')] #action_list=[r1,r2,r3,...]\n",
        "\n",
        "                #エラーのため省略\n",
        "                #if action_list[unit_idx] == 0:\n",
        "                    #保全の意思決定時点のとき、保全中の場合は保全を選択できない\n",
        "                    #保全の意思決定時点でないとき、保全を選択できない\n",
        "                    #mask[(state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)\\\n",
        "                        #, a] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        mask[(state[:, 4] == 1) & (state[:, 9] == 1) & \\\n",
        "             (state[:,self.n_states*self.n_units-1 + 1] == 1)\n",
        "            ,1:] = torch.tensor(1) #2 ** self.n_units-1\n",
        "            #状態とユニット数により要変更\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "    def create_cnt_mask(self, state):\n",
        "        mask= torch.zeros(state.size(0), self.n_units)\n",
        "        for unit_idx in range(self.n_units):\n",
        "            a = 2**(self.n_units)-1 - 2**(self.n_units-1-unit_idx)\n",
        "\n",
        "            #エラーのため省略\n",
        "            #保全の意思決定ができる時\n",
        "            mask[(state[:,-1] == 1) & \\\n",
        "                 ((dist_dsc[:,a] <= 0.0001) |\n",
        "                  (state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "                  (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "            mask[((state[:,unit_idx*self.n_states+self.n_states-1] == 1) | \\\n",
        "                  (state[:,(self.n_states*self.n_units-1+(self.MAX_maintenance_time+1)*unit_idx+1)] == 0)), unit_idx] = torch.tensor(1)\n",
        "\n",
        "\n",
        "        return mask.bool()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        #print(state[0,-1].item())\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x2 = F.relu(self.fc2(x))\n",
        "        x3 = F.relu(self.fc3(x))\n",
        "\n",
        "        #first_mask = self.create_dsc_mask(state)\n",
        "\n",
        "        #離散行動の分布\n",
        "        dist_dsc = self.dsc(x2)\n",
        "        #dist_dsc = dist_dsc.masked_fill(first_mask,-1e5)\n",
        "\n",
        "        dist_dsc = self.softmax(dist_dsc)\n",
        "        #if (state[0,4]==1 and state[0,9]==1 and state[0,14]==1 and state[0,15]==1 and state[0,19]==1 and state[0,23]==1 and state[0,-1]==1):\n",
        "        #print(state)\n",
        "        #    print(dist_dsc)\n",
        "        #    print(\"&&&&&&&&\")\n",
        "\n",
        "\n",
        "        #second_mask = self.create_cnt_mask(state)\n",
        "        dist_dsc = Categorical(dist_dsc)\n",
        "\n",
        "        #連続行動の分布\n",
        "        mean = self.mean(x3)\n",
        "        #mean = self.softmax(mean)\n",
        "        #mean = self.Tanh(mean)/2+0.5#[0,1]に補正\n",
        "        mean = self.Tanh(mean)\n",
        "\n",
        "        #load_max=min(state[0,-1].item(),1) #operationへ移動\n",
        "        #load_min=max(state[0,-1].item() -1,0)\n",
        "        #mean=load_min + (load_max-load_min)*mean\n",
        "\n",
        "        #mean = mean.masked_fill(second_mask, -1) #セカンドマスク\n",
        "\n",
        "        #print(dist_dsc, mean)\n",
        "        #print(state)\n",
        "        #print(dist_dsc)\n",
        "        #mean = torch.clamp(mean,min=-5,max=5)\n",
        "        log_std = self.log_std(x3)\n",
        "        log_std = torch.clamp(log_std,min=-20,max=2)\n",
        "        std = log_std.exp()\n",
        "        #std = std.masked_fill(second_mask, 1e-4) #セカンドマスク\n",
        "        #print(mean)\n",
        "        #print(\"AAAA\")\n",
        "\n",
        "        #print(mean,std)\n",
        "\n",
        "        #dist_cnt = torch.distributions.MultivariateNormal(loc=mean, covariance_matrix = torch.stack([torch.diag(x**2+1e-10) for x in std]))\n",
        "        #mean=state[0,-1].item()/2#試験\n",
        "\n",
        "        variance = std ** 2\n",
        "        #print(variance)\n",
        "        #if variance > mean*(1-mean)/2:#補正\n",
        "            #variance = mean*(1-mean)/2\n",
        "        variance = torch.min(variance, mean*(1-mean)/2)\n",
        "\n",
        "        # alpha と beta の計算式を安全に実施\n",
        "        epsilon = 1e-6  # ゼロ除算を防ぐための小さな値\n",
        "        mean_clamped = mean.clamp(min=epsilon, max=1-epsilon)  # mean を [epsilon, 1-epsilon] でクランプ\n",
        "        variance_clamped = variance.clamp(min=epsilon)  # variance を epsilon 以上でクランプ\n",
        "\n",
        "        alpha = ((1 - mean_clamped) / variance_clamped - 1 / mean_clamped) * (mean_clamped ** 2)\n",
        "        beta = alpha * (1 / mean_clamped - 1)\n",
        "        #print(\"After:\", mean)\n",
        "        # これらの値が正であることを保証\n",
        "        alpha = torch.max(alpha, torch.tensor(epsilon))\n",
        "        beta = torch.max(beta, torch.tensor(epsilon))\n",
        "\n",
        "        dist_cnt = torch.distributions.Normal(loc=mean, scale=std) #1次元化のため\n",
        "        #dist_cnt = torch.distributions.Beta(alpha, beta) #ベータ分布にしてみる\n",
        "        #print(state[0,-1].item(),mean)\n",
        "\n",
        "\n",
        "        return dist_dsc, dist_cnt\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78j9p-tgTYPb"
      },
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=32, fc2_dims=32, fc3_dims=64, chkpt_dir=\"\"):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, \"critic_torch_ppo\")\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims,fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(fc2_dims,fc3_dims),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(fc2_dims,1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "        # 重みの初期化\n",
        "        #self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.critic:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(layer.weight)  # Xavier初期化を適用\n",
        "                torch.nn.init.constant_(layer.bias, 18000)  # バイアスを14000で初期化\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jRw3Bjb7TYPb",
        "outputId": "836d92e2-26ac-4104-ceb3-0543aa8a65f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMZpQl6gTYPc"
      },
      "outputs": [],
      "source": [
        "test_batch = 0\n",
        "class Agent:\n",
        "    def __init__(self, n_units, n_states, MAX_maintenance_time, input_dims, beta=0.0005, GAE_lam=0.00, interval=24,\n",
        "                 alpha_actor=0.03, alpha_critic=0.01,\n",
        "                 policy_clip=0.2, batch_size=512*4, n_epochs=4):\n",
        "        self.beta = beta\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.loss_history_detail = []\n",
        "\n",
        "        self.actor_loss_history = []\n",
        "        self.actor_loss_history_detail = []\n",
        "        self.critic_loss_history = []\n",
        "        self.critic_loss_history_detail = []\n",
        "        self.entropy_history = []\n",
        "        self.kl_divergence_history = []\n",
        "\n",
        "        self.actor = ActorNetwork(n_units, n_states, MAX_maintenance_time, input_dims, alpha_actor)\n",
        "        self.critic = CriticNetwork(input_dims, alpha_critic)\n",
        "        self.memory = PPOMemory(batch_size, interval=interval, beta=beta, GAE_lam=GAE_lam)\n",
        "\n",
        "    def remember(self,state,action_dsc,action_cnt,probs_dsc,probs_cnt,vals,reward, time):\n",
        "        self.memory.store_memory(state,action_dsc,action_cnt,probs_dsc,probs_cnt,vals,reward, time)\n",
        "\n",
        "    def save_models(self):\n",
        "        print(\"... saving models ...\")\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print(\"... loading models ...\")\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        act_dsc = dist_dsc.sample()\n",
        "        act_cnt = dist_cnt.sample()\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "\n",
        "        if act_dsc.item() == 3:\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "          #print(log_prob_cnt, \"log_prob_cnt\")\n",
        "        else:\n",
        "          #log_prob_cnt = 0 #dist_cntを参照しないことの補正\n",
        "          log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "\n",
        "\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def choose_action_max_prob(self,observation):\n",
        "        state = torch.tensor(np.array([observation]),dtype=torch.float).to(self.actor.device)\n",
        "        dist_dsc, dist_cnt = self.actor(state)\n",
        "        print(\"state:\", state, \"dist_dsc.probs:\", dist_dsc.probs, \"dist_cnt.mean:\",dist_cnt.mean,\"dist_cnt.scale\", dist_cnt.scale)\n",
        "        act_dsc = torch.argmax(dist_dsc.probs)\n",
        "        act_cnt = dist_cnt.mean\n",
        "        #print(act_dsc, \":act_dsc\")\n",
        "        #print(act_cnt, \":act_cnt\")\n",
        "\n",
        "        value = self.critic(state)\n",
        "\n",
        "\n",
        "        log_prob_dsc = torch.squeeze(dist_dsc.log_prob(act_dsc)).item()\n",
        "        log_prob_cnt = torch.squeeze(dist_cnt.log_prob(act_cnt)).item()\n",
        "        log_prob = log_prob_dsc + log_prob_cnt\n",
        "\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, value\n",
        "\n",
        "    def learn(self,episode, threshold):\n",
        "        self.memory.generate_advantage()\n",
        "        actor_loss_sum = 0\n",
        "        critic_loss_sum = 0\n",
        "        entropy_sum = 0\n",
        "        kl_divergence_sum = 0\n",
        "        for _ in range(self.n_epochs):\n",
        "        #for _ in tqdm(range(self.n_epochs), desc=\"Training Progress\"):  # tqdmを用いて進捗表示\n",
        "            \"\"\"\n",
        "            rewards = self.memory.rewards\n",
        "            values = self.memory.vals\n",
        "            times = self.memory.time\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * \\\n",
        "                        (reward_arr[k]+math.exp(-self.beta * (-times[k]%self.interval+self.interval))*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "            \"\"\"\n",
        "            state_arr, act_dsc_arr, act_cnt_arr, old_probs_dsc_arr, old_probs_cnt_arr, vals_arr, reward_arr, advantage, batches=self.memory.generate_batches()\n",
        "            values = vals_arr\n",
        "            \"\"\"\n",
        "            values = vals_arr\n",
        "            times = time_arr\n",
        "            advantage = np.zeros(len(reward_arr),dtype=np.float32)\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += math.exp(-self.beta * times[k]) * (reward_arr[k]+self.gamma*values[k+1]-values[k])\n",
        "                advantage[t] = a_t\n",
        "            \"\"\"\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = torch.tensor(values).to(self.actor.device)\n",
        "            start = time.time()\n",
        "            for batch in batches:  # 各バッチの進捗を表示\n",
        "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
        "                log_old_probs_dsc = torch.tensor(old_probs_dsc_arr[batch]).to(self.actor.device)\n",
        "                log_old_probs_cnt = torch.tensor(old_probs_cnt_arr[batch]).to(self.actor.device)\n",
        "                acts_dsc = torch.tensor(act_dsc_arr[batch]).to(self.actor.device)\n",
        "                acts_cnt = torch.tensor(act_cnt_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist_dsc, dist_cnt = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "                critic_value = torch.squeeze(critic_value)\n",
        "\n",
        "                log_new_probs_dsc = dist_dsc.log_prob(acts_dsc) #pi_new\n",
        "                log_new_probs_cnt = dist_cnt.log_prob(acts_cnt)\n",
        "\n",
        "                #log_new_probs = dist_dsc.log_prob(acts_dsc) + dist_cnt.log_prob(acts_cnt)\n",
        "                #print(\"log_old_probs_cnt.exp():\",log_old_probs_cnt.exp())\n",
        "                #print(\"log_new_probs_cnt.exp():\",log_new_probs_cnt.exp())\n",
        "                #print(\"log_old_probs_cnt:\",log_old_probs_cnt)\n",
        "                #print(\"log_new_probs_cnt\",log_new_probs_cnt)\n",
        "\n",
        "                prob_ratio_dsc = log_new_probs_dsc.exp()/log_old_probs_dsc.exp() #確率比\n",
        "                prob_ratio_cnt = log_new_probs_cnt.exp()/log_old_probs_cnt.exp()\n",
        "                #print(\"prob_ratio_dsc:\",prob_ratio_dsc)\n",
        "                #print(\"prob_ratio_cnt:\",prob_ratio_cnt)\n",
        "\n",
        "\n",
        "                weighted_probs_dsc = advantage[batch]*prob_ratio_dsc\n",
        "                weighted_clipped_probs_dsc = torch.clamp(prob_ratio_dsc, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss_dsc = -torch.min(weighted_probs_dsc, weighted_clipped_probs_dsc).mean()\n",
        "\n",
        "                weighted_probs_cnt = advantage[batch]*prob_ratio_cnt\n",
        "                weighted_clipped_probs_cnt = torch.clamp(prob_ratio_cnt, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss_cnt = -torch.min(weighted_probs_cnt, weighted_clipped_probs_cnt).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = F.mse_loss(returns.float(),critic_value.float())\n",
        "                #critic_loss = critic_loss.float()\n",
        "                #print(actor_loss)\n",
        "                #print(critic_loss)\n",
        "                #print(\"#####\")\n",
        "                entropy_dsc = dist_dsc.entropy().sum(dim=0).mean()\n",
        "                entropy_cnt = dist_cnt.entropy().sum(dim=0).mean()\n",
        "                entropy = torch.clamp(entropy_dsc,min=0) + torch.clamp(entropy_cnt,min=0)\n",
        "                #entropy = torch.clamp(dist_dsc.entropy().mean(),min=0) + torch.clamp(dist_cnt.entropy().mean(), min=0.0)\n",
        "\n",
        "                total_loss_dsc = actor_loss_dsc #+ 0.01 * entoropy_dsc\n",
        "                total_loss_cnt = actor_loss_cnt #+ 0.01 * entoropy_cnt\n",
        "                total_loss = total_loss_dsc + total_loss_cnt + 0.5*critic_loss #+ 0.01*entropy\n",
        "\n",
        "                if episode >= threshold:\n",
        "                  print(\"FLAG\")\n",
        "                  self.actor.optimizer_discrete.zero_grad()\n",
        "                  total_loss_dsc.backward(retain_graph=True)\n",
        "                  self.actor.optimizer_discrete.step()\n",
        "\n",
        "                #if episode >= threshold:\n",
        "                  #print(\"FLAG\")\n",
        "                  self.actor.optimizer_continuous.zero_grad()\n",
        "                  total_loss_cnt.backward(retain_graph=True)\n",
        "                  self.actor.optimizer_continuous.step()\n",
        "\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "                self.loss_history_detail.append(total_loss.item())\n",
        "\n",
        "                actor_loss_sum += actor_loss_dsc.item()+actor_loss_cnt.item()\n",
        "                critic_loss_sum += critic_loss.item()\n",
        "                entropy_sum += entropy.item()\n",
        "                kl_divergence_sum += torch.distributions.kl_divergence(Categorical(logits=log_old_probs_dsc+log_old_probs_cnt), Categorical(logits=log_new_probs_dsc+log_new_probs_cnt)).mean().item()\n",
        "\n",
        "                print(\"advantage[batch].size(),advantage[batch]:\",advantage[batch].size(),advantage[batch])\n",
        "\n",
        "        print(f'actor loss: {actor_loss_sum}, critic loss: {critic_loss_sum}, entropy: {entropy_sum}, KL divergence: {kl_divergence_sum}')\n",
        "        self.loss_history.append(np.mean(self.loss_history_detail[-self.n_epochs:]))\n",
        "        self.actor_loss_history.append(actor_loss_sum)\n",
        "        self.critic_loss_history.append(critic_loss_sum)\n",
        "        self.entropy_history.append(entropy_sum)\n",
        "        self.kl_divergence_history.append(kl_divergence_sum)\n",
        "            # Update sums\n",
        "        #self.actor.scheduler_actor.step()  # 学習率を更新\n",
        "        #self.critic.scheduler_critic.step()  # 学習率を更新\n",
        "        self.memory.clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PW7F3zTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkbRyHFHTYPc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JV5KgBxTYPd"
      },
      "outputs": [],
      "source": [
        "#エージェントの初期化\n",
        "n_units = 2\n",
        "n_states = 5\n",
        "MAX_maintenance_time = 0\n",
        "#input_size = n_units * n_states + n_units * (MAX_maintenance_time) + 2 #MDPのため[残り時間]と[保全意思決定時]の2つの入力は入れない\n",
        "input_size = n_units + 1\n",
        "action_size = 2**n_units  # 行動数は2^3個\n",
        "batch_size = 512*4#512-5120\n",
        "interval = 24\n",
        "alpha_actor = 0.0003#ここを変更する\n",
        "alpha_critic = 0.01#ここを変更する\n",
        "n_epochs = 4\n",
        "policy_clip = 0.1\n",
        "beta=0.0005\n",
        "\n",
        "\n",
        "agent = Agent(n_units=n_units,\n",
        "              input_dims=input_size,\n",
        "              n_states=n_states,\n",
        "              MAX_maintenance_time=MAX_maintenance_time,\n",
        "              beta=beta,\n",
        "              interval=interval,\n",
        "              alpha_actor=alpha_actor,\n",
        "              alpha_critic=alpha_critic,\n",
        "              policy_clip=policy_clip,\n",
        "              batch_size=batch_size,\n",
        "              n_epochs=n_epochs)\n",
        "env = Environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation0=[0,0,1]\n",
        "state0 = torch.tensor(np.array([observation0]),dtype=torch.float).to(agent.actor.device)\n",
        "dist_dsc0, dist_cnt0 = agent.actor(state0)\n",
        "value0 = agent.critic(state0)\n",
        "print(\"state:\", state0, \"dist_dsc.probs:\", dist_dsc0.probs, \"dist_cnt.mean:\",dist_cnt0.mean,\"dist_cnt.scale\", dist_cnt0.scale,\"value0:\",value0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV6_pfkGblJK",
        "outputId": "e67a1214-b5de-40c5-8747-e43ce7121895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 1.]]) dist_dsc.probs: tensor([[0.0040, 0.0030, 0.0030, 0.9900]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.0185]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[0.8463]], grad_fn=<ExpBackward0>) value0: tensor([[0.1742]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15IhPu1nTYPd",
        "outputId": "ced70f03-d1ae-4879-b27a-5df54317cddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-bcc7374e03e9>:221: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.load_total=float(dist_N.rvs(1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0132, -5000.0308, -5000.0151,  ..., -2500.0134, -4999.9922,\n",
            "        -4999.9868])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9590, -5000.0176, -5000.0122,  ..., -5000.0078, -4999.9736,\n",
            "        -5000.0225])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9844, -4999.9766, -5000.0439,  ..., -4999.9463, -4999.9951,\n",
            "        -5000.0830])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0049, -5000.0166, -2499.9578,  ..., -5000.0146, -4999.9507,\n",
            "        -5000.0391])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9995, -5000.0225, -2499.9531,  ..., -4999.9771, -5000.0293,\n",
            "        -5000.0356])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3.6901e-02, -5.0000e+03, -5.0001e+03,  ..., -5.0000e+03,\n",
            "        -5.0000e+03, -5.0000e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0151, -5000.0005, -4999.9414,  ..., -4999.9419, -5000.0210,\n",
            "        -4999.9580])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9761, -4999.9272, -4999.9912,  ..., -5000.0151, -5000.0273,\n",
            "        -2499.9700])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9556, -4999.9985, -4999.9678,  ..., -4999.9780, -5000.0176,\n",
            "        -4999.9941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2500.0427, -4999.9688, -4999.9814,  ..., -5000.0259, -5000.0728,\n",
            "        -4999.9404])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0000, -4999.9292, -2500.0107,  ..., -5000.0103, -5000.0244,\n",
            "        -4999.9688])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03, -5.0000e+03, -5.0000e+03,  ..., -5.0000e+03,\n",
            "        -4.9999e+03,  4.0874e-02])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.0015, -5000.0093, -2499.9746,  ..., -4999.9443, -5000.0259,\n",
            "        -5000.0020])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9834, -4999.9487, -5000.0005,  ..., -5000.0132, -2500.0337,\n",
            "        -5000.0190])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0000e+03,  4.9408e-02, -3.8025e-02,  ...,  6.2778e-02,\n",
            "        -5.0001e+03, -5.0000e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.9707, -5000.0400, -5000.0059,  ..., -4999.9707, -2499.9749,\n",
            "        -4999.9824])\n",
            "actor loss: 146361.36963194454, critic loss: 356283390.0, entropy: 43629.243896484375, KL divergence: 0.013760753835334573\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5510825594436763], 離散行動：[1, 1], 連続行動：0.6690946966409683\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "0エピソード目の累積報酬：-484147.38386564824, 一つ保全の回数：8108, 二つ保全の回数：47, 三つ保全の回数：37, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2500.5784, -5000.2402, -5001.4048,  ..., -5000.7363, -5000.0171,\n",
            "        -4999.2842])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5001.0474, -5001.5601, -5000.8564,  ..., -5000.0132, -4999.0410,\n",
            "        -4999.6372])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.5601, -2500.0801, -4999.4219,  ..., -5001.1597, -5000.7510,\n",
            "        -5000.8442])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5.0002e+03, -4.9994e+03, -4.9990e+03,  ..., -5.0021e+03,\n",
            "         1.4759e-01, -5.0009e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.0708, -4999.9282, -5000.9917,  ..., -5002.1030, -5001.0752,\n",
            "        -5000.0308])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5001.8740, -4998.9233, -2498.3911,  ..., -5001.2129, -4998.2979,\n",
            "        -5000.3887])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5002.3374, -5001.7915, -1499.5439,  ..., -4999.3091, -2498.6157,\n",
            "        -5000.6768])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5001.2339, -4998.4136, -4999.0869,  ..., -5000.3867, -4999.7061,\n",
            "        -4999.7891])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.1333, -5000.5386, -5001.3076,  ..., -4998.9644, -4999.0649,\n",
            "        -5000.2510])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5002.1206, -2499.3071, -5000.6689,  ..., -5001.7339, -4999.4331,\n",
            "        -5000.3228])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.6533, -4999.7202, -5000.6177,  ..., -5000.8701, -2502.4993,\n",
            "        -4999.5225])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.6362, -5001.7300, -5001.4487,  ..., -2298.6829, -4999.7549,\n",
            "        -4999.1348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.3745, -5001.2104, -4999.1079,  ..., -5000.5610, -4999.7388,\n",
            "        -4998.9858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4999.7881, -2501.1782, -5001.2998,  ..., -4999.2339, -4999.4541,\n",
            "        -5001.2559])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4998.8232, -5000.6968, -4999.7876,  ..., -4999.1309, -5000.7837,\n",
            "        -4997.9033])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5000.9355, -5000.8340, -4999.9048,  ..., -4997.5190, -4999.2217,\n",
            "        -4999.9160])\n",
            "actor loss: 147422.68502288434, critic loss: 357901988.0, entropy: 45355.889892578125, KL divergence: 0.007045789992001761\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.3635186298228263], 離散行動：[1, 1], 連続行動：-0.10702961683273315\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "1エピソード目の累積報酬：-476146.7029876567, 一つ保全の回数：8103, 二つ保全の回数：52, 三つ保全の回数：37, 違反回数：0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f8c0db9f64b5>:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(self.checkpoint_file))\n",
            "<ipython-input-5-9a2b36fae17f>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(self.checkpoint_file))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -537.8828, -1360.3602,  3305.7332,  ...,  2679.9626,  4043.1189,\n",
            "        -1639.2242])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -662.4105, -2094.7764, -3650.3855,  ..., -3096.1001, -3378.1255,\n",
            "          519.2951])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  572.1290,  -958.9802, -3436.1663,  ...,  3164.6169,  -311.0993,\n",
            "        -5822.5752])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-7888.0093, -8866.2969,  -679.3098,  ...,  4196.6729,  -941.9092,\n",
            "        -3028.3677])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1920.9193,  3941.6868,  2508.8271,  ...,  2406.2566,  2230.5786,\n",
            "          138.6125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1092.4562, -2597.0461,   140.1951,  ...,  1110.6144,  5305.0010,\n",
            "        -2873.9070])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1470.7574,  2098.8696, -6754.6060,  ..., -3883.8315,  1178.9275,\n",
            "         1848.3983])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2054.9902, -3636.1541, -1671.4025,  ..., -1073.3114,   524.7900,\n",
            "         -683.6078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  161.3163,  1573.6840,   957.4669,  ...,  1109.3876, -3552.0605,\n",
            "           27.9236])\n",
            "actor loss: 35886.55079099933, critic loss: 177218922.0, entropy: 74121.53125, KL divergence: 0.006732784555481666\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.059381257377109], 離散行動：[1, 1], 連続行動：0.17250505089759827\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "35エピソード目の累積報酬：-132102.5554469467, 一つ保全の回数：1599, 二つ保全の回数：6582, 三つ保全の回数：11, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2391.4692,   171.7846, -8538.0645,  ..., -3089.1775, -5807.3145,\n",
            "         6960.6445])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3089.4417,  1709.0918, -7515.2148,  ...,  -504.6590, -1989.6583,\n",
            "         5284.8530])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1525.3748, -3542.3274, -7682.9629,  ...,  4460.9546,  2942.3000,\n",
            "        -4579.0137])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1554.5724, -3884.6748, -7346.1758,  ...,  3571.0305, -3179.0979,\n",
            "        -2494.0693])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-7948.2241,  -712.6393,  1593.5287,  ..., -6313.3188,  1818.6605,\n",
            "         -381.4453])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3242.7876,   663.7060,  -994.7892,  ...,  1776.7177, -3983.8274,\n",
            "        -3842.5146])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2726.8237,  -452.7309, -2735.8691,  ..., -1119.3530, -5449.9253,\n",
            "         1994.0754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  422.6619, -7870.4590, -1921.4661,  ...,  1036.7850, -1292.8898,\n",
            "        -2742.4919])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2482.0491, -5169.8486,  2197.8386,  ...,  2420.1487, -1132.6168,\n",
            "         4125.3433])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -526.4385, -3791.2012,  1857.8989,  ...,   200.6457, -1629.3851,\n",
            "        -2845.5081])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1844.7476, -1171.4462,  3268.3992,  ...,  3076.5134, -4577.3579,\n",
            "          803.4956])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  999.5547, -2408.0442,  -676.7820,  ...,  -687.5895, -1332.9871,\n",
            "          431.5771])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  264.4964, -1309.3121,  -419.3489,  ..., -2653.2280,  -187.7974,\n",
            "          696.1338])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  751.1849, -1962.1439,  -749.8964,  ...,   988.0353, -2660.5859,\n",
            "         1626.5782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4725.3154,  2967.6052,   602.9642,  ..., -1075.1023,   701.8961,\n",
            "         -789.1777])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2590.1472, -1410.2213,  1458.1519,  ...,  4714.9521, -1080.9961,\n",
            "         2110.5085])\n",
            "actor loss: 35367.95305589416, critic loss: 156603990.0, entropy: 74449.6728515625, KL divergence: 0.007722519242933138\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5731372082350428], 離散行動：[0, 1], 連続行動：-0.03590118885040283\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "36エピソード目の累積報酬：-108894.29889189474, 一つ保全の回数：1501, 二つ保全の回数：6682, 三つ保全の回数：9, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2896.0530, -3766.4971,   935.7442,  ...,  1839.3544,  1946.6500,\n",
            "        -1647.6508])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2593.4272,    59.0936,  -523.7679,  ...,  4901.6299,  -589.9348,\n",
            "        -2073.5388])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3598.8523, -2032.0217,  2325.0815,  ...,  -902.2401, -5473.2739,\n",
            "          713.3710])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4863.2920, -5102.5771, -4744.6250,  ..., -4007.8533,   404.1090,\n",
            "         -333.7905])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2586.5115,  3221.5408, -1684.5532,  ..., -6768.6323,  1801.3182,\n",
            "        -3280.8027])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 6074.0542,  -277.1204,  -583.5230,  ...,    29.1891, -2228.8044,\n",
            "        -8840.5850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4877.3071,   179.9468, -1999.0400,  ...,  -918.0391,  -566.7448,\n",
            "         1082.3795])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4777.9565,   834.3533, -1266.0669,  ...,  -511.8732,  -971.2408,\n",
            "          435.4805])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3233.7227,   -60.9242,  1595.8107,  ..., -1188.0660, -2582.0193,\n",
            "         2127.9810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -765.1721, -1288.2692, -2842.6614,  ..., -1571.8505, -1829.6743,\n",
            "        -1105.8544])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-9046.0801, -6520.1895,  -946.7194,  ..., -1467.0127,  1335.2205,\n",
            "        -1421.6128])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5596.6528, -4964.3198,  4006.3105,  ...,   562.1216,  2371.2698,\n",
            "         -111.4767])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -85.1741, -3709.2844,  -538.3558,  ...,   390.4249,   182.5883,\n",
            "        -2596.5032])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5324.6553,  2711.5486, -4919.0034,  ...,  3743.6343,  2445.1814,\n",
            "          859.9562])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  310.6958, -2465.8193, -1548.4081,  ...,  3338.2949, -2310.5791,\n",
            "         5750.9893])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1154.8710, -2123.9185,  -804.8061,  ...,  4044.4607, -5507.4956,\n",
            "         2407.1440])\n",
            "actor loss: 36309.79069431288, critic loss: 134691322.0, entropy: 73514.5390625, KL divergence: 0.006471587527572206\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6714218945154429], 離散行動：[0, 1], 連続行動：-0.23087847232818604\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "37エピソード目の累積報酬：-121370.39203630513, 一つ保全の回数：1441, 二つ保全の回数：6743, 三つ保全の回数：8, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1668.1329,    34.5178,  3100.9705,  ...,    78.8302,  1108.4906,\n",
            "        -1208.1993])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  532.8005,  -750.0482, -3262.4109,  ..., -3706.7869, -1137.0264,\n",
            "         3992.5981])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2354.7405,  -441.1160,  4032.1477,  ..., -1475.1824,  -492.3688,\n",
            "          594.9591])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -215.8459, -2544.8953, -2068.7986,  ..., -4597.5215,  2873.1619,\n",
            "         2486.8784])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3732.8948, -3926.0574, -1368.1202,  ...,   329.9666,  -692.0515,\n",
            "         3548.5376])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1464.0325,  1619.9965,  1993.8171,  ..., -3825.3052,   876.1714,\n",
            "        -1835.7429])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1350.2852,  1046.8320, -3877.5042,  ..., -5387.2100, -3615.6016,\n",
            "        -3553.0852])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  480.7784, -1378.5096, -2309.1350,  ..., -3050.1091, -1069.1881,\n",
            "          407.7211])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -192.9803,  -208.4716, -4472.3359,  ...,  3751.2581,   135.8458,\n",
            "        -5472.4941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5615.7646,  2305.6753, -6001.4678,  ...,  2066.1238, -5843.7300,\n",
            "         3968.0149])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1247.5331, -4657.5898,  -225.2635,  ..., -2074.4727, -4442.9487,\n",
            "         -583.9186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2280.8765, -3526.9956, -1885.0458,  ...,  4125.1221,  1261.9432,\n",
            "         -645.7091])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3517.1775,   553.3864,  1761.4014,  ..., -1242.7339, -4116.7734,\n",
            "         -375.5262])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2447.5925, -1955.3965,  -850.6360,  ...,   516.7290, -2270.8452,\n",
            "        -3190.4265])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2582.4961,  2335.5901, -7368.4307,  ...,  3143.5437, -1157.3396,\n",
            "         3498.8359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3362.1367, -3825.3052,  -630.5831,  ...,  -994.1148, -7307.9331,\n",
            "        -6798.8101])\n",
            "actor loss: 36798.52160540321, critic loss: 122176489.5, entropy: 72737.80712890625, KL divergence: 0.0061733173469716695\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.3095133747933585], 離散行動：[0, 1], 連続行動：1.229323923587799\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "38エピソード目の累積報酬：-118274.53291359625, 一つ保全の回数：1344, 二つ保全の回数：6843, 三つ保全の回数：5, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3104.6619, -4705.4336, -2953.8228,  ..., -1640.3507, -3356.8496,\n",
            "        -1185.4995])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  291.1533,  -796.7614,   776.5377,  ..., -1045.4469,  4566.9692,\n",
            "         -296.9864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-9.7268e+02, -4.4863e+03, -8.6127e+03,  ...,  5.7910e+00,\n",
            "        -1.8293e+03, -6.6702e+01])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-6919.8091,  1827.0419,  -898.0070,  ...,   -91.4455, -2082.5803,\n",
            "        -1909.2588])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.5517e+03, -3.4174e+03, -1.9307e+03,  ...,  3.5384e+03,\n",
            "        -4.3475e+00, -5.2266e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1355.8254, -1766.7008, -3152.3628,  ..., -1264.8478,  3324.5911,\n",
            "         -676.9067])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5760.2300,    89.3497, -1937.3920,  ..., -2158.9617, -6552.4580,\n",
            "         3724.0264])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3205.6309, -4174.1421, -5539.7090,  ...,  -865.3325, -5694.7041,\n",
            "        -6422.3770])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3864.0159,  3516.2063, -2908.0439,  ...,  1803.5912,  3473.8372,\n",
            "         2532.9255])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2597.8301,  2923.0547, -3351.3240,  ..., -3334.8125, -5066.1997,\n",
            "         -234.0480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2132.2029, -3371.3887,  1018.8239,  ..., -3532.2925, -3217.0466,\n",
            "        -3061.9836])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -843.9459,  2760.4629, -7865.9380,  ...,  1418.5111, -2105.9355,\n",
            "        -1539.2271])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2319.8804,   600.2325, -1927.1139,  ...,   887.8763,  1017.8361,\n",
            "         -256.3795])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -413.8236,   800.8278,  1477.4503,  ...,  1856.1776, -1551.6111,\n",
            "        -5605.6543])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3183.9441,  1394.4396, -1681.9631,  ..., -1366.3809, -1264.8478,\n",
            "         3230.7009])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1858.5669, -5113.4238, -2121.0256,  ..., -1211.5863, -5160.0332,\n",
            "        -3609.0969])\n",
            "actor loss: 35681.38478966535, critic loss: 102709140.5, entropy: 72230.91015625, KL divergence: 0.005192309532788348\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.0489762991779568], 離散行動：[1, 1], 連続行動：0.8346311450004578\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "39エピソード目の累積報酬：-107094.60107610373, 一つ保全の回数：1207, 二つ保全の回数：6977, 三つ保全の回数：8, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2247.7065, -3605.3274,   981.7664,  ..., -4273.1118, -3136.8728,\n",
            "          542.1284])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4388.6504, -1736.1089, -2022.3127,  ..., -3765.0933,  2319.6060,\n",
            "          448.3642])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -980.7899, -3469.8909, -2135.1021,  ..., -1228.3298,  -127.7408,\n",
            "        -1065.1646])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3694.1458,   971.5942,  -874.0898,  ..., -3112.9827,  2169.7263,\n",
            "         1609.2433])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2248.2000,    89.3753, -2230.2954,  ...,   827.4364,   221.6622,\n",
            "        -1476.1078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3241.3252,  -215.3578,   262.1887,  ..., -4658.2563,   738.2914,\n",
            "        -2663.2498])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3389.0618,  1221.9259, -1427.6440,  ...,  -200.9048, -1979.7209,\n",
            "        -6242.4858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3485.3572,  1519.0122, -1647.7639,  ...,  -945.7507, -2292.0081,\n",
            "        -4028.7278])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  978.4062,  4069.2271,  -600.2405,  ...,  1109.7592, -2680.1477,\n",
            "        -2226.2246])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -488.8116, -6276.7290, -3917.0291,  ..., -5434.8999,  3399.7297,\n",
            "          189.7865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 4624.0576, -2522.5183,  -164.6376,  ...,  -998.8455, -1652.2030,\n",
            "        -3917.6187])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3351.0161, -5266.0571, -2370.9075,  ..., -2755.9202, -4297.5059,\n",
            "        -3493.7991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1646.1464,   543.2807,  1835.3707,  ...,  4624.0576,   755.3839,\n",
            "        -1457.1509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  257.4890, -2749.1665,  -777.1880,  ...,  -147.2462,   903.2893,\n",
            "        -3583.5044])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  969.0132,   601.2084,  3081.0208,  ..., -5302.2163,  3222.8740,\n",
            "           21.5517])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4069.0432,   901.2263, -1948.6072,  ...,  2610.9832,  1367.1993,\n",
            "          467.3751])\n",
            "actor loss: 35467.36152786572, critic loss: 89760781.0, entropy: 71358.283203125, KL divergence: 0.005144199902246982\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.868160740463849], 離散行動：[0, 1], 連続行動：-0.4013350009918213\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "40エピソード目の累積報酬：-111958.77834299671, 一つ保全の回数：1175, 二つ保全の回数：7013, 三つ保全の回数：4, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 3555.2524, -2387.9192,   954.7131,  ..., -1746.4049, -1597.5908,\n",
            "         -683.7626])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2587.6199, -1330.2460, -2995.8203,  ...,  1320.3311, -2247.6414,\n",
            "        -2372.9751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1693.0431,  -663.9723,  -507.5775,  ..., -2423.9690,  -846.0708,\n",
            "          -46.9348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -246.4203,  -175.2231, -2051.0522,  ...,  -583.5917,  3060.7615,\n",
            "          103.7012])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2014.6216,  2880.8879,   710.8254,  ..., -2702.8572,  1376.9415,\n",
            "          119.9187])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1724.6869,  -799.9260, -1508.8655,  ..., -2799.6472,  2334.2817,\n",
            "          840.6427])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3496.1445,  1330.0845,  -125.0829,  ...,  1833.5880,  1414.7144,\n",
            "         1565.0569])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2531.6321,   -27.4681,  -564.5844,  ..., -1649.2263, -2209.7007,\n",
            "        -4023.0608])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1840.6722, -1502.4124, -4095.9575,  ...,  1356.8921, -1417.4069,\n",
            "          795.8150])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2031.0724, -4041.6758, -1276.6985,  ...,   722.7069,   648.6957,\n",
            "        -1515.6653])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2495.3289,  1841.5187, -3558.9131,  ..., -2603.8120,  2336.6340,\n",
            "         -751.3232])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-5490.2520,  -904.4396, -3572.8528,  ...,  2566.3420,  -855.1920,\n",
            "          901.0253])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2116.8135, -1186.7061, -2039.6058,  ...,   723.2987,  -883.5160,\n",
            "         2099.2900])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -789.6926,  -420.3839, -3209.1304,  ...,  1396.7113, -4242.6655,\n",
            "        -2023.0057])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4162.9531,  1265.9634, -1354.5679,  ..., -1412.3225, -4788.9932,\n",
            "           99.3464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1990.6107,   241.6369,   859.5967,  ..., -3767.1978, -2701.0864,\n",
            "        -1226.9067])\n",
            "actor loss: 35474.213323876815, critic loss: 75651562.5, entropy: 70873.9111328125, KL divergence: 0.004074916859948994\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.3707408940901975], 離散行動：[0, 1], 連続行動：0.05891317129135132\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "41エピソード目の累積報酬：-94424.40659510551, 一つ保全の回数：1104, 二つ保全の回数：7082, 三つ保全の回数：6, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3211.5935, -4277.8423, -2446.3538,  ..., -2578.8301, -1186.3496,\n",
            "         1252.8389])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1904.3503, -1304.1238,  2818.3140,  ..., -2352.8640, -3019.4041,\n",
            "        -1738.6605])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3549.7769, -1224.3640, -2087.3145,  ...,   618.3031,  -459.1187,\n",
            "        -3856.0312])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3248.2888, -3343.7400, -1424.8860,  ..., -1248.1119, -1536.4080,\n",
            "         1332.8978])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -285.9027, -1044.8811, -1011.9120,  ..., -2981.6721, -6940.6401,\n",
            "        -2546.0784])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1843.3761, -1276.9622,  1432.5299,  ..., -4315.4536, -1994.8728,\n",
            "          389.7058])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3635.4724,  -779.1091,  -173.9696,  ...,  -966.2127, -1667.3103,\n",
            "        -1109.1084])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2.2812e+03, -1.9737e+03, -2.2237e+03,  ..., -9.4064e-01,\n",
            "         2.6765e+03,  1.0735e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3121.5090, -4506.9038,  3184.7349,  ...,  2025.4297, -1601.5365,\n",
            "          211.4500])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1486.1439, -1504.8085, -2566.3547,  ...,  -574.8657,  3033.5674,\n",
            "         -325.9821])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  796.5161,  1987.2911, -3778.0925,  ...,  -752.8732, -2933.1172,\n",
            "        -2189.6648])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2154.6394, -1418.5723,   222.3383,  ..., -4217.7803, -3522.4839,\n",
            "        -1981.5720])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1305.1038, -6149.7397, -2503.0825,  ...,  -640.2662,  -559.2578,\n",
            "        -2917.7151])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1399.4547,   199.8842, -2806.8843,  ..., -2620.1013,  -794.8671,\n",
            "        -3203.0840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2393.7981,  -756.3528,  3379.6182,  ..., -2707.7446, -4308.6533,\n",
            "          595.5665])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1684.9204,  -772.4613, -2386.9446,  ...,   861.3627,  -629.5182,\n",
            "          -49.2571])\n",
            "actor loss: 34896.65856921461, critic loss: 68073235.25, entropy: 70119.49169921875, KL divergence: 0.0035158087240754507\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.9143965787862465], 離散行動：[0, 1], 連続行動：-0.126620352268219\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "42エピソード目の累積報酬：-106513.7044780272, 一つ保全の回数：1040, 二つ保全の回数：7142, 三つ保全の回数：10, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  820.0931, -3337.8569,  -154.5818,  ..., -1397.5636, -3074.7144,\n",
            "         -364.6119])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3589.9438, -1423.5857, -1917.3374,  ...,  -937.1609, -4717.3237,\n",
            "        -1524.7023])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 2129.6802, -2240.7422,   283.7491,  ...,  -477.5249,  -978.4159,\n",
            "         2327.7988])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1668.5797, -1560.9796,  1665.4684,  ...,  -225.1134,   -80.2018,\n",
            "        -2328.9363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-4087.1602,   878.9858,   515.7499,  ..., -2436.3545, -2399.1396,\n",
            "        -2795.9937])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1380.3868, -1323.8918, -1337.3724,  ...,  -844.9688,    28.4774,\n",
            "         -154.7563])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1918.5664, -1104.8942,   -85.9959,  ...,  1437.1512, -2660.9729,\n",
            "         -153.8801])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1817.2670, -2419.8718, -2727.9226,  ..., -4590.6934,   195.2529,\n",
            "         -919.7975])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2683.3640, -1517.6411,   664.2074,  ...,  1180.1771, -1427.7822,\n",
            "        -1867.6088])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  423.8003, -1876.9670,  2153.8582,  ...,    24.7776,   -20.8508,\n",
            "        -2476.4126])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1830.3184,  -855.2969, -3309.5955,  ..., -1317.6201,   641.7227,\n",
            "           21.5259])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  768.4807,  -147.3886, -2724.0820,  ...,   -15.5920, -3596.2192,\n",
            "          956.4532])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1947.3506,  2687.9553,    21.1931,  ...,  1422.9360,   547.1153,\n",
            "        -1147.2174])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -145.7634, -1475.0065, -4707.0757,  ..., -2685.6858,  1671.3445,\n",
            "         2395.0574])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  126.5355, -1906.0259, -1911.1439,  ...,   831.2491, -3522.4446,\n",
            "          452.5245])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1661.2933, -5415.8823, -1060.8796,  ...,  1826.6680, -1767.6697,\n",
            "         -221.4453])\n",
            "actor loss: 35375.611251957256, critic loss: 57632341.25, entropy: 69554.03857421875, KL divergence: 0.003302884069634979\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2967269874968292], 離散行動：[0, 1], 連続行動：1.2360318899154663\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "43エピソード目の累積報酬：-118673.59199377867, 一つ保全の回数：969, 二つ保全の回数：7218, 三つ保全の回数：5, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  754.4886, -2169.6296, -1954.0446,  ..., -2906.6543,  -889.2067,\n",
            "        -3357.7056])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  437.3388, -1654.7572,   489.3184,  ..., -2005.7168,  -339.3109,\n",
            "        -1965.2543])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3085.6133, -1270.0186, -4752.0273,  ...,   334.6241,  -567.2279,\n",
            "          526.7197])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -507.8603,  1326.0026, -2594.0295,  ..., -4334.5459,   482.9111,\n",
            "         1800.7532])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3523.0913, -2866.6912, -2589.5449,  ...,  -742.3158, -3163.2834,\n",
            "        -3864.8271])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1421.0669, -2827.1567,  1054.1268,  ...,  1177.3485,   963.8024,\n",
            "         -948.8545])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2788.2566,  1217.9509, -1273.4081,  ..., -1795.7234, -2308.9272,\n",
            "        -1605.9695])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1437.2278, -1710.2739, -4032.2368,  ...,  -979.8055, -1170.9164,\n",
            "         1295.7970])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -845.5527,  -896.2273, -3142.5020,  ...,  1487.2358, -2146.8616,\n",
            "        -1245.9117])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3083.2107,  -511.7583,  -742.3158,  ...,  -772.2424,  -567.2279,\n",
            "        -1079.5161])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -549.9735, -1732.3608,   159.0526,  ..., -2611.1875, -2769.1978,\n",
            "        -1110.2552])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2252.8169, -4581.1021, -3908.4170,  ..., -1806.8154,   157.0239,\n",
            "         -440.9659])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1243.1869, -1774.7396, -2003.9058,  ..., -1607.5963,  1355.2720,\n",
            "          496.2103])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1128.6808, -3692.4839, -3134.6892,  ...,   191.1447, -3492.0823,\n",
            "        -2640.4121])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  456.1808, -1128.7269,  2313.5332,  ...,   922.2527,   178.9456,\n",
            "           -6.0533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 879.8710,  451.9892,  642.8075,  ...,   78.0592, -771.1894,\n",
            "        -289.7149])\n",
            "actor loss: 34384.21199275373, critic loss: 48805999.25, entropy: 69498.359375, KL divergence: 0.004056938719964608\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.8476000732740587], 離散行動：[0, 1], 連続行動：0.7187695503234863\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "44エピソード目の累積報酬：-118248.07443167952, 一つ保全の回数：871, 二つ保全の回数：7317, 三つ保全の回数：4, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2120.2576, -1133.5747, -1611.1639,  ...,  -621.5419, -2630.6411,\n",
            "         -333.2789])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1632.5857, -2339.1033, -3569.6553,  ..., -1631.0900,   388.5825,\n",
            "        -1407.7170])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -941.5540, -1294.3231, -3246.0208,  ...,   156.0679,  2219.6975,\n",
            "        -1474.0505])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1556.4369,   288.2913,  -261.5092,  ...,  1434.7847,  -665.1393,\n",
            "        -3430.6458])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -508.3954,  -604.2397, -2155.3369,  ..., -1558.8076, -2817.4067,\n",
            "         -954.2640])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 166.0922, 1764.7532,  681.7949,  ..., -602.4941,  331.5827,\n",
            "         840.1694])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -229.7772, -1626.9078,   111.4497,  ...,   470.2510, -1051.9045,\n",
            "        -1141.8438])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3384.4873,  1162.5345, -1172.0570,  ...,   -73.2920,  -823.3096,\n",
            "        -1072.9796])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1373.9971, -4126.3857,  -200.3525,  ...,  -256.0442, -1653.6919,\n",
            "          400.6734])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -979.3212, -3365.3516, -2138.6987,  ...,  -534.8890,  -706.7266,\n",
            "           45.6782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -590.1460,   581.6575,   580.0872,  ...,   747.6218, -1482.6906,\n",
            "         -611.9827])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1428.5089,  -545.9276, -2197.6077,  ...,    20.4035,  -849.6385,\n",
            "         -484.8611])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -349.0638, -3753.6636, -1785.2456,  ..., -2324.7800,   529.3868,\n",
            "        -3402.2363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -261.6702, -2323.0923,    72.3269,  ..., -2450.5178, -1473.0649,\n",
            "        -3557.4500])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1448.9108, -3763.9065,   832.6555,  ...,   468.3762,  -238.7012,\n",
            "        -2274.1968])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1891.5530, -1087.4298,    21.0720,  ..., -2736.9243, -1973.1710,\n",
            "        -2942.3818])\n",
            "actor loss: 34492.69179277088, critic loss: 44192102.75, entropy: 69713.787109375, KL divergence: 0.0022917242051878504\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0062217060036993], 離散行動：[0, 1], 連続行動：0.834100067615509\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "45エピソード目の累積報酬：-115232.38432830955, 一つ保全の回数：810, 二つ保全の回数：7379, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1517.0247, -2775.5823,  1249.9600,  ..., -2033.6774,  -586.2637,\n",
            "         -579.6469])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2283.6643, -2350.0786, -2084.5173,  ..., -1033.3185,  -372.3928,\n",
            "          707.3361])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 1163.9778,  1514.3357, -2302.8440,  ..., -3422.2109, -1933.5096,\n",
            "           33.4489])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  924.8876,  -667.9283,  -512.9540,  ...,   -64.4705,    20.5940,\n",
            "        -2674.8242])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -762.4570,  -137.2152, -2099.7002,  ...,  1254.1731,   166.4567,\n",
            "        -2575.9500])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3226.7087,  -872.4882,  -825.1763,  ...,   491.8775,  -959.8015,\n",
            "        -2084.5173])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1262.7450, -3263.3621, -2046.7235,  ...,  -892.3459, -2862.2002,\n",
            "          945.4711])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1606.7582, -2214.5349, -1011.9066,  ..., -1578.8470, -1004.6760,\n",
            "         -887.6844])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2421.9514, -1525.8361, -1415.8007,  ..., -1530.2709,  -736.0967,\n",
            "        -4507.1553])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -930.4220,  -510.9673, -1217.0995,  ...,   924.8876, -3104.2817,\n",
            "        -1263.2876])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -588.7035, -1505.5106,  -580.5224,  ...,  -252.7629,  2063.1130,\n",
            "        -1257.7676])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2466.4053, -2855.6592,  2092.4109,  ...,  -359.9302, -3026.5750,\n",
            "        -2709.3906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2276.5371, -2310.3311, -1537.1443,  ...,  -687.6575, -1172.2866,\n",
            "         1320.1858])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1637.7208,   552.0890, -3450.8977,  ...,  -940.5632, -2223.5674,\n",
            "        -1756.8160])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1318.2109,  -736.0967,  -265.0213,  ..., -2776.0542, -1166.8383,\n",
            "        -1041.7662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1854.9561,   478.0651, -3964.5857,  ...,   128.1507, -2303.0930,\n",
            "         -441.5657])\n",
            "actor loss: 34328.38504883951, critic loss: 40018184.0, entropy: 69576.615234375, KL divergence: 0.0035146624735995806\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.8781817890078327], 離散行動：[0, 1], 連続行動：1.0974662899971008\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "46エピソード目の累積報酬：-103725.5400651725, 一つ保全の回数：761, 二つ保全の回数：7428, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1569.0433,  -929.6141, -1390.7687,  ..., -1094.9348,  -582.5018,\n",
            "          -76.0959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1859.7242, -1628.6954, -1149.2539,  ..., -3042.7695, -2537.7625,\n",
            "        -1825.3340])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2552.4036, -1144.5679, -1187.7135,  ...,   -27.0772,  -878.2266,\n",
            "         1347.6298])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2388.1536,   270.7693, -2293.3445,  ..., -3177.3000,  -967.3875,\n",
            "         -472.0584])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1290.2188, -2405.8545, -1843.3942,  ...,  -421.4132,   149.2951,\n",
            "        -1299.4382])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1126.4551, -3177.3000,  -293.4446,  ..., -1198.6205, -2544.3357,\n",
            "        -1945.0469])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1056.4658,   288.2281, -2252.1709,  ...,    28.5264,  -636.8675,\n",
            "        -2265.6785])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2839.6453, -1052.4723, -2320.6516,  ..., -1640.6316,  -787.8054,\n",
            "          532.9478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2097.7031,   307.0256,  -637.8054,  ..., -2331.3027,   -23.5199,\n",
            "        -1256.5302])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2462.9456,  -408.4938,  1075.0060,  ..., -3441.9299, -1570.7872,\n",
            "        -1894.8801])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3466.4204,   187.2222,  -648.4511,  ...,  -636.4703,   351.9945,\n",
            "          305.1662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -344.6183,  -296.2923,   801.3224,  ..., -3368.5078, -1894.3770,\n",
            "          358.3384])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1473.8716,  -460.0554, -1305.5690,  ..., -1041.3737, -4443.0449,\n",
            "        -3043.0212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -236.9273,  -616.7444,  -786.1463,  ..., -1820.3782, -1757.3281,\n",
            "          611.0749])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2907.3730,    30.8650,  -864.1129,  ...,  -214.8024,     7.3122,\n",
            "        -1279.2679])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -755.8965,  -485.2915,  -359.4088,  ..., -1010.4317, -3850.1155,\n",
            "           -5.9934])\n",
            "actor loss: 33039.51121706392, critic loss: 34522034.625, entropy: 69759.98095703125, KL divergence: 0.0014340123391864306\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.405965726111598], 離散行動：[0, 1], 連続行動：-0.39835184812545776\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "47エピソード目の累積報酬：-102529.45415249154, 一つ保全の回数：593, 二つ保全の回数：7593, 三つ保全の回数：6, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1387.9674, -2105.0566,  -174.4689,  ...,  1080.0154,  -616.0637,\n",
            "          757.1171])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2579.8997,  -736.6181,  -958.6875,  ..., -2597.9941, -2708.7761,\n",
            "        -2438.7949])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -568.2991,  -754.6243, -1950.1078,  ..., -1439.1514, -1310.3466,\n",
            "        -1102.2283])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1273.8862,  -438.2041,   440.5092,  ...,  -130.1967,    63.7262,\n",
            "        -1378.6083])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1711.8845,  -716.1416, -2343.6313,  ...,   488.9331,   323.4972,\n",
            "        -2293.5356])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1158.1058,   326.1353,  -814.3749,  ...,  -192.8528,  -858.0809,\n",
            "         1370.1786])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1192.9994,  -954.9243,  -771.6407,  ...,  -391.4534, -3033.6809,\n",
            "        -1432.8148])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1277.5880,   764.2978,    23.7860,  ..., -1111.5718,   544.7317,\n",
            "        -1330.4631])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  468.7496,  1152.8110, -1100.4664,  ..., -1195.5443,  -849.0814,\n",
            "          203.6344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2501.3635, -1097.2986, -2083.5933,  ...,   476.6661,   638.9406,\n",
            "        -2054.3325])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -476.3443,  -648.6364, -2013.3646,  ...,  -435.6582,   179.1580,\n",
            "        -3807.0117])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -599.3653, -2617.1147, -1751.0778,  ..., -2917.3618, -1060.1193,\n",
            "        -3162.3569])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1700.4530,   430.6039, -1079.2218,  ..., -1768.3492,  -296.5801,\n",
            "        -1310.5286])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -265.6903,  -696.1701, -2202.3752,  ..., -2942.9421, -1746.3936,\n",
            "          -81.4007])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -674.2570, -1654.2797,  -406.4372,  ..., -3712.5881,   344.8653,\n",
            "        -1452.8579])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2802.2395, -1884.4333,  -677.7233,  ..., -3120.7749, -2591.1279,\n",
            "        -2385.6221])\n",
            "actor loss: 33033.92754390666, critic loss: 31133595.75, entropy: 68195.5966796875, KL divergence: 0.0019546844089993803\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0018329635243808], 離散行動：[0, 1], 連続行動：0.20837479829788208\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "48エピソード目の累積報酬：-99134.88053925267, 一つ保全の回数：535, 二つ保全の回数：7653, 三つ保全の回数：4, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1031.9133,  -793.0948, -2427.8184,  ..., -1677.2667, -2009.4805,\n",
            "        -1792.9781])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -523.1927,   164.3450, -2820.5806,  ..., -1940.3290,  -860.6827,\n",
            "        -1001.0718])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -64.0791, -3958.8342, -3567.7490,  ..., -3179.1255,   151.4258,\n",
            "        -2157.5371])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -691.2165, -4194.4028, -2792.9736,  ...,  -257.1183,  -410.1321,\n",
            "        -3701.7229])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   53.5690, -1829.6449,  -140.6658,  ..., -1853.8159, -2295.3977,\n",
            "          -94.4484])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -341.6800,  -872.0943,  -499.5288,  ..., -2288.5698,  -638.4991,\n",
            "         -221.7912])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1430.4159, -1826.1460, -1045.4128,  ...,  -574.9429, -1660.4143,\n",
            "         -349.3469])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2285.0559, -1913.6876,    55.4430,  ..., -1460.8685, -1262.7638,\n",
            "        -1493.7534])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1058.7753, -1460.8685, -2279.1538,  ...,   383.2962,  1362.6378,\n",
            "        -1335.5132])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -842.8849, -2701.9919, -1937.7092,  ..., -2275.0056,  -984.3544,\n",
            "         -800.3438])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -80.2803,  -284.8324, -1001.6852,  ...,  -469.0333,  -655.0952,\n",
            "         -425.9249])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2595.9956,   292.2083,   753.9224,  ...,  -552.9898,  -724.8533,\n",
            "        -2452.3340])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1065.5171,  -954.8320,   -63.3001,  ...,  -967.5091, -2107.7778,\n",
            "        -1422.9907])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -96.7310,  -778.5662, -1257.2245,  ...,  -252.6322, -1854.2987,\n",
            "         -419.1298])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -698.3487, -1158.8967,    52.7957,  ..., -2504.0354, -2470.9343,\n",
            "         -944.7189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  353.4603, -1643.8350, -2200.8379,  ..., -3594.7988, -1624.0431,\n",
            "        -1340.2856])\n",
            "actor loss: 32706.041652107444, critic loss: 29150717.875, entropy: 67866.42724609375, KL divergence: 0.0008645130872471343\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7890757936196193], 離散行動：[0, 1], 連続行動：0.06677976250648499\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "49エピソード目の累積報酬：-100459.57603852072, 一つ保全の回数：513, 二つ保全の回数：7675, 三つ保全の回数：4, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -30.3046, -1547.3959, -1891.7814,  ...,  -930.3900, -1433.2388,\n",
            "          645.1650])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1708.7218,  -477.4012,  -830.5959,  ..., -2417.6917, -1883.1016,\n",
            "         -818.1309])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1254.7467,   201.9456, -1215.8793,  ..., -1663.7568,   175.9457,\n",
            "        -1145.4563])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  260.9431,  -125.0817,  -512.7230,  ..., -1764.1420,  -839.1643,\n",
            "         -123.9594])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2274.8057,   541.6085,  -348.1164,  ..., -1555.4131,  -577.7436,\n",
            "        -1620.8489])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -209.3928,   825.9510, -2115.4746,  ...,   150.5049, -2264.3010,\n",
            "          266.7612])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1683.3422,   884.6412,   406.0282,  ...,  -580.6873,   -66.3750,\n",
            "         -177.0040])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -546.3226,   118.9248, -1702.9044,  ..., -1887.1075,   754.7365,\n",
            "        -1645.8464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3069.6880,  1009.6025,  -784.5536,  ...,   498.4769,  -997.9228,\n",
            "        -1610.7214])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2331.9475,  -616.2682, -3066.0159,  ..., -1598.6215,  -805.4677,\n",
            "         -778.1068])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -612.5839, -1206.3274, -1770.2903,  ..., -1113.8848,  -695.5353,\n",
            "        -1455.8679])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -635.1812,   740.1307, -1076.9370,  ..., -1471.9071, -1094.7333,\n",
            "         -180.1327])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1632.0636,    96.4912,  -712.5338,  ..., -1213.9266,  -739.1214,\n",
            "          211.4611])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2810.2605, -2903.1125,  -175.1301,  ...,   -87.2325, -1126.6078,\n",
            "          636.3354])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -227.6078,   -22.9192,   153.2182,  ..., -1885.0665,  -502.9209,\n",
            "          -66.3750])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  386.4483, -1414.8018, -1907.8014,  ...,   177.5056, -1923.1534,\n",
            "         -949.3223])\n",
            "actor loss: 32572.567892046758, critic loss: 27373949.25, entropy: 66926.302734375, KL divergence: 0.0010859150885740567\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.64069127889615], 離散行動：[0, 1], 連続行動：-0.08588337898254395\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "50エピソード目の累積報酬：-107203.3899399169, 一つ保全の回数：521, 二つ保全の回数：7669, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2616.9495,  -939.2735, -2398.9646,  ..., -1304.8792, -1917.5148,\n",
            "        -2269.8147])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1424.9497, -2040.3552, -2557.0518,  ..., -1772.2698, -2954.4070,\n",
            "        -1538.7207])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-167.2751, 1001.6611, -775.9973,  ..., -897.5207, 1054.3821,\n",
            "        -134.3203])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1996.0157,   283.6533, -1922.1230,  ...,  -475.6452,  -779.7861,\n",
            "          220.8392])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -850.1772, -1634.1860,  -239.2665,  ..., -1640.5907, -1298.5756,\n",
            "        -1033.6780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -672.8922, -1063.2614,  -622.0215,  ...,   494.5455,  -563.7144,\n",
            "         -681.0591])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  577.5520,  -428.0475,  -688.4308,  ...,   712.9033, -2290.5034,\n",
            "        -1364.4122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   10.5670,   642.2032,  -934.2023,  ..., -1049.3888,   543.0837,\n",
            "          938.5139])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1970.5770,  -937.2925,   136.4469,  ..., -1335.3660, -1492.2310,\n",
            "          -94.2603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   94.1054,   272.7859, -1304.3450,  ..., -2399.8362, -1973.7292,\n",
            "         -420.4332])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1550.5438, -1475.2396,  -717.4092,  ..., -1338.9977,  -860.9489,\n",
            "         -272.5005])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -753.6762,  -881.8824,  -754.1055,  ..., -2179.9070, -2104.1621,\n",
            "        -1106.9648])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1964.6293,  -549.3846, -1682.0641,  ..., -1953.6259, -1241.1863,\n",
            "          427.2379])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -23.8297, -1719.7952, -2668.9851,  ..., -1107.4797,  -805.4058,\n",
            "          317.7078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1831.7875,    -7.2705, -2482.8179,  ...,    10.5670,   -91.4031,\n",
            "        -2226.3142])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -758.7215,  -694.4246, -1233.0399,  ..., -2374.3579,  -486.6404,\n",
            "        -1114.9412])\n",
            "actor loss: 32136.401850957784, critic loss: 25240158.8125, entropy: 66357.2529296875, KL divergence: 0.0005974415086949105\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.4228601615829477], 離散行動：[0, 1], 連続行動：0.1356574296951294\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "51エピソード目の累積報酬：-104002.46814368456, 一つ保全の回数：387, 二つ保全の回数：7802, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  141.5387, -1119.2260, -4059.4380,  ...,  -894.0859, -1523.1324,\n",
            "        -2901.7695])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1233.3988, -1341.4620, -1629.8507,  ...,  -952.1430,    65.0043,\n",
            "        -2287.8069])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1540.3783, -2507.6060, -1621.8107,  ..., -1740.7332, -2081.2461,\n",
            "        -1377.1664])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2838.5547,  -888.2292, -1425.0922,  ...,  -392.4426,  -406.4791,\n",
            "        -1718.1599])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1489.4004,    66.8133, -2339.0281,  ..., -2140.0554,     5.6993,\n",
            "          -53.6779])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -729.9423,  -301.3367,  -789.9207,  ..., -1211.9453, -1739.9478,\n",
            "         -923.8224])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2350.9797,  -413.3633,  -974.0957,  ..., -1482.3160, -2177.2268,\n",
            "        -1071.5006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2196.3625,    56.0456, -2840.2893,  ..., -1122.1089,   750.2009,\n",
            "        -1202.0079])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  121.6691,  -978.7566,  -426.7581,  ..., -1073.5299, -1708.0898,\n",
            "         -367.6621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -997.3733,  -900.6429, -1541.5201,  ...,   391.2058, -1210.8752,\n",
            "        -1997.2195])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1777.1968, -1467.7596, -1478.7118,  ..., -1046.1747, -1260.4275,\n",
            "         -639.8621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1111.3118, -2350.5234, -1001.7056,  ...,   357.3913, -1484.0962,\n",
            "         -382.2383])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -720.4083,  -809.6929,  -308.3497,  ..., -1263.0676, -1095.1085,\n",
            "         -599.4148])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -357.4291, -1494.9004,  -129.3196,  ..., -1747.7826,  -149.6168,\n",
            "         -483.4681])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1491.7731, -3630.1262,  -940.7205,  ...,  -652.0409, -1372.7178,\n",
            "        -1432.9012])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1350.9711,  -806.3709, -1961.7711,  ..., -1387.9034,  -506.3284,\n",
            "          639.5447])\n",
            "actor loss: 32362.586387877924, critic loss: 24587534.8125, entropy: 65421.246826171875, KL divergence: 0.0008316445122658419\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.385283800869059], 離散行動：[0, 1], 連続行動：0.7005417048931122\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "52エピソード目の累積報酬：-96166.6048330401, 一つ保全の回数：388, 二つ保全の回数：7801, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1697.9904,  -511.1212,  -695.9784,  ..., -2064.3467, -1135.1868,\n",
            "         -923.2488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -885.0438, -1660.8992,  -865.2033,  ..., -1124.6421, -1414.4382,\n",
            "         -719.7975])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  502.2494,  -997.7355, -1228.8627,  ..., -1059.0530,  -242.2886,\n",
            "        -2584.5779])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1615.6432,  -983.9154,   175.7778,  ..., -1266.7677, -1181.2551,\n",
            "        -1539.4852])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1735.5631,  -925.9633,  -429.9281,  ..., -1861.7859,  -776.8279,\n",
            "         -602.7372])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -532.8455, -1071.9567,   443.2387,  ..., -1212.1610,    73.5249,\n",
            "          389.1158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -570.7356,    19.6414,  -811.0850,  ..., -2748.5266, -1221.3483,\n",
            "           47.4368])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -756.6995, -2055.5898, -1844.9705,  ..., -1035.2285, -1224.1602,\n",
            "         -513.2617])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  407.7063,  -330.3465,  -829.5580,  ...,  -140.3846,   212.0027,\n",
            "        -2538.6672])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1455.2252,  -273.4847, -1052.1072,  ..., -1828.8912, -1107.0647,\n",
            "        -1304.5471])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1865.1296,  -612.4567,    17.6820,  ...,  -212.4541,   741.7101,\n",
            "         -682.3729])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  248.1654, -1347.4244, -2838.0271,  ...,  -631.2381,  -957.7850,\n",
            "          157.5259])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -654.6788,  -301.6382, -3413.1350,  ..., -2185.6113, -1614.0289,\n",
            "        -1045.7540])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -398.4839, -1245.4424,  -731.7494,  ..., -1018.8395, -1688.4106,\n",
            "        -1476.5004])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2219.3906,  -858.3400, -2415.5310,  ...,  -839.9421,  -966.4350,\n",
            "          259.8839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1361.9691, -1367.5228,   635.6115,  ..., -2391.5674,  -875.9764,\n",
            "        -1317.1780])\n",
            "actor loss: 31976.50202702871, critic loss: 22980734.9375, entropy: 64977.556884765625, KL divergence: 0.0007310549506607486\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.4285015105081196], 離散行動：[0, 1], 連続行動：0.7694457173347473\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "53エピソード目の累積報酬：-106750.30119130989, 一つ保全の回数：334, 二つ保全の回数：7856, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  199.2540,  -352.2850, -1249.8351,  ...,  -993.3795,   -65.5339,\n",
            "         -543.6780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -630.5210, -1542.4067, -1706.8635,  ...,  -752.4267,  -838.1804,\n",
            "        -2169.4202])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1194.6923,  -559.2238,  -434.6512,  ...,   173.3915, -1745.5402,\n",
            "          218.2793])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-3567.4084,  -759.1017,   425.4404,  ..., -1480.5366,  -470.7537,\n",
            "         -337.7206])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -963.8068, -1021.0085,  -436.1856,  ...,  -848.9986, -1041.4089,\n",
            "        -1575.5947])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -453.9454,   539.9434, -2016.9902,  ..., -1657.4971, -1316.7701,\n",
            "         -271.6644])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1474.8975,  -668.5446,  -257.3619,  ...,  -721.2238,  -516.3699,\n",
            "          837.7393])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -525.9792,  -982.4311,  -856.3336,  ..., -1171.6409, -1332.8079,\n",
            "         -219.8190])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -205.3963, -1243.0361,  -678.6976,  ...,  -131.5295,  -723.4753,\n",
            "         -953.3574])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1593.7113,  -936.3051,   136.0605,  ..., -1575.3915, -2279.6245,\n",
            "        -1716.0250])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -17.6965,    34.3467,  -759.6368,  ..., -2119.7957,  -711.2576,\n",
            "          -48.9026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -649.4214, -1155.8463, -1436.5114,  ...,  -941.6296,  -829.3221,\n",
            "         -558.1472])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1103.6644, -1241.2849, -1646.4436,  ..., -2406.1223,  -761.6093,\n",
            "         -200.9568])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -499.2299,  -749.9548,  -634.3605,  ..., -1284.3275, -1679.3937,\n",
            "           23.7537])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -953.4003,  -142.7318, -1604.1456,  ..., -1070.2931, -1266.4001,\n",
            "         -471.1025])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -937.4777, -2114.7205,  -746.0258,  ...,  -103.7889, -1004.8873,\n",
            "        -1243.9257])\n",
            "actor loss: 31740.795592832954, critic loss: 22416888.625, entropy: 64322.4248046875, KL divergence: 0.0005757946197848424\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2664950490400462], 離散行動：[0, 1], 連続行動：0.4145920127630234\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "54エピソード目の累積報酬：-98507.49532999632, 一つ保全の回数：285, 二つ保全の回数：7902, 三つ保全の回数：5, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1221.1504, -1902.3501, -1226.1216,  ...,  -509.1613,  -959.2664,\n",
            "          -94.0667])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -902.8511, -1965.1549,  -994.3300,  ...,  -773.2670,  -544.1717,\n",
            "        -1466.2227])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -606.4202, -2307.9280,  -765.9691,  ...,  -898.6259,  -753.6821,\n",
            "         -914.9899])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1004.7295, -1442.0552, -1597.6050,  ...,  -520.9911, -1148.8795,\n",
            "         -730.7388])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -179.1771, -1287.0599, -1813.2330,  ..., -1267.6958, -2464.1782,\n",
            "        -1037.2534])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1066.7227, -2890.5393, -1357.4719,  ..., -2926.1104, -1261.8726,\n",
            "          301.8164])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1012.5723,  -789.1102,   264.5917,  ..., -1432.6401,  -843.5188,\n",
            "        -1917.3813])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   55.5205,  -135.0322, -1973.6237,  ..., -2323.4146, -1263.8831,\n",
            "         -857.2114])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -388.3090, -1877.6764, -2275.5793,  ...,  -263.6093,  -709.7817,\n",
            "         -945.7186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -830.2999,  -334.8331,  -784.9741,  ...,  -805.1493,  -643.3596,\n",
            "        -1236.4917])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -625.3981,  -172.1416,  -389.5127,  ..., -1097.6462,  -826.3126,\n",
            "        -1540.2042])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -578.8504,  -477.5143,  -974.7928,  ...,  -993.0850,   489.5625,\n",
            "        -1166.7311])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -64.3096,  -928.8077, -1236.5946,  ...,  -867.2396, -1175.1287,\n",
            "         -905.6464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -137.1389,   643.7705,  -994.5937,  ..., -1743.4862, -3929.9958,\n",
            "         -330.3925])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1959.1385, -2084.1514,  -846.6675,  ...,  -894.8975, -1338.0282,\n",
            "        -1865.5732])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -948.7880,  -276.0400, -2006.6625,  ..., -1778.4261, -1891.7369,\n",
            "           34.6908])\n",
            "actor loss: 31524.504930119692, critic loss: 21526128.3125, entropy: 64082.725341796875, KL divergence: 0.0003018402238237034\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1418766552194994], 離散行動：[0, 1], 連続行動：-0.9579390287399292\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "55エピソード目の累積報酬：-101918.57953567394, 一つ保全の回数：271, 二つ保全の回数：7920, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1192.8353, -1061.9659,  -951.2289,  ..., -1181.1766, -1727.1195,\n",
            "        -1408.6262])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -615.0803, -1441.0720, -1340.5441,  ...,  -840.6284, -1128.2971,\n",
            "        -1748.9066])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1566.0839,  -580.4852,  -193.1159,  ..., -1017.0152,  -328.3380,\n",
            "        -1855.2358])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1684.8706,  -114.7702, -1069.7642,  ...,  -384.9051,  -530.1983,\n",
            "        -1092.2328])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1203.4088, -1039.4507,  -950.9726,  ...,   239.1809,  -101.0950,\n",
            "         -489.4706])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1486.6720,   232.9170,  -115.4006,  ..., -2141.9495, -1170.5673,\n",
            "          545.9037])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -579.8276, -1139.5287, -2615.7402,  ...,  -632.2068, -1535.3967,\n",
            "         -535.0839])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   82.7746,  -654.6283, -1007.7943,  ..., -1306.1108,  -680.3841,\n",
            "          178.2955])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2065.6313, -1048.6461, -1613.7426,  ..., -2430.2234, -1139.3596,\n",
            "         -553.8108])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -104.8084,  -216.8481, -1502.6500,  ..., -1407.4523,  -506.7421,\n",
            "        -2021.2328])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1189.3589,   298.7468, -1818.6525,  ...,  -625.2103, -1252.9175,\n",
            "        -1702.4159])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2546.4678, -2147.8264,  -537.4473,  ...,  -587.1050,  -761.2800,\n",
            "         -516.2961])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   55.7438, -1416.2738,  -200.4579,  ...,  -561.8456,   461.9219,\n",
            "        -1337.5150])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -515.9454, -1348.1692,  -760.9266,  ...,  -910.9756, -1036.5342,\n",
            "         -763.1654])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -460.3298, -1535.3967, -1290.9678,  ..., -1391.3120,  -498.0142,\n",
            "         -585.9069])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -759.8180, -1613.7426,   -77.2113,  ...,  -821.4852,  -220.6390,\n",
            "         -448.9823])\n",
            "actor loss: 31309.4599158989, critic loss: 21677288.8125, entropy: 63778.916015625, KL divergence: 0.0012670098897459394\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1898572921190547], 離散行動：[0, 1], 連続行動：0.5932084619998932\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "56エピソード目の累積報酬：-104141.95328776432, 一つ保全の回数：245, 二つ保全の回数：7943, 三つ保全の回数：4, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1045.0264,  -888.7768, -1333.8997,  ...,  -279.2377, -1135.3906,\n",
            "         -945.1929])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2501.5581,   -85.2773,  -213.9834,  ...,   193.6001, -2311.4888,\n",
            "        -1461.8756])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -727.2222, -1662.6205,     7.4782,  ...,  -932.2414, -2538.7061,\n",
            "        -1150.7931])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -459.7404, -2019.9789, -2159.0786,  ..., -1606.9913, -1507.4636,\n",
            "        -1401.0332])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -339.0939, -1180.2805,  -393.2885,  ...,   -13.6547,  -670.5501,\n",
            "         -875.7889])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1025.7108,  -832.0418, -2221.8035,  ..., -1557.9639, -1570.8666,\n",
            "        -1240.6433])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1407.1066,  -823.2872,  -691.5674,  ...,  -598.1563,  -669.9489,\n",
            "         -512.0032])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  801.3940, -1330.0610, -1896.3853,  ..., -1042.0515,  -673.3649,\n",
            "         -650.5909])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1964.6979,  -513.2473,   278.7148,  ..., -1795.9878, -2337.5151,\n",
            "         -312.3078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -258.9698,  -183.0969,  -480.2581,  ..., -1978.7513,  -875.7889,\n",
            "        -1207.8250])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -891.0694,  -452.3793, -1614.9457,  ..., -1564.9692,   209.1805,\n",
            "           24.7180])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -219.7886, -1072.4193, -1240.4338,  ..., -1081.6910, -1384.5046,\n",
            "         -397.7961])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1467.8142,  -470.5623,  -911.5784,  ...,  -644.2643, -2378.2017,\n",
            "        -1164.6422])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2106.3440, -1195.8795, -1176.6702,  ...,  -559.4613,  -826.5956,\n",
            "         -828.8572])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1293.4346,    -6.5184, -1174.4109,  ..., -1086.8911,  -371.7735,\n",
            "        -1661.1223])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 113.1614, -175.3221, -755.1044,  ..., -837.4842, -872.4869,\n",
            "        -654.7061])\n",
            "actor loss: 31145.304146787617, critic loss: 20939705.1875, entropy: 64040.991943359375, KL divergence: 0.0019794087122139868\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6333465004199699], 離散行動：[0, 1], 連続行動：0.0005778670310974121\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "57エピソード目の累積報酬：-102817.86917662766, 一つ保全の回数：211, 二つ保全の回数：7980, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -939.8322, -1744.8148,   130.4852,  ...,   341.5115,  -922.0259,\n",
            "        -1197.0424])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1208.5436, -1298.7461, -1208.3329,  ..., -2356.0457,  -658.6708,\n",
            "         -734.2914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -910.3864,  -996.5828, -1631.1666,  ...,  -253.5452,  -694.1484,\n",
            "         -878.5903])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -201.1070, -1739.0931,  -993.0941,  ...,  -809.7226, -1021.8016,\n",
            "        -1310.7067])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-824.2225,   19.9285,  433.9798,  ..., -860.7595,  611.7681,\n",
            "        -866.7739])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1929.6111,  -640.8025,    12.1091,  ..., -1289.6794, -1444.3315,\n",
            "        -2802.0818])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -251.0465, -1159.8042, -2203.4941,  ..., -2308.5078,  -566.0653,\n",
            "        -1397.2582])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1504.4816,  -407.4615, -1814.9462,  ...,   115.3878,  -697.4313,\n",
            "         -818.7603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ 168.3468, -972.4768,  -92.5947,  ...,  -89.3609, -525.4973,\n",
            "         536.0377])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1256.0002, -1289.6794, -1147.1067,  ..., -1310.9497,  -824.7565,\n",
            "          514.3638])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -656.3080, -1000.4681,   115.2758,  ..., -1113.8586,  -205.8334,\n",
            "        -1455.8877])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -545.1376, -1717.2109,  -700.9252,  ..., -1867.2953, -2098.4729,\n",
            "         -417.0304])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -768.1587, -1522.0101,  -822.5602,  ...,  -834.0800,  -968.7106,\n",
            "         -941.0745])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1093.3981,  -292.9309, -1485.0469,  ...,  -241.4185,  -849.0038,\n",
            "          499.2479])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  489.2016, -1506.1475,   -34.5047,  ..., -1852.0775,    70.0135,\n",
            "        -2559.0186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2197.1982,  -716.1343, -1159.1304,  ..., -2182.4939,  -577.9575,\n",
            "         -883.7083])\n",
            "actor loss: 31438.163063080025, critic loss: 20788491.5625, entropy: 63921.66162109375, KL divergence: 0.0022659605892163754\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.180092744734455], 離散行動：[1, 1], 連続行動：-0.8420867919921875\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "58エピソード目の累積報酬：-104321.71058552583, 一つ保全の回数：250, 二つ保全の回数：7939, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1307.0160,  -723.1361, -1563.6561,  ..., -1579.9287, -2008.9680,\n",
            "        -1098.9943])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1336.4559,  -606.3016, -1040.7494,  ...,   -54.1241,   -72.5385,\n",
            "        -1483.3712])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -747.7708, -1092.1167, -1678.9305,  ..., -1600.7590, -1257.4877,\n",
            "        -1169.9343])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1552.7920, -1034.5598, -1163.5261,  ...,  -865.9369, -1587.5891,\n",
            "        -1628.3896])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -223.0941,  -576.9412,  -294.9453,  ..., -1055.9575,  -843.2900,\n",
            "         -568.8292])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1046.7892, -1892.5886,  -245.2049,  ...,  -925.8307,  -494.3929,\n",
            "        -2500.9006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -512.5984, -1159.1394,  -753.0733,  ...,  -787.7122,  -184.2744,\n",
            "        -1331.1293])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1623.8455, -1943.7223, -1781.2517,  ...,  -754.0189,  -582.4614,\n",
            "        -1286.4989])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -971.2278, -1149.7285, -1199.9683,  ...,  -222.1074, -1614.7379,\n",
            "         -450.3770])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -432.5593,   121.6450, -1242.8164,  ..., -1428.2550, -1205.3514,\n",
            "        -1322.2069])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1529.1493, -1103.3668, -1126.0914,  ...,  -724.6965,  -978.3304,\n",
            "          217.8359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -904.4227, -2295.9854, -1218.3108,  ..., -2216.3330,   -45.0843,\n",
            "        -1105.9209])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -355.3979,  -223.0941, -1573.0800,  ...,  -514.9771,  -709.6718,\n",
            "         -751.2758])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   94.6912,  -825.9969, -1329.8986,  ..., -1235.3127,  -942.0548,\n",
            "         -490.1397])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1114.4009,  -645.7493, -1244.7234,  ..., -1011.7620, -1702.9325,\n",
            "         -432.2060])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1147.1925,   337.7637, -1358.4602,  ...,  -886.6483, -1613.4628,\n",
            "        -1015.8151])\n",
            "actor loss: 31509.287031927983, critic loss: 21056353.5625, entropy: 63564.771728515625, KL divergence: 0.002840531663505679\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.512834579736733], 離散行動：[0, 1], 連続行動：1.0709522366523743\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "59エピソード目の累積報酬：-100522.61204951012, 一つ保全の回数：200, 二つ保全の回数：7990, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1045.4818,  -397.0212, -1244.5571,  ...,  -513.9661, -1269.3771,\n",
            "        -1204.2654])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1582.1561,  -534.8416,  -961.3535,  ...,  -998.8629,  -189.2994,\n",
            "        -1288.6335])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -82.4043,  -736.0080,  -953.3685,  ...,  -425.4891, -1243.8917,\n",
            "        -1249.3752])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -428.8105,  -569.5721, -1618.7791,  ...,  -581.6653, -1612.3823,\n",
            "          197.3068])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1698.7775, -1842.2784,  -132.7962,  ...,  -884.7433,  -654.9032,\n",
            "         -825.7744])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1957.2466, -1207.8549,  -716.1424,  ..., -1455.9001,  -307.3272,\n",
            "         -105.1800])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1018.1856,  -393.9812,  -975.6918,  ...,  -313.9273,  -144.5222,\n",
            "        -1029.2866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -826.3654,  -574.6629,  -887.0009,  ..., -1080.5167, -1870.4916,\n",
            "        -1266.5015])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1481.1479,  -662.6940, -1348.3746,  ..., -1326.8014,   -10.2146,\n",
            "        -1161.6985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1354.4379, -1115.2836,  -511.0017,  ...,  -874.8234,  -624.9797,\n",
            "        -1397.4480])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1058.0171,  -682.9261,  -976.3594,  ..., -1664.5947,  -227.9834,\n",
            "         -155.3721])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1377.3458,  -490.3145, -1936.7224,  ...,  -679.9031,  -306.0241,\n",
            "         -364.9747])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -690.7484,  -632.3698,  -393.6678,  ..., -2623.4919,  -875.6396,\n",
            "         -688.8095])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -264.7498,   189.6193,  -759.2875,  ...,  -785.4637,  -947.7167,\n",
            "        -2107.7048])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1593.1172,  -166.1673, -1077.1726,  ..., -1115.3861, -1636.4358,\n",
            "         -874.2559])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1224.9612,  -944.9000,    64.9983,  ..., -1024.6819, -1451.2452,\n",
            "          202.6650])\n",
            "actor loss: 31098.771067439386, critic loss: 20365759.0625, entropy: 63210.07275390625, KL divergence: 0.0020768176272207787\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.8240077944271544], 離散行動：[0, 1], 連続行動：0.9188313484191895\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "60エピソード目の累積報酬：-99789.67538278691, 一つ保全の回数：166, 二つ保全の回数：8024, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -419.7387,  -844.8923,  -580.6044,  ...,  -857.4690,  -364.1337,\n",
            "        -1940.3566])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2589.9875,  -514.0657, -1801.9666,  ...,  -724.6260,  -107.3237,\n",
            "         -268.1785])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1167.3544, -1336.8047, -2030.2598,  ..., -1956.9694,  -830.9020,\n",
            "         -663.3182])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -875.3846, -1215.6793,  -589.9540,  ..., -1119.3054,  -855.5308,\n",
            "         -349.7682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -300.1159,  -949.0717,  -703.0350,  ..., -1094.9493, -2025.7766,\n",
            "         -901.7230])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -556.6000,  -576.6100, -1225.5656,  ...,  -573.6534, -1336.8047,\n",
            "         -869.0287])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2553.2913, -1721.3466,  -148.8222,  ..., -1001.1062,  -741.9088,\n",
            "        -1625.6550])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1003.8358, -1852.9945,  -400.5544,  ...,  -356.3637, -1492.0847,\n",
            "        -1197.6071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1756.8857, -1223.3684,  -947.3871,  ..., -1062.4542, -1542.0845,\n",
            "         -437.6284])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -543.2092, -1363.1130,  -837.3939,  ..., -2781.4150, -1322.4149,\n",
            "        -1186.9580])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -900.0666,  -732.2377, -1491.7812,  ...,  -787.7727, -2006.5552,\n",
            "         -925.9212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  182.0483,  -677.3398, -1946.4225,  ..., -1650.8469, -1843.6925,\n",
            "         -565.2014])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-752.9300, -720.0029, -998.6096,  ...,  -16.2974, -996.3429,\n",
            "        -163.5159])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1937.5009, -1812.3038, -1461.5778,  ..., -1203.4448, -1858.2928,\n",
            "        -1012.7167])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -638.6909,  -493.1207, -1614.4962,  ..., -1703.5858,  -641.5015,\n",
            "        -1910.9221])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2317.6089, -1405.0033, -1228.9161,  ...,  -383.8818, -1569.2815,\n",
            "         -801.3986])\n",
            "actor loss: 31058.188548656024, critic loss: 20012193.5625, entropy: 62740.783203125, KL divergence: 0.0012375969721211061\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2396100976687752], 離散行動：[0, 1], 連続行動：0.7651311159133911\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "61エピソード目の累積報酬：-99046.331479606, 一つ保全の回数：144, 二つ保全の回数：8046, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1387.0409, -2470.9868,  -761.4295,  ...,  -519.0890,  -346.8974,\n",
            "         -774.9688])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -550.0524, -1324.0024,  -704.2210,  ...,  -724.6301, -1431.1240,\n",
            "         -831.9763])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -866.6631,  -729.8390,  -427.4224,  ..., -1237.1543, -1096.8988,\n",
            "         -909.1653])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1591.8859, -1055.1792, -1416.3759,  ...,  -620.5927, -1047.4678,\n",
            "         -239.2599])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1071.3052,  -423.4918,  -700.2510,  ...,  -438.8925,  -963.4478,\n",
            "         -104.2422])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -276.1515,  -198.0735, -1309.5929,  ...,  -450.3721, -2117.4316,\n",
            "        -1240.0243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1256.8058, -1679.9989,  -979.8427,  ...,  -322.0201,  -642.0469,\n",
            "         -412.4560])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1005.4703, -1128.3108, -1526.5369,  ..., -1044.9196,  -973.8572,\n",
            "         -888.7778])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1696.2893, -1297.2496, -1820.1497,  ..., -1532.0161, -1080.6942,\n",
            "         -255.3539])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -388.1071, -1307.5343,  -940.6424,  ..., -1549.8802,  -377.6396,\n",
            "          195.3544])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2322.5012, -2053.2349, -1995.2423,  ..., -1280.9817,  -647.1701,\n",
            "         -763.3892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1096.4993, -1189.2297, -1494.9188,  ..., -1514.3110,  -683.8845,\n",
            "         -227.8924])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1457.4812, -1244.1641,  -936.2831,  ..., -1686.7209,  -327.6268,\n",
            "        -1676.8145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  173.9303,  -424.1874, -1286.1028,  ...,  -259.5161,  -128.1142,\n",
            "        -1081.1910])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -29.2419,  -931.1710,  -846.0090,  ...,  -725.6797, -1965.0023,\n",
            "        -1083.2786])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1738.0073, -1988.0652, -1155.5345,  ...,   172.4299,  -530.2526,\n",
            "        -1624.7000])\n",
            "actor loss: 31003.060067199534, critic loss: 20027654.21875, entropy: 62741.622802734375, KL divergence: 0.0015689319864680455\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.178995272464314], 離散行動：[0, 1], 連続行動：0.11529219150543213\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "62エピソード目の累積報酬：-100118.23959246418, 一つ保全の回数：140, 二つ保全の回数：8051, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -46.0351,  -545.8392,  -409.1634,  ...,  -565.9209,  -387.2854,\n",
            "        -1067.9589])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1532.9846,  -726.7066,  -757.4484,  ...,  -672.6499,  -515.5671,\n",
            "         -925.2518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1055.9197, -1087.8617, -1866.9211,  ..., -1007.5394,  -759.5563,\n",
            "        -1271.1145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -818.7877,  -988.2660, -1333.5244,  ..., -1013.7147,   163.8443,\n",
            "        -1230.6161])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1269.3358, -2067.3911,  -874.2255,  ..., -1297.0258, -1236.3707,\n",
            "        -1511.3588])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -418.0300,  -102.9636, -1560.6971,  ...,  -459.3612,   -16.5442,\n",
            "        -2231.8628])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   68.2117,  -652.3371,  -566.7029,  ..., -1322.2000,  -368.9713,\n",
            "        -1287.9666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1440.7987,  -635.0967, -1130.5748,  ..., -2276.6646, -1199.3190,\n",
            "        -1502.0306])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1582.7875, -1320.6602,  -903.0864,  ...,  -961.5438, -1163.2319,\n",
            "         -453.8057])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   33.0121,  -747.4540, -1304.9630,  ..., -1645.8602,  -981.9055,\n",
            "         -793.9083])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1047.4745, -1851.0393,  -930.9863,  ..., -2546.8728,  -172.9206,\n",
            "         -533.4158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -438.0776, -1851.2666, -2050.3713,  ...,  -564.8420,  -793.8700,\n",
            "         -217.8294])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -643.6053, -1046.6769, -2554.5603,  ..., -1191.3254,  -342.8758,\n",
            "         -731.2892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -521.1188, -1208.7269, -1084.8378,  ..., -1150.6252,   248.2875,\n",
            "         -593.1603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -852.1530,  -611.5908, -1006.7311,  ..., -1079.7928, -1081.5762,\n",
            "        -1437.9985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -917.3647,  -937.5914,  -464.7902,  ..., -1231.9703, -1270.4896,\n",
            "           54.7240])\n",
            "actor loss: 30980.17899681282, critic loss: 20447960.40625, entropy: 64371.638427734375, KL divergence: 0.006164345388548553\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5804425119612668], 離散行動：[0, 1], 連続行動：-1.903024673461914\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "63エピソード目の累積報酬：-104081.58070361186, 一つ保全の回数：130, 二つ保全の回数：8062, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -198.7989, -1444.0261, -1101.3625,  ...,  -304.8627, -1130.0767,\n",
            "         -941.7403])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -262.0035,  -593.9490,  -915.6040,  ...,  -950.2645, -1010.5576,\n",
            "         -850.4151])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1097.8831,  -952.0962, -1141.6824,  ..., -1090.3241, -1234.8291,\n",
            "         -609.9142])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -897.9424, -1279.3947, -1063.7700,  ...,  -912.9721,   -53.5933,\n",
            "        -1764.8651])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -351.7390, -1768.8329,  -305.2906,  ...,  -915.0435,  -784.3766,\n",
            "         -730.2004])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1759.7095,  -871.8828, -1224.2229,  ..., -2880.9690, -1593.5898,\n",
            "        -1747.5258])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1802.4312, -2094.2556, -1469.0237,  ...,   139.2941, -1009.5287,\n",
            "         -892.7460])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1418.1199,  -566.8886, -1093.5248,  ..., -1170.1196, -1130.0767,\n",
            "         -869.8918])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1497.7555, -1075.2336,  -504.4851,  ...,  -717.7641,  -869.8918,\n",
            "        -1794.4058])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2225.9434,  -414.2012,   -68.4326,  ...,  -819.7789, -1164.7694,\n",
            "        -2230.3577])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2062.0139,  -158.1711, -1070.1919,  ...,  -604.4029,  -738.7036,\n",
            "          185.1658])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -763.4258,  -661.0084,  -659.4680,  ...,  -842.9827, -1315.7377,\n",
            "        -1260.4132])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  413.8163,  -993.3848, -1847.3607,  ...,  -662.8348,  -165.7508,\n",
            "        -1960.1566])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1480.7733, -1670.1368,  -944.7151,  ...,  -948.7487,  -746.1090,\n",
            "          -57.4892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1114.6761, -1647.5237,  -524.0994,  ..., -1083.8928,  -824.9202,\n",
            "         -678.2236])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1485.1195, -1188.8250,    81.9556,  ...,  -877.8062,  -829.6448,\n",
            "         -887.8781])\n",
            "actor loss: 31104.589367164546, critic loss: 20562319.09375, entropy: 64169.6552734375, KL divergence: 0.003138181904168625\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7921633611706307], 離散行動：[0, 1], 連続行動：-0.40907812118530273\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "64エピソード目の累積報酬：-98721.91947568528, 一つ保全の回数：100, 二つ保全の回数：8092, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -190.7769,  -742.5676,  -113.0682,  ..., -1192.2982,  -332.0291,\n",
            "        -1675.8497])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1915.9062,  -486.9342, -1313.1285,  ..., -1411.3411,  -871.7909,\n",
            "         -723.1324])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -331.9043,  -975.2339, -1457.6183,  ..., -1102.7487, -1077.4622,\n",
            "           57.1840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1362.2512, -1126.8138, -1088.0323,  ..., -1725.8483, -1150.1611,\n",
            "        -1293.2539])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1012.9916,  -608.2803, -2148.0046,  ...,  -347.5106, -1430.6310,\n",
            "        -1434.6355])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -380.3784, -1010.5966, -1320.6359,  ..., -1435.8447, -1322.7948,\n",
            "        -1407.2573])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1415.5646,  -574.5500,  -762.5370,  ...,  -950.4450,  -992.0898,\n",
            "        -1806.9984])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -841.1589,  -984.2679,  -658.4229,  ...,  -890.1540, -2046.8899,\n",
            "        -1330.0021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1194.0007,  -572.7772,  -395.2541,  ..., -1035.1927, -1350.3627,\n",
            "         -626.4448])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -432.3241, -1782.9744, -1329.3923,  ..., -1030.8756, -1167.0496,\n",
            "        -1081.0500])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -404.5224,  -262.8017, -1268.0441,  ..., -1092.3627, -1034.8715,\n",
            "         -509.2683])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -410.8380, -1353.2119, -1476.0093,  ...,   105.4925,  -534.1157,\n",
            "        -1506.1655])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1573.9836, -1554.3501,  -664.4918,  ..., -1712.7306, -1996.5043,\n",
            "         -918.4915])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -975.2339, -1407.9623, -1372.6179,  ...,  -376.2883,  -879.3582,\n",
            "         -900.9877])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1349.0521, -1016.3248, -1045.9760,  ...,  -977.4081, -1135.5845,\n",
            "        -1295.3003])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -469.0220,  -375.3375, -1316.8229,  ..., -1289.8944, -1883.2916,\n",
            "        -1614.1837])\n",
            "actor loss: 30977.363881837045, critic loss: 20442670.46875, entropy: 63714.051513671875, KL divergence: 0.0016280468674152576\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7396606555464258], 離散行動：[0, 1], 連続行動：-0.07341921329498291\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "65エピソード目の累積報酬：-103869.68089237156, 一つ保全の回数：118, 二つ保全の回数：8073, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1121.0011, -1405.3182,  -939.2202,  ..., -1039.1090, -1125.7748,\n",
            "         -521.5666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -424.3856, -1104.4784, -1104.6178,  ..., -1662.5215,  -585.6761,\n",
            "        -1154.5997])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1991.4772, -1247.1964, -1175.9830,  ..., -2171.4194,  -802.7289,\n",
            "        -1018.2986])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -460.9606, -1598.8855,  -970.2684,  ..., -1402.5754,  -583.5573,\n",
            "         -196.1579])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -967.0144, -1194.4918,  -908.3663,  ...,  -329.0227, -1172.5876,\n",
            "         -626.7684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1285.4731, -1269.1425, -1537.0563,  ..., -1048.6937, -1726.4576,\n",
            "         -796.2251])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -738.3555, -2200.4446,  -986.5978,  ..., -1771.9371,  -647.9214,\n",
            "        -1190.5004])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1362.1982, -1043.1267, -1468.7727,  ...,   -54.5167, -1551.6698,\n",
            "        -1250.3071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1100.3707,  -606.5840,   407.2242,  ...,  -183.8609,  -843.2186,\n",
            "         -932.7819])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1032.3835, -1884.9139, -1726.5985,  ..., -1277.8990,  -633.2886,\n",
            "         -585.4123])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2231.0010,  -968.3923,  -567.5364,  ...,  -605.7421, -1118.8898,\n",
            "        -1324.0891])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1263.3245,  -501.2952, -1058.4247,  ..., -2338.3865,  -640.0168,\n",
            "         -955.4358])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1156.2185, -1549.5553,  -625.2352,  ...,  -499.0175, -1343.0381,\n",
            "        -1023.9763])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1411.5475,  -909.8307,  -847.0029,  ...,  -213.7705, -2203.7629,\n",
            "         -525.7026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1295.9541,   -10.8301, -1344.9415,  ...,  -564.2117,  -847.2645,\n",
            "         -663.7488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -415.4867, -1492.1082,  -878.0395,  ...,  -971.8471, -1931.4220,\n",
            "         -611.7603])\n",
            "actor loss: 30724.916334727855, critic loss: 20123395.125, entropy: 63042.654296875, KL divergence: 0.0016338371079678761\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7052001458331376], 離散行動：[0, 1], 連続行動：0.43197086453437805\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "66エピソード目の累積報酬：-102134.9933516874, 一つ保全の回数：89, 二つ保全の回数：8103, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -478.9613, -1770.7606, -1237.6840,  ...,  -992.0245,  -388.5135,\n",
            "           67.8978])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -765.9638,  -627.2302,  -338.0842,  ...,  -514.6202, -1069.3416,\n",
            "        -2244.9426])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -408.2709,  -380.2588, -1320.0145,  ...,  -151.6739,  -643.4106,\n",
            "         -349.5557])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1343.1451,  -876.5038, -1658.7520,  ...,   -39.0240, -1197.0137,\n",
            "        -1609.2419])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2098.4712,  -941.3118,  -979.0426,  ...,  -988.3455,  -739.1198,\n",
            "         -501.9856])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -849.3248, -1430.9882,  -597.8896,  ...,  -448.1436,  -570.8695,\n",
            "         -939.4074])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1064.5797,  -272.8078,  -620.3130,  ..., -1497.4875, -2146.3435,\n",
            "        -1411.5667])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2022.7625, -1869.5770, -1529.1259,  ...,  -162.0184,  -808.2491,\n",
            "        -1011.3255])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1536.7830,  -695.1350,  -701.6606,  ...,  -390.7812,  -265.0530,\n",
            "        -1430.9882])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -982.9606,  -338.0842, -1278.2136,  ...,  -301.2771,  -909.1661,\n",
            "        -1370.8079])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1032.2557,  -948.8174,   -10.4846,  ...,  -769.8340, -1144.1605,\n",
            "         -859.3054])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -564.1967, -1059.6078, -1395.5928,  ..., -1692.4850,  -716.4099,\n",
            "           67.8978])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1707.8929, -2421.6968,  -931.4230,  ..., -1293.3160, -1570.9742,\n",
            "         -859.3054])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2054.8098,  -320.2038,  -973.5482,  ..., -1426.9622,  -796.2955,\n",
            "        -2248.4717])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.2335e+03, -1.3963e+03, -1.3146e+00,  ..., -3.2055e+02,\n",
            "        -1.9025e+03, -1.1383e+03])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -473.6886, -1061.7413,  -151.9198,  ..., -1755.2507, -1982.7178,\n",
            "        -1395.5928])\n",
            "actor loss: 30947.90990017976, critic loss: 20345366.15625, entropy: 62983.458740234375, KL divergence: 0.001563592518982273\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9505443935579652], 離散行動：[0, 1], 連続行動：0.47083889693021774\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "67エピソード目の累積報酬：-103367.93143996983, 一つ保全の回数：76, 二つ保全の回数：8116, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1477.5205,  -960.9067, -1529.2025,  ..., -1348.4952, -1036.4159,\n",
            "          283.3085])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2022.2615, -1379.0212, -1084.3370,  ...,   198.9795,  -635.3643,\n",
            "        -1712.6721])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -635.7986,  -485.0408, -1198.8317,  ...,  -522.7546, -2191.7239,\n",
            "         -904.1375])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1701.8835, -1549.7777, -1163.4506,  ...,  -567.6232, -1654.2432,\n",
            "         -654.6281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -890.2161, -1655.5026,  -463.5855,  ...,  -569.4399,  -833.5448,\n",
            "         -784.8764])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1575.8313,  -751.3182,  -591.8998,  ..., -1120.5009, -1135.4353,\n",
            "         -753.4081])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1828.2810, -1502.7826,  -298.4279,  ...,  -418.4225, -1666.1993,\n",
            "        -1756.3728])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -346.5671,  -340.2534, -1474.8374,  ...,  -610.5085,  -765.8907,\n",
            "         -854.5138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -977.5061, -1581.4940,  -911.8652,  ..., -1299.6426,  -481.5375,\n",
            "        -2149.5703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -875.9032,   -77.0298, -1145.6146,  ..., -1246.9398, -1129.4658,\n",
            "        -1416.6033])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1165.9406,   -58.4865,  -693.3277,  ...,  -256.5948, -1228.0348,\n",
            "        -1182.8662])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -645.4529,  -251.9682, -1238.7821,  ...,  -854.7924,  -374.7271,\n",
            "        -1084.2791])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   51.6795, -1131.1046,  -567.6791,  ..., -1171.7452,  -884.4598,\n",
            "         -997.0146])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -824.5859, -1004.6150, -1794.0068,  ..., -1313.1399,  -312.9004,\n",
            "          -82.7518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1662.2806, -1712.6721, -1368.3972,  ..., -1167.0834, -1238.4048,\n",
            "         -697.4771])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1075.6400, -1201.2753, -1128.3109,  ...,  -591.7003,  -651.6318,\n",
            "        -1703.2164])\n",
            "actor loss: 30821.978187175144, critic loss: 20169385.28125, entropy: 62897.291259765625, KL divergence: 0.0018436601334454183\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6228556575037074], 離散行動：[0, 1], 連続行動：1.245238482952118\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "68エピソード目の累積報酬：-99938.68967601882, 一つ保全の回数：71, 二つ保全の回数：8121, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -944.4266, -1054.9852, -1417.1254,  ...,  -271.6235,  -551.4984,\n",
            "        -1822.9841])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -369.3739,  -628.8734,  -564.8196,  ..., -1394.8662, -1145.2430,\n",
            "         -268.9687])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -469.0451,    -5.2584,  -726.9825,  ..., -1533.9250, -1959.2156,\n",
            "        -1081.0214])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   -6.4619, -1698.7402, -1121.7606,  ..., -1022.4064,  -765.6088,\n",
            "        -1032.7234])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1.2784e+03, -9.0380e+02, -1.5039e+03,  ..., -1.4293e+03,\n",
            "        -1.2074e+03,  5.8124e-01])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1857.6222,  -415.6505, -1574.7156,  ..., -1039.6710,  -502.2296,\n",
            "        -1852.3387])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -904.5046,   239.8729,  -618.9315,  ..., -1665.0680,  -702.1057,\n",
            "        -1674.9664])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1231.7860,  -829.8314, -1553.1425,  ..., -1005.2538, -1170.9899,\n",
            "         -524.1288])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1184.9221, -1031.0292,  -524.1505,  ...,  -902.2427, -1068.0579,\n",
            "        -1197.3516])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -522.6365,  -925.2971, -1246.6053,  ...,  -779.1353,  -624.5078,\n",
            "        -1206.7361])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1212.8120, -1142.3802, -1188.5033,  ...,  -914.8194,  -823.0543,\n",
            "         -730.7133])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1322.7307, -1161.5072,  -723.8527,  ..., -1233.6815, -1261.8298,\n",
            "        -1557.8441])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1159.4691, -1898.0153,  -768.4366,  ...,   -10.4914, -1364.0728,\n",
            "         -800.3071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -823.0884,  -848.0247,  -807.8912,  ...,  -860.0269,  -854.0463,\n",
            "        -1536.0653])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -565.9707, -1533.9250,    48.9628,  ...,  -145.6628, -1419.2092,\n",
            "         -870.0934])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1041.4390,    86.3809,  -633.8116,  ...,  -469.4042,  -403.1228,\n",
            "        -1733.0063])\n",
            "actor loss: 30797.02050453549, critic loss: 20306764.40625, entropy: 62623.3408203125, KL divergence: 0.0019147217223668058\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2438233771308882], 離散行動：[0, 1], 連続行動：0.7867212295532227\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "69エピソード目の累積報酬：-103652.74310247296, 一つ保全の回数：64, 二つ保全の回数：8128, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -825.9606,  -215.8387,   -49.7462,  ..., -1018.0948, -1709.6981,\n",
            "        -1253.2916])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -346.9773, -2029.0979,  -854.9398,  ..., -1766.7209,  -489.7690,\n",
            "        -1486.3281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1286.6287, -1785.5044,  -840.7382,  ...,  -703.5887, -1699.1837,\n",
            "        -1123.0813])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-211.5140, -892.6519, -966.5370,  ..., -565.0273, -599.4869,\n",
            "        -936.9579])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -702.4814, -1242.7058,  -479.8778,  ...,  -581.6110,  -730.2363,\n",
            "         -898.5477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -718.8820,  -109.0931,  -294.1043,  ...,  -383.4010, -1448.8778,\n",
            "         -111.4456])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1172.4297, -1312.2316,  -458.4179,  ..., -1421.8610,  -541.5984,\n",
            "        -1574.4432])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -665.1874, -1006.9961,  -965.8830,  ...,  -623.2224,  -518.5474,\n",
            "          224.7703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -619.6270,  -846.7062,   -75.3894,  ...,  -513.7180, -1407.7733,\n",
            "        -2086.0229])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2265.8013, -1375.0580,  -639.6699,  ..., -1090.5460, -1554.8848,\n",
            "        -1487.2328])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -893.0688,  -822.9452,  -804.7430,  ..., -1252.3782,  -806.9125,\n",
            "         -866.5591])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1145.7219, -1287.0278, -1028.9788,  ..., -1689.0349, -1165.4688,\n",
            "         -995.3058])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -757.1236,  -701.9742,  -902.4930,  ..., -1488.1666, -1341.3035,\n",
            "        -1068.5999])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1569.3333, -1219.6754,  -970.1899,  ...,  -410.5423, -1390.6008,\n",
            "         -419.6704])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -892.2881, -1641.0699,  -723.1081,  ...,  -607.2381, -1303.7418,\n",
            "        -1483.8440])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -622.4689, -1331.2952, -1432.5654,  ...,  -404.6689,  -243.5759,\n",
            "          119.2079])\n",
            "actor loss: 30449.957688868883, critic loss: 19993377.84375, entropy: 63557.702392578125, KL divergence: 0.005661505823786171\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.3552177763904065], 離散行動：[0, 1], 連続行動：-0.9154268503189087\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "70エピソード目の累積報酬：-101049.76164176085, 一つ保全の回数：52, 二つ保全の回数：8138, 三つ保全の回数：2, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1696.0107, -1589.3900,  -377.5027,  ..., -1098.4498,  -878.0635,\n",
            "        -1748.8186])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1489.1171, -1253.4010, -1037.4440,  ...,  -786.6998, -1391.5660,\n",
            "        -1102.5729])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -313.0327, -1553.9561,  -968.9099,  ..., -1314.6299,  -794.8166,\n",
            "        -1159.2863])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -309.0111, -1533.6895, -1002.7834,  ...,  -843.8967,  -272.8896,\n",
            "         -522.6845])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -841.4008, -1337.1196,  -300.7161,  ..., -1300.4973,  -957.1870,\n",
            "        -1216.2800])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1489.1171,  -872.9473, -1907.2020,  ..., -1276.1125, -1864.7903,\n",
            "        -1000.8695])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1826.5063,  -260.2428,  -212.0476,  ...,  -724.1749,  -895.5872,\n",
            "         -849.3299])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -363.6560,  -945.3323,  -250.0657,  ..., -1302.9075, -1326.2303,\n",
            "        -2098.1914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1067.8597,   -89.1226,  -996.6548,  ..., -1265.1497,  -641.1316,\n",
            "         -989.2683])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -551.4846,  -938.4945, -1305.0715,  ..., -1355.1823,  -468.8647,\n",
            "         -307.4367])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -467.3593,  -809.6058, -1625.6681,  ...,  -701.5670,  -121.6055,\n",
            "         -304.2962])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1327.1760,  -215.6823,  -884.7063,  ...,  -639.9870,  -913.4644,\n",
            "         -850.0875])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2042.3215, -2162.5369, -1090.2023,  ..., -1198.9302, -1088.5044,\n",
            "        -1233.4226])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1246.2664, -1391.3180,   -89.1226,  ...,   -94.2195,  -785.2181,\n",
            "        -1537.4506])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -965.8376, -1064.3607, -1019.5416,  ..., -1280.3809,  -954.4410,\n",
            "        -1048.0782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1422.2748,  -767.2419, -1283.1600,  ..., -1452.1302,  -357.8327,\n",
            "        -1644.2002])\n",
            "actor loss: 30543.146731420475, critic loss: 20492892.09375, entropy: 65291.441162109375, KL divergence: 0.0036611071344973944\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6237443871362585], 離散行動：[0, 1], 連続行動：0.3734069764614105\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "71エピソード目の累積報酬：-99361.3629783716, 一つ保全の回数：52, 二つ保全の回数：8140, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -958.2662, -1126.3619,  -803.9016,  ...,  -858.5295,  -201.6947,\n",
            "         -578.5511])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2043.9191,  -583.0844, -1709.6359,  ..., -1085.6160,  -819.0421,\n",
            "         -807.1278])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1334.1777,  -903.6033,  -127.4209,  ...,  -218.5702,  -883.2031,\n",
            "         -471.4034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1227.2080,  -965.5900,  -149.3557,  ...,  -931.9684,  -474.0847,\n",
            "         -660.3578])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1020.7369,  -975.8566,  -810.5967,  ..., -1359.3368,  -387.5616,\n",
            "        -1218.4305])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   45.6957,  -808.5942, -1401.6384,  ..., -1147.0482, -1331.0804,\n",
            "        -1268.2448])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1349.1737,  -937.7435,  -636.8532,  ...,  -396.1435,  -930.1771,\n",
            "         -538.9549])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1397.1542,  -573.7413, -1513.8328,  ...,  -934.4624,  -638.7561,\n",
            "         -891.7666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1077.5983, -1626.1681,  -179.0689,  ..., -1372.2225,  -877.2469,\n",
            "         -337.3233])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -603.4694, -1225.1410,  -885.1685,  ..., -1365.6821, -1304.9437,\n",
            "        -1319.2126])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1031.2847, -1470.4248,  -258.9152,  ..., -1540.7133,  -879.7605,\n",
            "         -480.5526])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1533.1061, -1445.4995, -1154.8805,  ...,  -599.3881,  -781.5178,\n",
            "        -1578.3365])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -416.1956, -1649.7417,  -586.7096,  ..., -1117.4889,  -258.1292,\n",
            "        -1224.5476])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1328.1073,  -514.4335, -1350.1050,  ..., -1413.6718, -1452.6387,\n",
            "        -1528.0709])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1065.3745, -1020.7369, -1165.3938,  ...,  -468.3645, -1269.2410,\n",
            "        -1442.0154])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1181.7786, -1110.7400, -1079.4819,  ..., -1055.3951,  -537.3989,\n",
            "        -1638.7517])\n",
            "actor loss: 30715.69792320386, critic loss: 20676920.59375, entropy: 65589.46704101562, KL divergence: 0.0032598249228981166\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0415801018824835], 離散行動：[0, 1], 連続行動：0.9261853098869324\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "72エピソード目の累積報酬：-99868.13287115048, 一つ保全の回数：57, 二つ保全の回数：8135, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1547.2910, -1250.2478,  -894.6748,  ..., -1854.9360, -1107.4521,\n",
            "        -1043.3485])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -388.1744, -1000.7103, -1309.3492,  ...,  -923.8621,  -357.8522,\n",
            "         -733.6631])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -180.5739,  -384.5315,  -249.9044,  ..., -1456.2092, -1026.0302,\n",
            "         -898.7012])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1024.0034, -1337.6707,  -481.4317,  ..., -1001.0920,  -488.6460,\n",
            "        -1372.9771])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -306.1019,  -312.2263,  -967.7931,  ..., -1085.5747,  -699.7920,\n",
            "        -1522.0344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -878.2302, -1560.3488, -1233.3195,  ...,  -931.2806, -1297.8707,\n",
            "        -1923.0967])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -702.3795, -1119.7173,  -659.4716,  ..., -1899.6422,  -982.9233,\n",
            "        -1398.5928])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  235.2294,  -326.9161, -1112.1060,  ..., -1442.0859,  -798.3781,\n",
            "        -1078.9730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -358.6799, -2754.3848, -1124.4845,  ...,  -951.6865, -1201.7084,\n",
            "         -866.8948])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -659.8628,  -906.6078,  -240.3425,  ..., -1566.1774,  -384.5315,\n",
            "         -876.4825])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1100.8519,  -428.5616,  -427.2980,  ...,  -616.0261,  -659.3452,\n",
            "        -1014.3454])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1505.7593,  -585.9042,  -951.9199,  ...,  -210.5346,  -805.6689,\n",
            "         -288.4179])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1293.6742, -1504.5142,  -455.6421,  ...,  -815.0950, -1538.6488,\n",
            "        -1107.7212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -783.0184, -1368.7327, -1145.2596,  ..., -1335.4948,  -723.6395,\n",
            "         -449.1115])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -489.2048,  -839.3397, -1546.9722,  ..., -1285.1168, -1106.4675,\n",
            "          -66.6980])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -383.6539,  -863.9305,  -867.0361,  ..., -1253.9768, -1039.0393,\n",
            "         -649.9928])\n",
            "actor loss: 30566.332775113922, critic loss: 20842537.25, entropy: 66093.2890625, KL divergence: 0.003500910264294154\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1308849480658063], 離散行動：[0, 1], 連続行動：0.06468045711517334\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "73エピソード目の累積報酬：-100090.72405743801, 一つ保全の回数：38, 二つ保全の回数：8153, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -764.0018,  -679.2601, -1219.9529,  ..., -1654.1359,  -722.2005,\n",
            "        -1027.9177])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -968.1434, -1388.8280,  -239.8949,  ...,    24.8901,  -243.2566,\n",
            "         -345.3743])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -594.9666, -1566.9530, -1114.3099,  ..., -1742.0608, -1050.5293,\n",
            "          173.0034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1091.5281, -1327.0244,  -240.5611,  ..., -1191.9148, -1056.0801,\n",
            "        -1363.9476])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -724.3926, -1066.0264,  -770.4786,  ...,    28.8776, -1569.1256,\n",
            "         -982.9860])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1150.5487, -1312.6780,  -754.8008,  ...,  -925.9019,  -919.5991,\n",
            "          146.9559])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -687.3686,  -757.3087,  -728.5217,  ..., -1257.3427,  -591.1064,\n",
            "        -1520.1953])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -248.5616,  -583.9367, -2008.6681,  ..., -1804.3418,  -697.8526,\n",
            "        -1022.7646])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -368.8859,  -703.6013,  -879.8343,  ...,  -349.8995, -1425.2023,\n",
            "         -768.9150])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1345.3252, -1742.3179, -1071.6543,  ..., -1198.2089, -1000.7953,\n",
            "        -1022.8911])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -822.9172,    71.0078,  -802.2704,  ...,  -797.3428, -1658.0122,\n",
            "         -553.1967])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1377.1305, -1033.2437, -1555.0125,  ...,  -958.1130, -1810.0714,\n",
            "         -951.2464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -677.1579, -1160.7109,  -181.4896,  ...,  -940.1431, -1250.8523,\n",
            "        -1186.0902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1320.4069, -1356.6588, -1330.7543,  ...,  -981.9307, -1463.0402,\n",
            "        -1275.6958])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1280.2988,  -665.4530, -1588.2296,  ...,  -852.3965, -1487.4508,\n",
            "         -867.2886])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -933.1258, -1256.7175,  -884.9979,  ..., -1084.0262,  -917.8426,\n",
            "         -816.7068])\n",
            "actor loss: 30532.475885609416, critic loss: 21190883.5, entropy: 67379.62353515625, KL divergence: 0.008058909673662818\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0781472811299182], 離散行動：[1, 1], 連続行動：-1.5597825050354004\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "74エピソード目の累積報酬：-98909.97857916649, 一つ保全の回数：60, 二つ保全の回数：8131, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -609.2146, -1508.7140,  -868.4658,  ...,  -945.7084, -1462.8387,\n",
            "        -2102.9607])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -606.6703,  -875.3810, -1305.6346,  ...,    36.0566, -1134.6555,\n",
            "         -745.1752])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1056.9950,  -770.9053,  -914.7481,  ..., -1183.3347,  -337.4046,\n",
            "        -1002.5043])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1135.2950,  -929.7878,  -499.9040,  ..., -1162.6926,  -966.6295,\n",
            "         -719.8600])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1568.1846, -1364.3995, -1246.7435,  ...,  -423.0494, -1873.2139,\n",
            "        -1180.6283])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -723.0081, -1162.2646, -1276.0842,  ...,  -682.1264, -1345.3164,\n",
            "        -1803.3428])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1190.1595,   167.7905, -1186.7710,  ...,  -832.2336,  -973.4009,\n",
            "           28.6945])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -142.2354,  -817.5049,  -881.0917,  ...,  -122.4349, -2180.7756,\n",
            "         -936.3809])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -749.1082,  -831.5618, -1179.5701,  ..., -1596.5042,  -646.1037,\n",
            "         -290.6135])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1131.9086,  -562.1639,  -987.5370,  ..., -1632.0054,  -756.4510,\n",
            "        -1508.6198])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -534.2679,  -601.6392,  -428.5426,  ...,  -471.6646, -1143.6078,\n",
            "         -845.3128])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -754.9803,  -747.4800, -1138.4675,  ...,  -653.8567, -1034.5676,\n",
            "        -1216.1289])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1133.0699,  -902.0191, -1155.9078,  ...,  -995.6216, -1286.8019,\n",
            "        -1411.3752])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -751.6463, -1367.5510,  -452.3752,  ..., -1568.4307,  -987.5910,\n",
            "        -1259.7466])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1233.0431,  -948.0530,  -601.7376,  ...,  -620.5634,  -702.0176,\n",
            "        -1242.1902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2976.4663,  -278.1829, -1840.0765,  ..., -1087.0795, -1123.0465,\n",
            "        -1439.0859])\n",
            "actor loss: 30567.59347064374, critic loss: 20992752.203125, entropy: 69123.482421875, KL divergence: 0.00846453538220226\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.588106692514375], 離散行動：[0, 1], 連続行動：1.527829885482788\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "75エピソード目の累積報酬：-100414.26732769638, 一つ保全の回数：43, 二つ保全の回数：8149, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1124.0420,  -564.4391,  -543.5764,  ..., -1216.4026,  -859.7407,\n",
            "        -1015.0128])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1024.8824, -1314.9564,  -932.7149,  ..., -1586.0210,  -185.7638,\n",
            "        -1096.2827])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1858.1687, -1584.4580,  -382.9518,  ..., -1554.1110, -1204.2393,\n",
            "         -780.7507])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1766.8065,  -830.0714, -1032.7714,  ..., -1417.8193, -1172.6176,\n",
            "         -137.0440])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1093.7942, -1312.5720, -1285.3671,  ...,  -255.4490, -1310.9070,\n",
            "         -215.7243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -772.1736, -1038.4069, -1576.2386,  ...,  -834.8351, -1046.0980,\n",
            "        -2002.6482])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -898.7147,  -608.0154, -1163.9602,  ..., -1467.4070,  -415.1760,\n",
            "        -1691.2483])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1491.0875,  -931.5164,  -917.5653,  ...,  -893.9138, -1565.5736,\n",
            "        -1080.7249])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -722.2697, -1597.2501,  -174.3328,  ..., -1014.9045, -1048.8590,\n",
            "        -1351.4785])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1622.6398,  -784.7425,  -708.5843,  ...,  -975.2410, -1201.4862,\n",
            "        -1145.8824])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -574.9605, -1003.6603,  -996.1529,  ...,  -246.7892, -1529.6790,\n",
            "        -1445.7869])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -787.7866,  -642.1512, -1204.6475,  ...,  -496.2759,  -274.5098,\n",
            "        -1232.2780])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2080.3606,  -608.7128,  -555.0863,  ...,  -496.6070, -1134.5416,\n",
            "         -161.6867])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1396.4485,  -791.3172,  -895.1922,  ..., -1158.5739,  -111.7976,\n",
            "         -708.9671])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -679.4089,   144.7768, -1176.4921,  ..., -1027.1177, -1678.3883,\n",
            "         -246.7892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1147.7194, -1176.2634, -1673.7134,  ..., -1146.5482, -1236.1161,\n",
            "        -1060.8199])\n",
            "actor loss: 30580.715419250144, critic loss: 21207015.484375, entropy: 70089.28564453125, KL divergence: 0.010442450422678983\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0078280748120527], 離散行動：[0, 1], 連続行動：1.7835811376571655\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "76エピソード目の累積報酬：-98344.9974507129, 一つ保全の回数：45, 二つ保全の回数：8147, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -429.6719,  -713.4457,  -924.8514,  ...,  -198.1048, -1067.0240,\n",
            "         -858.0779])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -930.8171,  -741.2426, -1983.9337,  ..., -1097.1790, -1713.1262,\n",
            "         -721.3591])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -298.9253,  -609.3538,  -540.7622,  ..., -1600.3716, -1327.4845,\n",
            "         -788.1204])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -425.4568,  -526.4781,  -874.8582,  ..., -1441.9651,  -497.3114,\n",
            "         -479.2258])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -443.0311, -1474.6595,  -873.7654,  ...,  -964.3409,  -506.1568,\n",
            "         -825.6954])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-427.3029, -971.8895, -456.7526,  ..., -766.1053, -417.5078,\n",
            "        -437.2165])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1057.6235,  -560.2731, -1585.2432,  ...,  -566.2930,  -795.0573,\n",
            "         -758.9512])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1548.0768,  -998.7081, -1108.0178,  ...,  -571.6064,  -917.3568,\n",
            "         -398.2377])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -456.7526,  -888.7416,  -643.7247,  ...,  -871.0674, -1036.4277,\n",
            "         -790.0523])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2014.3403,  -501.4605, -1679.3347,  ...,  -932.9559,  -143.1029,\n",
            "         -951.6741])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([   38.6329,  -947.5557, -1385.9088,  ...,  -976.4271,  -552.3157,\n",
            "        -1011.8060])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -581.3024,  -455.4249, -1421.1097,  ...,  -756.7460,  -239.0171,\n",
            "        -1619.5892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -745.6995,  -408.4652, -1060.0540,  ..., -1259.3918, -1081.3096,\n",
            "        -1213.4999])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1416.8749, -2812.2761,  -872.9653,  ...,  -987.3533,  -724.9391,\n",
            "        -1449.1080])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1508.4385, -1893.9622, -1067.2432,  ...,  -490.5457,  -563.1859,\n",
            "           66.0381])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1159.7131,  -786.0188, -1172.1519,  ..., -1518.4760, -1516.5881,\n",
            "         -357.7892])\n",
            "actor loss: 30584.880763916706, critic loss: 21575220.15625, entropy: 71023.1396484375, KL divergence: 0.0074116294866610095\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.722700691968375], 離散行動：[0, 1], 連続行動：-0.3670751452445984\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "77エピソード目の累積報酬：-101449.5244625862, 一つ保全の回数：46, 二つ保全の回数：8146, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -704.6530,  -907.4568, -1081.7747,  ..., -1209.3459,  -959.8513,\n",
            "         -756.0099])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -944.4525,  -684.3327, -1006.8080,  ...,  -776.7999, -1308.3735,\n",
            "        -1364.6296])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-922.5884, -550.1506, -600.2713,  ..., -788.3436, -453.3180,\n",
            "        -917.7546])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -883.5971, -1027.5574, -1060.5898,  ...,  -364.4319,  -497.3661,\n",
            "        -1913.5951])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1696.1577, -1296.6281, -1593.0918,  ...,  -802.5425,  -916.4060,\n",
            "         -839.2903])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1574.9572,  -695.4529, -1371.9907,  ...,  -992.8830,  -665.0717,\n",
            "         -987.2455])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-752.7609, -562.9878, -647.7029,  ..., -235.0087, -876.0658,\n",
            "        -334.4050])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1307.3949,  -223.3034,  -814.0768,  ...,  -959.7912,  -827.7284,\n",
            "         -817.3454])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -218.9855, -1286.9492, -1019.7220,  ..., -1086.3691, -1039.6204,\n",
            "        -1100.5460])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1454.2098, -1057.7152,  -699.7783,  ...,  -130.7008, -1360.3148,\n",
            "        -1036.5042])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -770.6404,  -997.6080, -1623.6918,  ...,  -942.6550,  -943.1923,\n",
            "        -1063.0535])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -958.3921, -1472.1274,  -666.8503,  ..., -1795.2943, -1206.3171,\n",
            "         -722.5504])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1313.4679, -1594.1929, -1102.2367,  ..., -1743.8146,  -435.5537,\n",
            "         -950.6722])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -897.5992, -1045.8383,  -162.2773,  ..., -1096.0630, -1053.6842,\n",
            "        -1087.5920])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1000.9670,  -868.9193,  -316.6447,  ...,   -25.2606, -1132.3259,\n",
            "        -1010.3459])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -473.4488, -1846.8700,  -990.9254,  ...,  -989.1516,  -930.1440,\n",
            "         -446.6735])\n",
            "actor loss: 30543.492003859275, critic loss: 21528693.046875, entropy: 71355.63330078125, KL divergence: 0.006733574143227357\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5694008001473816], 離散行動：[0, 1], 連続行動：-0.9352321624755859\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "78エピソード目の累積報酬：-99970.66901717412, 一つ保全の回数：36, 二つ保全の回数：8156, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -568.9506,  -756.9009,  -713.2255,  ...,  -804.1569, -1077.7896,\n",
            "        -1321.2034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -936.5912,  -990.0435, -1479.5732,  ...,  -549.5441, -1425.0804,\n",
            "        -1706.7938])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -607.7639,  -866.4447,  -708.9968,  ..., -1571.2253,  -364.6619,\n",
            "         -407.2585])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1008.9897, -1299.9796,  -494.4106,  ...,  -845.7498,  -914.4703,\n",
            "        -1217.3859])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1313.8236, -1391.4709,  -736.1930,  ...,  -854.6036,  -443.4698,\n",
            "        -1466.3326])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -545.2653,  -842.6072,  -490.2722,  ..., -1100.3236,  -740.9159,\n",
            "         -916.3785])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -829.4801,  -837.3248,  -620.8110,  ...,  -782.3290,  -900.1061,\n",
            "        -1149.6592])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -231.6874, -1069.2255,  -355.7548,  ...,  -905.3266,  -520.6321,\n",
            "         -883.1657])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1038.2662,  -917.9061,  -794.6853,  ...,  -922.1074, -1523.5739,\n",
            "         -730.9466])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1599.7826,  -337.4825,  -488.4925,  ..., -1269.7737,  -926.6465,\n",
            "         -822.3130])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -308.3289,  -715.3707, -1352.6306,  ...,  -800.5554, -1032.0690,\n",
            "        -1203.0425])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1030.1005,  -622.9177,  -951.2280,  ..., -1225.0347,  -787.8265,\n",
            "        -1083.3231])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1025.9280,  -360.9912,  -967.1600,  ..., -1054.4857,    17.6030,\n",
            "        -1341.8168])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -848.5311,  -337.6391, -1012.7405,  ...,  -843.1514, -1089.3441,\n",
            "         -649.9769])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1232.7111, -1255.1760,  -705.0863,  ...,  -802.9808, -1345.7853,\n",
            "         -837.9601])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1006.3851, -1622.9833,  -923.9877,  ...,  -723.8776,  -817.7686,\n",
            "        -1261.4434])\n",
            "actor loss: 30449.65853177099, critic loss: 21640193.484375, entropy: 73335.71435546875, KL divergence: 0.012829790036446622\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9382519546043029], 離散行動：[0, 1], 連続行動：-1.0875122547149658\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "79エピソード目の累積報酬：-102510.82097011345, 一つ保全の回数：34, 二つ保全の回数：8158, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([  -75.8875, -1150.0820, -1411.1337,  ...,  -545.7853, -1444.0969,\n",
            "        -1219.1021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1489.4218, -1683.9884,  -949.6536,  ..., -1531.7905, -1281.9030,\n",
            "         -117.7386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1093.9547, -1334.1421,  -571.3710,  ...,  -108.7265,  -873.5894,\n",
            "        -1080.2628])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -525.2609, -1387.9413,  -760.3879,  ...,  -984.8002, -1016.2964,\n",
            "        -1111.3312])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1264.7468,  -959.0228, -1122.9841,  ...,  -336.9682,  -606.3392,\n",
            "         -882.7998])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -942.2427,  -404.3358,  -638.4436,  ...,  -720.7433,  -837.3156,\n",
            "        -1310.0576])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -910.7350, -1476.9153,  -258.2744,  ..., -1227.6437, -1136.3215,\n",
            "         -638.8820])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -950.7905, -1214.4725,  -462.3921,  ..., -1214.2809,  -497.8336,\n",
            "         -470.8129])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -416.9894, -1265.2175, -1638.3383,  ...,  -529.8604, -1852.1013,\n",
            "        -1292.6777])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -381.1662,  -406.8809, -1044.3563,  ..., -1157.5170, -1016.2964,\n",
            "         -750.0826])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1573.4159,  -841.8594,  -188.4145,  ...,  -653.5726,  -925.8063,\n",
            "        -1406.2668])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -646.8461,  -913.7729, -1563.1273,  ...,  -765.8709,  -719.3809,\n",
            "         -701.5352])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1162.3580, -1181.2856,  -681.2280,  ..., -1293.3672,  -943.5079,\n",
            "        -1355.1161])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -637.9969,  -108.7265,  -721.7767,  ..., -1077.8691, -1228.4143,\n",
            "         -876.2374])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1018.1306, -1705.6870, -1244.0497,  ..., -1023.0599, -1008.4629,\n",
            "         -204.7710])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -942.3880, -1276.4707, -1092.8950,  ...,  -715.3826,   -38.7686,\n",
            "         -492.0148])\n",
            "actor loss: 30498.228141747833, critic loss: 21854137.09375, entropy: 74201.92529296875, KL divergence: 0.009095532746348875\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.4722179799417239], 離散行動：[0, 1], 連続行動：1.5676383972167969\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "80エピソード目の累積報酬：-99950.33162979732, 一つ保全の回数：37, 二つ保全の回数：8155, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -881.8162,  -249.0732, -1072.8982,  ..., -1657.5256,  -737.2510,\n",
            "        -1636.2511])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1596.6636, -1384.7689, -1282.4985,  ..., -1058.0010,  -977.0287,\n",
            "        -1035.4982])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1641.2358,  -489.0793, -1495.3011,  ..., -1676.5537,  -686.6267,\n",
            "        -1080.7833])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -627.9316, -1034.7036,  -903.3083,  ..., -1247.7573, -1368.1080,\n",
            "        -1260.7532])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1021.3221, -1069.7385,  -606.5778,  ..., -1381.8466, -1068.7545,\n",
            "        -1120.2599])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1038.1826,  -494.7039, -1049.0798,  ...,  -439.0138,    34.5722,\n",
            "         -906.8558])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -934.2956,  -373.9705,  -956.5746,  ..., -1412.2634, -1096.5134,\n",
            "         -636.7885])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1143.7722, -1134.6001, -1000.7656,  ...,   -30.3444, -1034.6486,\n",
            "         -714.2499])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -903.4341, -1595.5187,  -977.5195,  ..., -1416.4800,  -449.6665,\n",
            "         -977.1536])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1471.9202, -1514.7275,  -327.2092,  ...,  -469.6066,  -831.1331,\n",
            "         -734.7621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -949.3936,  -425.2970,  -949.7714,  ...,  -819.5377, -1018.8126,\n",
            "        -1600.9867])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -530.5486,  -725.9076, -1148.4568,  ...,  -919.9606,  -731.9240,\n",
            "        -1270.9507])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1470.9495,  -528.2200, -1015.1959,  ..., -1169.1937, -1118.6936,\n",
            "        -2352.9006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1787.0878,  -971.9702, -1778.6667,  ...,  -949.0666, -1365.1149,\n",
            "         -718.9810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -801.1307,  -232.1174, -1166.0712,  ..., -1078.3979,  -905.9230,\n",
            "        -1343.3171])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1166.7167, -1167.8400,  -787.1246,  ...,  -692.7267, -1501.3033,\n",
            "         -270.0352])\n",
            "actor loss: 30397.408408594678, critic loss: 21968398.875, entropy: 75521.29443359375, KL divergence: 0.010686426079172992\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9038712484758418], 離散行動：[0, 1], 連続行動：-0.6243155002593994\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "81エピソード目の累積報酬：-100485.92023914195, 一つ保全の回数：33, 二つ保全の回数：8158, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1044.7460,  -821.1531,  -633.3572,  ...,  -896.5742, -1243.0305,\n",
            "         -528.6716])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -448.1090, -1808.5563,  -594.5141,  ...,  -530.7952, -1109.5750,\n",
            "        -1215.0040])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1356.9895,  -512.4100, -1785.3140,  ...,  -647.1207, -1121.9657,\n",
            "         -633.0116])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1929.4242, -1370.1794,  -746.2036,  ..., -1080.8069, -1405.4120,\n",
            "         -809.9812])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-298.7150, -949.2767, -768.8096,  ..., -588.5522, -872.9675,\n",
            "        -668.6094])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -571.1307, -1104.7860, -1086.2405,  ..., -1221.5262,  -637.9081,\n",
            "        -1001.4540])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -453.5403,  -989.0200,  -728.7456,  ..., -1751.6704,   -56.3903,\n",
            "         -922.2581])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1033.8997, -1172.7482, -1134.9786,  ..., -1817.6700,  -895.6937,\n",
            "        -1229.4141])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1013.1035, -1073.4852,  -795.3641,  ...,  -753.2659,  -629.0847,\n",
            "         -696.2139])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1466.4055,  -926.1948, -1129.9467,  ..., -1129.1492, -1233.0641,\n",
            "        -1071.6494])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1284.3390, -1408.0859,  -924.1208,  ...,  -745.0521, -1035.4183,\n",
            "        -1651.2329])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -588.1406, -1154.1348,    -3.3031,  ...,  -385.4876,  -655.5873,\n",
            "        -1274.0505])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -830.6409,  -703.9603, -1148.9244,  ..., -1593.7756, -1417.3687,\n",
            "         -397.4294])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1248.1088, -1316.9830,  -631.4866,  ..., -1526.6660,  -442.8156,\n",
            "        -1355.2378])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -981.6472, -1151.4580,  -474.0032,  ...,  -930.7984,  -279.8162,\n",
            "         -751.0159])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -194.7606,  -962.0461, -1280.2834,  ...,  -427.9997, -1553.1616,\n",
            "         -234.3228])\n",
            "actor loss: 30456.708176502983, critic loss: 22490298.328125, entropy: 77306.31396484375, KL divergence: 0.0120570938325678\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.971511390418555], 離散行動：[0, 1], 連続行動：1.2509833574295044\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "82エピソード目の累積報酬：-99169.54498318183, 一つ保全の回数：32, 二つ保全の回数：8159, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1384.6241,  -660.0685,  -686.2949,  ...,  -906.3615, -1416.1240,\n",
            "         -579.6015])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1065.1533, -1271.9093,  -534.1028,  ..., -1027.9492, -1257.2040,\n",
            "        -1774.4767])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -384.3436,  -804.6611, -1127.1819,  ..., -1031.0234,  -428.9308,\n",
            "         -502.4006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -732.2438, -1291.2002,  -718.3827,  ...,  -790.3408,  -798.9783,\n",
            "         -705.7122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -371.3280,  -263.6571, -1047.9706,  ...,  -903.7069,  -848.6571,\n",
            "        -1835.0398])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -939.2356, -1406.3365,  -600.4638,  ...,  -872.3943, -1343.2501,\n",
            "        -1113.7231])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1050.9836,  -605.9294,  -625.6302,  ..., -1179.9097,  -743.9962,\n",
            "        -1162.7488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1268.0856,  -522.1459, -1342.6610,  ...,  -969.8788, -1077.1368,\n",
            "         -800.6415])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1442.4476,  -866.5714,  -807.5120,  ...,  -864.8836,  -610.7823,\n",
            "         -677.0302])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -922.9967,  -800.8516,  -451.4651,  ..., -1424.9943, -1219.0802,\n",
            "         -912.3231])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -965.8856,  -492.8800,  -676.2802,  ...,  -531.1655,  -747.1151,\n",
            "        -1083.1857])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -813.5840, -1325.2328,  -739.1200,  ...,  -664.4753, -1588.2000,\n",
            "         -989.1600])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -546.7365,  -800.7812,  -661.1335,  ..., -1076.2585,  -486.9189,\n",
            "        -1180.4005])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1071.4032,  -798.9783,  -837.4309,  ...,  -942.6847, -1142.9463,\n",
            "         -775.9984])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -801.7967,  -838.3483, -1522.2126,  ..., -1165.1790,  -743.4146,\n",
            "         -429.4791])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -818.2599,  -275.1932,  -818.5763,  ...,   -26.1271, -1218.9075,\n",
            "         -420.3722])\n",
            "actor loss: 30327.840024040463, critic loss: 22243829.453125, entropy: 78192.84326171875, KL divergence: 0.007145829390057723\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.3304214997655643], 離散行動：[0, 1], 連続行動：-0.78265380859375\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "83エピソード目の累積報酬：-99000.27623772595, 一つ保全の回数：19, 二つ保全の回数：8170, 三つ保全の回数：3, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1653.4370, -1165.8983,  -926.5115,  ...,  -750.4473,  -872.1556,\n",
            "        -1479.3600])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1075.2975, -1188.6606, -1449.7377,  ...,  -796.1530,  -963.5497,\n",
            "         -477.3464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -617.6499,  -497.4618, -1121.9304,  ...,  -647.3792, -1297.6355,\n",
            "         -857.5172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1464.3354,  -979.3825, -1563.9507,  ...,  -865.7630,  -349.9058,\n",
            "        -1094.9675])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1114.4380,  -930.4831, -1074.5891,  ..., -1271.8534, -1237.5792,\n",
            "         -511.2038])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -682.7944, -1090.9752, -1088.3473,  ...,  -496.7719, -1219.4520,\n",
            "        -1206.5529])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1188.9961,  -986.1584,  -797.7452,  ..., -1214.1747, -1330.8247,\n",
            "        -1329.5645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1078.1710,   -84.3000,  -396.0485,  ..., -1401.3483,  -813.4671,\n",
            "         -958.2134])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -679.8062,  -509.1994, -1192.7382,  ...,  -735.9865,  -749.4617,\n",
            "         -713.1542])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -772.2587,  -964.1249, -1056.4285,  ...,  -535.3676, -1114.4781,\n",
            "         -935.6654])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -852.1810, -1361.5010,  -927.8144,  ...,  -895.2122, -1163.2744,\n",
            "         -877.6667])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -887.3100,  -446.0166, -1043.5374,  ..., -1135.9465,  -415.1988,\n",
            "         -896.4183])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1015.4043, -1367.9973, -1257.0017,  ..., -1083.4778, -1304.6085,\n",
            "         -863.3945])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -933.6622, -1217.9215,  -271.6437,  ...,  -984.2878,  -845.3262,\n",
            "         -461.6451])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1269.1521, -1027.7432,  -622.3828,  ...,  -918.9809,  -905.2295,\n",
            "          -95.3783])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1090.0836, -1233.7111,  -947.4294,  ..., -1208.4305,  -675.8630,\n",
            "         -702.8069])\n",
            "actor loss: 30315.813659006162, critic loss: 22325833.734375, entropy: 78498.6357421875, KL divergence: 0.004186140782556449\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6086387681355077], 離散行動：[0, 1], 連続行動：-0.5380921363830566\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "84エピソード目の累積報酬：-99999.92030194361, 一つ保全の回数：13, 二つ保全の回数：8178, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -813.2241,  -818.4613, -1307.4287,  ...,  -713.3212, -1194.9709,\n",
            "         -827.0248])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -982.3712,  -477.4370,  -925.8927,  ...,  -819.9984, -1502.8883,\n",
            "        -1747.6864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -876.3495,  -462.2151,  -699.6488,  ...,  -378.3620, -1365.0746,\n",
            "        -1118.9062])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -760.1390, -1515.1023, -1101.5858,  ..., -1209.7440,  -121.1886,\n",
            "        -1389.5009])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1069.0688,  -521.6314,  -909.2673,  ...,  -247.7235, -1535.6687,\n",
            "         -797.2358])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -892.6727, -1190.5803,  -960.9154,  ...,  -815.1928, -1308.4586,\n",
            "         -778.9472])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1495.3922, -1166.8413,  -830.9229,  ...,  -898.2946,  -781.5792,\n",
            "        -1059.5122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1323.7717, -1151.6267, -1100.5857,  ..., -1093.8353, -1018.3793,\n",
            "        -1566.4564])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -846.1675,  -627.2932, -1643.7441,  ...,  -807.4369,  -751.9586,\n",
            "        -1085.7897])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -948.0519,  -450.1750, -1286.8759,  ...,  -587.5885, -1047.9720,\n",
            "         -404.7325])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1207.4077,  -696.9185,  -743.0413,  ..., -1155.9968,  -141.5901,\n",
            "        -1092.0464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -981.9187,  -920.9877, -1041.4067,  ..., -1067.5294, -1318.6619,\n",
            "        -1266.5076])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1689.0911, -1287.8073,  -727.1860,  ..., -1145.2848,  -616.9255,\n",
            "         -698.2716])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -606.5701,  -720.3694,  -118.4399,  ..., -1866.0535, -1104.6520,\n",
            "        -1210.8754])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -629.7552,  -901.5637,  -940.0737,  ..., -1529.7969,  -800.1184,\n",
            "         -800.2564])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1502.8804, -1780.4905, -1277.3994,  ...,  -624.7535,  -862.1669,\n",
            "         -602.4017])\n",
            "actor loss: 30324.083760808608, critic loss: 22671080.625, entropy: 78454.70849609375, KL divergence: 0.0030929787119097127\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.763796307735928], 離散行動：[0, 1], 連続行動：0.9144681394100189\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "85エピソード目の累積報酬：-100009.55500697729, 一つ保全の回数：21, 二つ保全の回数：8171, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -188.4101,  -755.4318,  -958.1467,  ..., -1557.9250, -1481.6366,\n",
            "        -1471.6927])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -406.6063,  -778.0117,  -260.2201,  ..., -1428.5946,  -487.5894,\n",
            "         -470.0947])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1254.1014,  -421.4607, -1070.0908,  ..., -1323.6908, -1447.4117,\n",
            "        -1048.1753])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1088.6177,  -964.0687, -1080.2058,  ...,  -727.2133, -1173.2097,\n",
            "         -747.3676])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1622.5494,  -735.1721,  -842.2202,  ..., -1035.5266,  -162.1590,\n",
            "         -816.7228])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -429.0699, -1017.4988,  -784.1787,  ..., -1119.7484,  -836.3752,\n",
            "         -587.2004])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1323.8522, -1341.0212, -1188.3990,  ..., -1384.3003,  -868.3021,\n",
            "        -1250.0022])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1013.1161, -1595.9244, -1106.9230,  ..., -1394.4121, -1458.8611,\n",
            "        -1344.7878])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1033.0029, -1644.4109,  -919.0110,  ...,  -506.6179,  -850.7462,\n",
            "        -1298.8597])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1242.0530, -1027.1790,  -973.3840,  ...,  -966.4825, -1022.0181,\n",
            "        -1514.0892])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -499.7553,  -618.5535,  -747.6098,  ...,  -747.3407,  -796.8761,\n",
            "        -1492.7084])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1294.4365,  -634.0955,  -754.2791,  ...,  -981.4044,  -689.6295,\n",
            "        -1723.9359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -695.2882, -1433.2886, -1324.8872,  ..., -2532.3933,  -951.4859,\n",
            "        -1129.3212])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1132.2175,  -477.7820, -1288.5406,  ..., -1479.0544,  -318.3192,\n",
            "        -1259.6365])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -802.0576,  -634.7339, -1132.1156,  ...,  -315.7645, -1013.1677,\n",
            "         -909.7390])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -746.8875,  -919.7592,  -736.8819,  ...,  -278.0829,  -690.6163,\n",
            "        -1239.6641])\n",
            "actor loss: 30346.303682294, critic loss: 22920969.15625, entropy: 78971.4619140625, KL divergence: 0.002386741259247233\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.734152125803186], 離散行動：[0, 1], 連続行動：1.9791316986083984\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "86エピソード目の累積報酬：-100010.39615463676, 一つ保全の回数：19, 二つ保全の回数：8173, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -624.2443,  -565.0538,  -457.6505,  ..., -1202.5521,  -745.7191,\n",
            "         -571.0281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1185.4740,  -310.2332, -1096.1639,  ..., -1012.7406,  -682.6756,\n",
            "         -837.9348])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -816.8763,  -852.3713, -1003.1833,  ..., -1226.4199,  -562.6644,\n",
            "        -1487.9303])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-892.5060, -172.4033, -776.5566,  ..., -424.2672, -701.4657,\n",
            "        -908.4509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -726.4809,  -597.1732, -1037.4561,  ..., -1001.4084,  -670.9692,\n",
            "        -1552.4362])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -671.9967,  -890.1956, -1851.4594,  ..., -1127.7273, -1380.2491,\n",
            "        -1192.1653])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-516.5064, -823.4131, -926.7985,  ..., -735.8196, -703.4573,\n",
            "        -397.7137])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -818.5355,  -669.3212, -1061.8840,  ..., -1161.3555,  -647.9283,\n",
            "        -1062.2065])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -683.3199, -1016.7766,  -483.9310,  ..., -1222.6289,  -778.8268,\n",
            "        -1220.1145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1307.7805,  -713.2459, -1532.0564,  ...,  -845.4214,  -852.9900,\n",
            "         -641.0605])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1052.2944,  -485.8564,  -573.6416,  ...,  -931.3719,  -337.7147,\n",
            "        -1056.2578])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1007.2629,  -947.9242,  -896.9152,  ..., -2296.9866, -1396.7180,\n",
            "        -1118.5853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1219.1515, -1363.3674, -1462.1989,  ...,  -883.6615, -1576.9248,\n",
            "        -1071.7618])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -462.0157,  -866.2754, -1873.7104,  ...,  -756.5974,  -837.5486,\n",
            "         -992.0566])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -754.7874,  -763.2745,  -906.1489,  ...,  -312.8721,  -740.8863,\n",
            "        -1158.7778])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-953.1217, -400.2892, -691.6107,  ..., -735.5877, -709.3960,\n",
            "        -702.2444])\n",
            "actor loss: 30247.478243283826, critic loss: 23449372.0625, entropy: 80894.1328125, KL divergence: 0.015775722527375042\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.949780478123259], 離散行動：[0, 1], 連続行動：3.1169893741607666\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "87エピソード目の累積報酬：-100000.62767084321, 一つ保全の回数：10, 二つ保全の回数：8182, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1657.1506, -1052.4026,  -470.0613,  ..., -1434.2831, -1428.9347,\n",
            "         -692.9689])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -847.7380,  -607.4512, -1193.6145,  ...,  -828.0138,  -882.8883,\n",
            "        -1564.2026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1063.0870,  -560.2890,  -702.9494,  ...,  -874.1140,  -507.4743,\n",
            "         -876.9461])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -408.5543,  -745.5190, -1203.7300,  ...,  -543.9062,  -825.3232,\n",
            "        -1443.3387])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -414.9124, -1388.6227, -1299.6404,  ...,  -677.7242,  -890.0833,\n",
            "         -769.7237])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -790.0118, -1197.4014, -1044.4297,  ...,  -975.0275,  -909.9857,\n",
            "         -962.3517])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1072.8646,  -603.7985,  -677.5984,  ..., -1677.2509, -1470.0286,\n",
            "         -256.9172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -738.8011, -1259.3611, -1023.6198,  ...,  -975.8236,  -666.1447,\n",
            "        -1287.8011])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1183.6040,  -671.6232,  -653.5200,  ..., -1020.7239,  -536.1655,\n",
            "        -1288.4344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1159.7350,  -932.6483,  -831.7965,  ...,  -528.6686, -1053.5272,\n",
            "         -757.2557])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1096.7339,  -830.5785,  -764.0339,  ...,  -471.2303, -1415.4172,\n",
            "         -958.5571])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-913.4791, -956.4644, -250.5768,  ..., -612.8244, -389.1638,\n",
            "        -835.3078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1060.1838,  -912.1459,  -807.8997,  ...,  -647.0254, -1196.3174,\n",
            "        -1298.0305])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -759.1215,  -715.0986, -1140.4182,  ..., -1304.7715,  -888.2641,\n",
            "        -1526.6794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1250.4634,  -774.6104, -1257.6434,  ..., -1264.0165,  -954.9692,\n",
            "        -1404.1940])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -928.3245,  -623.9143, -1538.1122,  ...,  -296.5378, -1583.3409,\n",
            "         -991.3007])\n",
            "actor loss: 30287.107631807234, critic loss: 23357015.265625, entropy: 82424.26904296875, KL divergence: 0.00711464398904848\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1204744234383694], 離散行動：[0, 1], 連続行動：0.6849907040596008\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "88エピソード目の累積報酬：-100001.24660589345, 一つ保全の回数：16, 二つ保全の回数：8176, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -104.0550,  -259.7693,  -921.7708,  ...,  -641.8254, -1395.5780,\n",
            "         -313.9949])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1000.0470, -1640.9346, -1330.7429,  ..., -1032.6853, -1471.7898,\n",
            "         -875.7407])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1538.2129,  -860.9600,  -728.3041,  ...,  -591.9163,  -804.4235,\n",
            "        -1409.8782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1304.7729,  -969.6502,  -986.2668,  ..., -1003.5093, -1365.5564,\n",
            "         -938.6902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -471.6172,  -559.1716,  -787.8622,  ..., -1243.4580,  -942.0779,\n",
            "         -859.0556])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1333.1135,  -799.1705,  -515.1013,  ..., -1053.8258, -1113.9508,\n",
            "         -872.7225])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -845.6564, -1036.3583,  -223.5667,  ...,  -752.3435,  -987.1823,\n",
            "         -836.2795])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -586.7877, -1087.7567, -1201.1366,  ...,  -936.7943,  -882.0807,\n",
            "        -1053.3727])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1332.7483, -1070.9259,  -581.7693,  ..., -1388.8337, -1197.9678,\n",
            "        -1100.2128])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -771.7046, -1386.6864, -1346.4384,  ...,  -956.5226, -1047.0668,\n",
            "         -797.4127])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -584.8812,  -257.2910, -1163.6255,  ..., -1064.0146, -1661.4700,\n",
            "         -938.6902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1043.0151,  -902.0778, -1320.7659,  ..., -1015.2100,  -472.8858,\n",
            "         -473.0167])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1325.7356, -1365.6511,  -858.1143,  ...,  -880.0690,  -614.5495,\n",
            "        -1304.3093])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -689.9047,  -873.8629,  -738.2150,  ..., -1092.6378, -1227.2009,\n",
            "         -619.0767])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -979.9904,  -757.5706,  -711.8377,  ..., -1118.7120, -1180.6139,\n",
            "         -624.8764])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -860.9706, -1137.9100, -1126.6437,  ...,  -648.7524,  -379.9264,\n",
            "         -947.3407])\n",
            "actor loss: 30253.112931926516, critic loss: 23474324.234375, entropy: 84111.75537109375, KL divergence: 0.0042742101313218\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.600536003035054], 離散行動：[0, 1], 連続行動：1.797106385231018\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "89エピソード目の累積報酬：-100056.64573088256, 一つ保全の回数：14, 二つ保全の回数：8178, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -632.7070,  -923.6779, -1217.4102,  ..., -1402.2646, -1404.7180,\n",
            "        -1445.6666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -858.2391,  -694.7566, -1481.1045,  ..., -1449.6288,  -785.0262,\n",
            "         -867.5619])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1281.5408,  -373.9223, -1050.2390,  ...,  -696.0233, -1469.2281,\n",
            "         -658.8506])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1162.7576,  -796.4986,  -561.8953,  ..., -2299.9199,  -281.6348,\n",
            "         -168.2477])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1056.7950, -1040.1154, -1227.1110,  ...,  -678.5117,  -965.0613,\n",
            "         -764.5630])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1642.9517, -1702.2190,  -845.0859,  ..., -1079.4545, -1100.0565,\n",
            "        -1020.1191])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1418.9989,  -497.8442,  -408.5629,  ...,  -134.3578, -1419.3033,\n",
            "         -819.4911])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1010.0955,  -694.0543, -1001.6692,  ..., -1371.1384, -1088.0504,\n",
            "         -996.0568])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -808.1881,  -947.7865,  -913.9631,  ..., -1388.5645,  -294.8087,\n",
            "        -1277.9548])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1380.4543, -1131.4702, -1575.3250,  ...,  -717.2768,  -925.7318,\n",
            "         -742.7708])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -658.7686, -1023.2324, -1422.2849,  ...,  -754.0420,  -687.4748,\n",
            "        -1038.4390])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -595.4466,  -280.2903, -1430.9984,  ..., -1178.0469, -1315.2119,\n",
            "        -1157.2273])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1031.3569, -1044.2598,  -741.4484,  ...,  -488.4611,  -796.5958,\n",
            "         -676.2255])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-2703.5818, -1114.5742, -1110.0350,  ..., -1093.4829, -1552.5608,\n",
            "        -1235.7518])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1567.9630, -1614.2246,  -972.7776,  ...,  -865.8887,  -623.2549,\n",
            "         -920.4359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -882.7330, -1389.1858,  -810.2137,  ..., -1321.5323,  -220.5587,\n",
            "         -632.1379])\n",
            "actor loss: 30262.493835402056, critic loss: 23846139.125, entropy: 83468.5703125, KL divergence: 0.0012694494050915436\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5264226898918771], 離散行動：[0, 1], 連続行動：1.5892908573150635\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "90エピソード目の累積報酬：-99999.99999223405, 一つ保全の回数：15, 二つ保全の回数：8177, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -574.7626, -1004.7689,   -57.3405,  ...,  -201.8317,  -985.9295,\n",
            "         -886.2794])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-658.7698, -653.7360, -927.2635,  ..., -939.6725, -299.0581,\n",
            "        -879.4359])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1581.6234,  -917.8154,  -852.6428,  ..., -1499.5731,  -503.0739,\n",
            "         -959.4883])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -835.2610,  -692.0648,  -961.9718,  ...,  -904.9352, -1151.7494,\n",
            "         -517.5375])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-989.7921, -940.4115, -797.1963,  ..., -876.9677, -960.4875,\n",
            "        -562.2055])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1558.2557,  -844.1621,  -767.9740,  ..., -1021.8127,  -974.8689,\n",
            "        -1086.0042])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1301.5792, -1253.7769, -1066.6718,  ..., -1563.4537,  -517.6642,\n",
            "        -1538.6451])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1081.6575,  -908.7501, -1401.0964,  ...,  -850.8256, -1546.3502,\n",
            "         -919.8696])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1392.1295,  -831.6307, -1277.2885,  ..., -1230.3672, -1262.7734,\n",
            "        -3263.2737])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -701.0972, -1123.1278,  -979.9377,  ...,  -509.6051,  -651.4483,\n",
            "         -902.1481])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1663.5836, -1574.3248,  -602.0555,  ..., -1352.1792,  -795.1093,\n",
            "         -345.7743])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -912.7771, -1464.2540,  -666.2027,  ...,  -985.2871,  -703.1125,\n",
            "         -202.3645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -833.0317, -1394.9253, -1030.3190,  ...,  -940.1403,  -818.5983,\n",
            "         -872.7943])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -873.8701,  -744.7864, -1124.1559,  ..., -1508.1763,  -990.8651,\n",
            "         -384.2946])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -543.1968, -1142.2351,  -619.4926,  ...,  -328.0832,  -722.9259,\n",
            "        -1610.0800])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1296.8480, -1232.2828, -1343.6550,  ..., -1547.1699, -1144.6040,\n",
            "         -864.5767])\n",
            "actor loss: 30226.14955483297, critic loss: 24049731.71875, entropy: 83317.3759765625, KL divergence: 0.0003627289691345392\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6923280794429403], 離散行動：[0, 1], 連続行動：-0.9462320804595947\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "91エピソード目の累積報酬：-100529.3817937233, 一つ保全の回数：17, 二つ保全の回数：8175, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1275.0676, -1342.2581,  -632.4436,  ...,  -906.5931, -1211.1650,\n",
            "         -655.1910])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1415.9843,  -988.4033,  -333.7478,  ..., -1092.1154,  -866.2458,\n",
            "         -842.4487])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1433.8124,  -799.7077, -1678.7385,  ..., -1080.3077,  -853.6354,\n",
            "         -797.8588])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1214.5400,  -974.7387, -1172.9446,  ...,  -899.2112,  -758.3910,\n",
            "         -971.9401])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -804.5126, -1000.7004,  -753.1862,  ..., -1025.1361, -1037.5833,\n",
            "        -1112.1765])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1023.9620,  -904.1465,  -726.6526,  ..., -1084.1477,  -442.5680,\n",
            "         -803.0167])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -253.3856,  -770.9295, -1122.3572,  ...,  -369.9970, -1125.8781,\n",
            "        -1566.7010])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1059.6925,  -433.6333, -1121.7928,  ...,  -968.6556, -1098.0591,\n",
            "        -1218.7563])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -784.1940, -1045.1458, -1166.7389,  ..., -1003.9813, -1143.6931,\n",
            "         -886.2864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1021.2983,  -811.4427, -1094.9207,  ..., -1039.6667, -1172.5619,\n",
            "        -1023.5782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1275.0273,  -740.2086, -1045.3690,  ..., -1213.6173, -1136.9185,\n",
            "         -669.6303])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -678.4901,  -821.6354, -1324.0685,  ..., -1090.2565,  -796.3198,\n",
            "         -909.7949])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -544.7297, -1476.4879,  -525.3447,  ...,  -706.4088, -1299.7749,\n",
            "        -1199.0503])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -780.7751, -1075.7618,  -953.6467,  ..., -1322.8403,  -746.6182,\n",
            "         -857.8160])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1319.3053, -1059.3708, -1078.0206,  ..., -1108.3969,  -751.7473,\n",
            "        -1687.1866])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -923.9727, -1499.5599,  -807.3429,  ..., -1239.5007,  -841.6953,\n",
            "         -902.3644])\n",
            "actor loss: 30157.645861181307, critic loss: 24236561.0703125, entropy: 83604.85595703125, KL divergence: 0.00037756642272225555\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1572688786467475], 離散行動：[0, 1], 連続行動：1.7216758728027344\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "92エピソード目の累積報酬：-99999.95186298878, 一つ保全の回数：13, 二つ保全の回数：8179, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-701.5811, -945.8481, -655.8583,  ..., -473.9521, -858.2574,\n",
            "        -897.4775])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1259.0782,  -943.8065,  -793.4050,  ..., -1140.3394,  -754.4793,\n",
            "         -918.2908])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -774.1201, -1294.0852, -1168.4497,  ...,  -745.9119, -1018.0076,\n",
            "         -843.6603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -915.2745, -1031.1608,  -904.9826,  ...,  -784.2919, -1425.3000,\n",
            "         -620.7531])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -785.3514, -1429.1387,  -689.0819,  ...,  -463.2946,  -949.6691,\n",
            "        -1045.6350])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -951.2905, -1096.4437, -1277.6161,  ...,  -936.7228,  -883.4016,\n",
            "         -862.2386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -743.4404, -1027.7274, -1017.7197,  ..., -1287.9146,  -662.4230,\n",
            "         -793.2100])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -481.7728,  -920.4631, -1234.9690,  ..., -1049.8519, -1008.5766,\n",
            "         -790.6541])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1174.4961,  -794.5176, -1331.0216,  ...,  -652.7822, -1141.8149,\n",
            "        -1289.1123])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -333.1644, -1285.1012, -1134.6816,  ...,  -744.5322,  -822.9495,\n",
            "         -595.9006])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1038.5886,  -725.9810, -1135.9751,  ...,  -602.3631,  -592.7919,\n",
            "        -1230.5641])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1132.7395,  -298.0668, -1220.5020,  ...,  -953.8405,  -955.6160,\n",
            "         -972.9417])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -825.2552, -1056.2345,  -678.6548,  ...,  -971.3589,  -556.1104,\n",
            "         -822.4330])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1748.3182, -1099.1222, -1346.8582,  ..., -1358.7306, -1465.4706,\n",
            "         -869.5439])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -668.4174, -1512.2537,  -996.5440,  ...,  -949.7775,  -592.8619,\n",
            "        -1605.1824])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1347.9901,  -885.6785, -1299.9170,  ...,  -766.8277,  -945.1716,\n",
            "         -991.4530])\n",
            "actor loss: 30091.984639473954, critic loss: 24550778.015625, entropy: 84521.13037109375, KL divergence: 0.00676506765319532\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.50840930976535], 離散行動：[0, 1], 連続行動：3.221769332885742\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "93エピソード目の累積報酬：-100002.75398278557, 一つ保全の回数：16, 二つ保全の回数：8176, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -870.2654, -1244.3334,  -382.8479,  ...,  -367.8545, -1248.4458,\n",
            "        -1114.8574])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -156.3189,  -880.7615,  -355.1180,  ..., -1313.8208, -1118.2242,\n",
            "        -1640.3756])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1385.8907, -1545.5851,  -880.5912,  ..., -1032.0280, -1129.4330,\n",
            "        -1000.0577])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -700.6912,  -923.9279,  -960.5772,  ..., -1046.7593, -1313.3256,\n",
            "        -1209.1405])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1079.7239,  -806.8704,  -531.6348,  ...,  -932.2251,  -836.2167,\n",
            "        -1153.5861])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1103.4016,  -820.9318, -1510.5789,  ...,  -872.8996,  -726.9092,\n",
            "         -988.0940])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -975.3438,  -501.9477,  -530.8217,  ...,  -535.4958, -1206.6638,\n",
            "         -933.4143])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -865.4529,  -609.6959,  -869.3895,  ..., -1121.7925,  -569.9870,\n",
            "        -1368.6239])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -669.1984,  -961.1765, -1472.7726,  ...,  -785.7482,  -784.7775,\n",
            "         -718.6823])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-905.4410, -926.3630, -693.1243,  ..., -967.4359, -847.4634,\n",
            "        -794.9836])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -624.0194,  -678.0476,  -658.8328,  ...,  -867.3495, -1212.1581,\n",
            "         -470.5168])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1503.2316,  -860.5400,  -712.2741,  ..., -1147.5068,  -881.5052,\n",
            "        -1067.7570])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -667.6461,  -878.6788, -1035.6343,  ...,  -787.6664,  -398.8793,\n",
            "         -846.4829])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -696.5198,  -755.7243,  -541.7538,  ..., -1015.6173,  -937.4407,\n",
            "         -858.8815])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1247.7777, -1233.6826, -1061.5149,  ..., -1206.9279,  -811.0912,\n",
            "        -1165.9723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -544.8135,  -886.2457,  -635.6003,  ..., -1217.6487,  -944.2935,\n",
            "        -1174.1155])\n",
            "actor loss: 30048.637217484593, critic loss: 24539706.484375, entropy: 87129.23486328125, KL divergence: 0.008644286733237633\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1305634571635317], 離散行動：[0, 1], 連続行動：1.6301058530807495\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "94エピソード目の累積報酬：-99973.9626509973, 一つ保全の回数：12, 二つ保全の回数：8180, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -766.0013,  -394.0440,  -983.4518,  ...,  -750.3737,  -578.5322,\n",
            "        -1046.1541])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1095.4197,  -823.2812,  -997.6160,  ...,  -464.6938,  -956.3497,\n",
            "         -694.9751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -809.2582, -1318.0687,  -839.9822,  ..., -1037.2882,  -900.6190,\n",
            "         -984.4073])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1622.6326,  -538.6500,  -851.7230,  ...,  -622.3749, -1281.2076,\n",
            "         -444.2914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -626.2877,  -569.2111,  -929.8464,  ..., -1042.4347, -1174.9376,\n",
            "        -1511.5977])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -918.4318, -1067.6068, -1131.2152,  ...,  -976.0052, -1303.2399,\n",
            "         -855.5894])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1254.3860, -1046.5459, -1237.6021,  ..., -1440.7561, -1099.3940,\n",
            "         -893.9996])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1421.7888,  -316.5854, -1361.3671,  ..., -1011.2680, -1265.3998,\n",
            "         -964.0558])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -779.9661,  -874.8547, -1026.4312,  ..., -1195.1626,  -976.0951,\n",
            "        -1141.6793])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -590.4025,  -584.5842, -1405.6705,  ..., -1037.2435,  -872.2496,\n",
            "        -1068.6849])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1023.2015,  -417.2329,  -969.4172,  ...,  -987.0562,  -888.2697,\n",
            "         -548.8191])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1366.6848, -1054.6821, -1686.4268,  ..., -1435.7607,  -967.7856,\n",
            "        -1089.8954])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -827.0484, -1546.0270,  -859.6204,  ..., -1129.6294,  -855.5894,\n",
            "         -416.4417])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -783.3018, -1110.4418,  -830.7826,  ...,  -633.7243,  -736.1790,\n",
            "        -1002.6346])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -325.5049,  -902.7983,  -445.7878,  ..., -1100.5156,  -180.5927,\n",
            "         -790.9898])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1071.7972,  -656.3570,  -272.7415,  ...,  -982.1207, -1586.6838,\n",
            "         -954.9660])\n",
            "actor loss: 30142.958039461326, critic loss: 25086642.5, entropy: 89071.9365234375, KL divergence: 0.01457502907456789\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.2756562910800433], 離散行動：[0, 1], 連続行動：-3.0341930389404297\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "95エピソード目の累積報酬：-100007.34219422837, 一つ保全の回数：9, 二つ保全の回数：8183, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1423.4946,  -631.7195, -1229.2302,  ...,  -910.7853,  -754.0264,\n",
            "         -896.6610])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -932.4881,  -533.2765, -1114.0605,  ...,  -909.3990, -1071.5796,\n",
            "        -1095.9561])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1015.0643,  -564.1693,  -663.7979,  ...,  -723.3102,  -522.3093,\n",
            "        -1084.4860])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1035.8822, -1029.0010,  -908.6464,  ..., -1420.8680,  -793.2234,\n",
            "         -943.3575])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1155.4753, -1166.4709,  -570.1403,  ...,  -792.4653,  -556.7110,\n",
            "        -1437.0233])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -978.1048, -1328.3071,  -970.8860,  ...,  -663.0674, -1051.2238,\n",
            "         -805.3853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1113.4792,  -925.2176, -1013.6840,  ...,  -385.2720,  -517.6298,\n",
            "        -1255.6172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -307.8462, -1267.0668,  -947.5212,  ...,  -959.2972, -1252.1407,\n",
            "        -1048.4027])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -190.5612,  -892.1815, -1008.9243,  ..., -1157.2131, -1008.9719,\n",
            "        -1071.3158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -983.3563, -1057.6564,  -602.8245,  ..., -1036.7046, -1117.5013,\n",
            "         -702.0713])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1069.9686,  -980.8165, -1568.9385,  ...,  -921.6204,  -808.2530,\n",
            "        -1146.7358])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1397.5327,  -366.4943,  -668.9945,  ..., -1117.6296, -1372.7294,\n",
            "        -1058.0227])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -783.0822,  -492.0339, -1290.5236,  ...,  -319.7208,  -669.5399,\n",
            "        -1258.8807])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1122.4550, -1273.9517,  -599.5639,  ...,  -437.7893,  -947.3804,\n",
            "        -1054.6293])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -880.3500,  -879.8469,  -591.9548,  ...,  -555.1953, -1250.3984,\n",
            "        -1250.4497])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1639.1123,  -996.6705,  -748.1103,  ...,  -631.7759, -1196.5745,\n",
            "        -1095.2767])\n",
            "actor loss: 30097.23190891539, critic loss: 25299102.4765625, entropy: 89191.5, KL divergence: 0.004013984373468229\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.368932959946328], 離散行動：[0, 1], 連続行動：-0.45949965715408325\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "96エピソード目の累積報酬：-100481.80183568256, 一つ保全の回数：18, 二つ保全の回数：8174, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -586.9770,  -935.6229,  -967.5081,  ..., -1403.2716,  -829.3240,\n",
            "        -1382.4131])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1253.6997, -1167.7108, -1549.9978,  ..., -1056.5834,  -781.7179,\n",
            "         -373.5809])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1336.4254,  -842.0882, -1075.9425,  ...,  -701.2248,  -914.2772,\n",
            "         -792.6257])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1186.8644,  -857.2903,  -959.6601,  ..., -1193.1716, -1024.4664,\n",
            "         -250.9030])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1057.9567, -1274.9647, -1178.5389,  ..., -1018.8655,  -632.5310,\n",
            "         -865.5303])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -998.0029, -1164.7888,  -848.8648,  ...,  -965.7075,  -442.7389,\n",
            "        -1126.1820])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -686.7679,  -703.8688,  -440.7095,  ...,  -647.7689,  -959.2712,\n",
            "        -1241.6344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -975.8789, -1013.9020,  -750.6980,  ..., -1173.2374,  -705.8112,\n",
            "         -924.7084])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -452.3990,  -880.9954,  -767.4365,  ...,  -431.2782, -1369.9427,\n",
            "        -1137.3715])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1105.3158, -1463.0640,  -627.6401,  ..., -1239.9384,  -855.1646,\n",
            "         -881.6763])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1331.6324, -1073.5907,  -932.3187,  ...,  -554.7319, -1018.5938,\n",
            "        -1155.3033])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -742.8821, -1141.5101, -1067.4321,  ...,  -770.1450,  -866.3928,\n",
            "         -387.3742])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -828.4210, -1388.9523, -1292.3315,  ..., -1040.6387,  -942.5672,\n",
            "        -1267.5565])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -702.1298,  -600.1415,  -704.9448,  ..., -1405.4496,  -526.0753,\n",
            "        -1383.0510])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -959.7333, -1083.1836, -1393.5732,  ...,  -684.7214,  -325.1984,\n",
            "        -1523.8464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1435.7166,  -565.7803,  -545.0276,  ...,  -527.4938,  -437.6840,\n",
            "         -766.6625])\n",
            "actor loss: 30090.476995902995, critic loss: 25397455.7734375, entropy: 89253.4443359375, KL divergence: 0.001298243706174341\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.346074927624017], 離散行動：[0, 1], 連続行動：-1.5448713302612305\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "97エピソード目の累積報酬：-100000.00004524132, 一つ保全の回数：7, 二つ保全の回数：8184, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -809.6153,  -874.0767,  -639.1831,  ...,  -550.3913,  -467.9908,\n",
            "        -1172.0386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -905.7512,  -737.8000,  -672.0555,  ...,  -819.6193, -1160.0651,\n",
            "        -1170.5488])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1328.9902,  -864.0984, -1062.0337,  ...,  -890.6160,  -829.6445,\n",
            "         -587.3906])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -941.4490, -1431.3156,  -864.7585,  ...,  -974.8272, -1093.6310,\n",
            "        -1400.7111])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -732.6748,  -960.1293, -1081.0940,  ...,  -929.7332, -1179.2742,\n",
            "        -1200.8645])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -958.8732,  -751.2051,  -940.8469,  ..., -1010.9575, -1023.3997,\n",
            "         -913.7136])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -667.2195,  -821.8094, -1395.2540,  ..., -1242.8829, -1171.2969,\n",
            "         -711.6826])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -810.0289,  -604.4108, -1064.7435,  ...,  -774.8712,  -853.1732,\n",
            "         -806.9698])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1065.9015, -1124.8895,  -821.2408,  ...,  -488.2308, -1042.1244,\n",
            "         -711.1691])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1191.2347,  -735.0043, -1490.1080,  ..., -1250.6143,  -675.1948,\n",
            "         -575.7125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -627.7387, -1070.6454,  -951.7570,  ...,  -994.5674,  -836.9384,\n",
            "         -419.1138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -932.8897,  -750.7961, -1344.4465,  ..., -1052.0822, -1222.8550,\n",
            "        -1306.4536])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1224.9562, -1039.4934,  -993.4131,  ...,  -756.9373,  -780.8872,\n",
            "         -431.5755])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -864.0984,  -779.9243,  -727.2423,  ...,  -263.3337, -1141.4199,\n",
            "         -856.9041])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1227.0132, -1510.2401,  -662.2360,  ...,  -672.1962, -1020.6954,\n",
            "        -1325.8291])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -922.8989, -1185.5276, -1157.0498,  ...,  -334.7130,  -956.8185,\n",
            "         -678.6574])\n",
            "actor loss: 30100.245305891043, critic loss: 25552295.5703125, entropy: 89377.107421875, KL divergence: 0.0006880613160494503\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5219372712227806], 離散行動：[0, 1], 連続行動：2.6173293590545654\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "98エピソード目の累積報酬：-101103.40447684092, 一つ保全の回数：11, 二つ保全の回数：8181, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1566.9983,  -656.6586, -1300.9980,  ..., -1215.2808,  -849.6984,\n",
            "        -1017.7775])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -896.8867,  -567.6744, -1132.5803,  ...,  -336.0118,  -951.5169,\n",
            "         -693.8871])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1037.3855,  -824.8201, -1046.3148,  ...,  -823.3487,  -950.5951,\n",
            "        -1184.5201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -571.2934, -1132.1469,  -612.9149,  ...,  -923.7520,  -863.1708,\n",
            "         -701.9828])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -498.0436, -1309.4379, -1331.1777,  ..., -1004.9863,  -864.3334,\n",
            "         -334.4987])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-847.1830, -791.5881, -831.2815,  ..., -732.4546, -592.9171,\n",
            "        -635.0991])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -804.7048,  -817.3959, -1007.6575,  ..., -1106.5044,  -785.6489,\n",
            "         -889.5703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1200.1187,  -802.0287, -1325.2657,  ..., -1098.7328, -1205.9651,\n",
            "        -1205.2416])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1157.9442,  -922.1157,  -779.8694,  ..., -1110.4453, -1228.1384,\n",
            "        -1032.0494])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -630.8712, -1010.7662,  -825.8080,  ...,  -842.4301, -1018.4879,\n",
            "         -881.6094])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1280.4625, -1148.3253,  -985.5632,  ...,  -577.1838, -1014.4139,\n",
            "         -840.4459])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -926.4657,  -912.4396, -1501.7673,  ...,  -650.4727,  -931.0228,\n",
            "         -963.6979])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -810.8323, -1014.8013,  -848.1812,  ..., -1116.1296,  -676.9685,\n",
            "         -909.5749])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1342.7941, -1265.8333,  -414.2451,  ...,  -906.6632,  -833.4584,\n",
            "        -1017.6682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1295.0112, -1275.6477,  -947.6592,  ...,  -925.2195,  -789.3613,\n",
            "        -1186.6365])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -865.1242, -1192.5958,  -661.0927,  ...,  -872.5803,  -701.0674,\n",
            "         -776.8844])\n",
            "actor loss: 30089.931622029842, critic loss: 25816086.9765625, entropy: 91245.201171875, KL divergence: 0.008500861215508632\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1058355484728006], 離散行動：[0, 1], 連続行動：1.8831056356430054\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "99エピソード目の累積報酬：-101137.86536447261, 一つ保全の回数：14, 二つ保全の回数：8178, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1217.1915, -1124.8118, -1250.4260,  ..., -1570.2996,  -598.1879,\n",
            "        -1332.8726])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -987.3781, -1016.6403,  -839.1302,  ...,  -980.2635,  -762.6105,\n",
            "         -310.6557])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1120.6412,  -574.9934, -1324.2290,  ..., -1006.2943,  -968.0422,\n",
            "         -400.8349])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1371.7234, -1228.7992,  -970.3091,  ..., -1038.1990,  -615.6111,\n",
            "         -875.9189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -883.2931,  -879.5058, -1070.0933,  ...,  -881.5764,  -799.8802,\n",
            "        -1572.2673])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1094.9510,  -941.0419,  -745.9577,  ...,  -912.0685,  -909.0483,\n",
            "        -1441.6410])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -939.1971,  -468.9156, -1134.1133,  ...,  -647.0329, -1175.6025,\n",
            "         -846.4395])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -842.2538, -1031.5129, -1056.9469,  ...,  -677.1679, -1201.4769,\n",
            "         -736.9806])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1003.3786, -1059.6327, -1053.5818,  ..., -1286.8193,  -948.5181,\n",
            "         -478.3285])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -859.2301, -1059.0996, -1109.2495,  ...,  -903.3834,  -903.7986,\n",
            "         -951.2734])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1186.6365, -1198.9099,  -442.1096,  ..., -1513.8146,  -956.2776,\n",
            "         -955.9078])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -988.0698, -1049.9064, -1311.1003,  ...,  -468.5291, -1324.2290,\n",
            "         -654.0204])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -981.8254, -1010.0328,  -943.3634,  ..., -1272.5283,  -661.0805,\n",
            "         -975.0032])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1027.6393, -1004.3378,  -518.0905,  ...,  -826.2126,  -686.1363,\n",
            "         -879.2781])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -925.2251, -1008.1443, -1257.9728,  ...,  -870.4691,  -526.9973,\n",
            "        -1091.3608])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -911.2077,  -577.6340, -1195.1152,  ..., -1050.3281, -1252.5934,\n",
            "         -761.3728])\n",
            "actor loss: 30046.096896149254, critic loss: 26175536.7109375, entropy: 94265.927734375, KL divergence: 0.011739267580710235\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.136775869837259], 離散行動：[0, 1], 連続行動：-0.0008690357208251953\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "100エピソード目の累積報酬：-99507.91201452108, 一つ保全の回数：8, 二つ保全の回数：8184, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -450.5966,  -787.2749,  -992.3080,  ..., -1285.2404, -1148.4547,\n",
            "         -659.6918])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1049.0427,  -806.3101,  -869.2644,  ...,  -718.0736,  -939.3768,\n",
            "         -799.8090])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-575.2078, -696.4243, -802.2759,  ..., -381.7920, -859.1629,\n",
            "        -865.9315])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1502.2794, -1084.3877, -1118.8846,  ..., -1190.0664,  -833.5239,\n",
            "        -1121.5144])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1133.1984,  -878.6914,  -916.1138,  ...,  -391.7213,  -997.2682,\n",
            "         -751.3538])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1072.8373,  -993.2354,  -428.9382,  ...,  -400.0160, -1178.6520,\n",
            "        -1028.5344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1058.6466, -1011.1735,  -857.3873,  ...,  -788.2561, -1171.9813,\n",
            "        -1059.2249])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1092.3256,  -701.8837, -1331.2202,  ..., -1281.8998, -1288.9874,\n",
            "         -994.5636])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -497.9245, -1053.6006, -1455.8872,  ...,  -675.5410,  -984.8326,\n",
            "         -965.4478])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1483.0466,  -969.4930,  -905.5607,  ...,  -645.9378,  -982.7155,\n",
            "         -751.4713])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -420.7574, -1014.9854, -1148.4414,  ...,  -368.4098,  -898.0531,\n",
            "        -1094.7871])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1266.3307, -1195.1396,  -912.4942,  ..., -1085.5818,  -732.5187,\n",
            "         -596.0811])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -583.1243,  -838.4282,  -887.9548,  ...,  -708.6567, -1453.4576,\n",
            "        -1010.9156])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -791.2922, -1022.1983,  -873.1079,  ...,  -576.5803, -1198.5702,\n",
            "         -931.2106])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1211.5502,  -830.6406, -1155.6686,  ...,  -813.3998,  -833.8660,\n",
            "         -761.0806])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1457.4865,  -947.8727,  -651.2610,  ..., -1013.8071,  -531.1773,\n",
            "        -1055.0752])\n",
            "actor loss: 30006.76646105762, critic loss: 26204513.6796875, entropy: 95507.17822265625, KL divergence: 0.014079929209526724\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0448073902311328], 離散行動：[0, 1], 連続行動：-2.2141401767730713\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "101エピソード目の累積報酬：-100225.41628928846, 一つ保全の回数：7, 二つ保全の回数：8185, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1219.6097,  -629.6376, -1179.0538,  ..., -1206.7773,  -803.8350,\n",
            "         -283.5056])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -678.3511, -1135.3436,  -614.4180,  ...,  -455.6992,  -873.7148,\n",
            "         -865.5914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1073.4961,  -692.3744, -1040.1396,  ..., -1091.6295, -1045.2964,\n",
            "         -819.8640])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -651.1562,  -973.2075, -1037.4626,  ..., -1041.4631,  -956.5068,\n",
            "         -948.0808])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -855.3812,  -805.6131, -1082.2317,  ...,  -870.7656,  -658.6085,\n",
            "         -739.0470])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -901.3540, -1168.4188,  -763.6006,  ...,  -789.6452,  -944.3057,\n",
            "        -1081.5383])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -584.3281,  -775.7228, -1120.5220,  ...,  -957.3578, -1457.0387,\n",
            "         -640.3444])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -978.3680, -1227.0424, -1139.1804,  ...,  -875.3414,  -917.9255,\n",
            "         -900.2050])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1143.2468, -1183.0928, -1037.5728,  ..., -1153.8828, -1389.0641,\n",
            "        -1114.5653])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -552.6395,  -720.4486, -1411.9014,  ...,  -881.5626,  -788.8470,\n",
            "        -1366.2959])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1362.6753,  -588.2709, -1200.3279,  ...,  -827.2974,  -957.1750,\n",
            "         -783.7519])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -867.5176, -1170.8882, -1284.5537,  ...,  -769.3782,  -958.9974,\n",
            "         -797.2957])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -879.4935,  -958.4187,  -876.6426,  ...,  -999.1126,  -484.2085,\n",
            "        -1180.0533])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -868.4728, -1069.4087,  -716.3840,  ...,  -656.2161,  -622.8694,\n",
            "        -1047.4379])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1201.8447,  -669.6002,  -521.9381,  ...,  -787.8116, -1412.2849,\n",
            "         -903.6015])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -953.6254, -1097.2512,  -440.2534,  ..., -1143.1743,  -875.3414,\n",
            "        -1026.8608])\n",
            "actor loss: 29983.0270304136, critic loss: 26659385.2421875, entropy: 95961.26220703125, KL divergence: 0.0039775221027474154\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.8342852539607193], 離散行動：[0, 1], 連続行動：-0.28812551498413086\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "102エピソード目の累積報酬：-100213.57095241176, 一つ保全の回数：13, 二つ保全の回数：8179, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -777.7755, -1034.6925,  -732.0709,  ...,  -632.8020, -1241.2200,\n",
            "         -990.8896])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1085.5151,  -686.8088,  -770.7668,  ...,  -994.0031, -1317.3671,\n",
            "         -921.1652])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -880.5536, -1295.1044, -1135.9246,  ..., -1042.5652, -1278.5835,\n",
            "         -740.8058])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -994.6751, -1198.9034,  -667.1442,  ..., -1082.8516,  -784.1063,\n",
            "         -714.8995])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -831.9045, -1409.6714, -1455.9050,  ...,  -731.0602,  -857.6832,\n",
            "         -459.0047])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1324.0042,  -982.9905,  -986.5012,  ...,  -949.0670,  -969.3251,\n",
            "        -1239.5085])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -725.6201, -1054.5830, -1075.7690,  ..., -1184.3302, -1161.9028,\n",
            "         -836.9181])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1066.0817,  -492.8783,  -710.3922,  ..., -1448.5200,  -822.9477,\n",
            "         -985.6121])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -403.1306,  -265.7707,  -623.7197,  ..., -1431.0533, -1518.6234,\n",
            "         -804.4136])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1378.0989,  -749.6008, -1012.4446,  ..., -1179.1254, -1069.1765,\n",
            "        -1107.0312])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -936.9544, -1280.6770,  -652.0140,  ..., -1093.3553, -1111.4180,\n",
            "        -1013.3759])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1243.4392,  -844.7239,  -388.1905,  ..., -1007.6744, -1608.8116,\n",
            "        -1090.3503])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -986.1285,  -909.4366,  -864.3769,  ..., -1147.3837,  -797.0036,\n",
            "         -971.0602])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -721.5906,  -526.7572,  -791.8781,  ..., -1248.4662,  -908.9961,\n",
            "         -751.4715])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1117.8832,  -935.7533,  -366.9393,  ..., -1321.3480,  -982.9905,\n",
            "         -673.4259])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1593.3009,  -868.0871,  -636.8816,  ...,  -645.7778, -1004.5146,\n",
            "         -802.0104])\n",
            "actor loss: 29995.0878827432, critic loss: 26833219.4765625, entropy: 96275.4287109375, KL divergence: 0.0012709661772185206\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0925098855442006], 離散行動：[0, 1], 連続行動：2.6112804412841797\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "103エピソード目の累積報酬：-101416.34774218472, 一つ保全の回数：5, 二つ保全の回数：8187, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -641.9955,  -793.5421, -1116.0441,  ..., -1294.6796,  -808.6830,\n",
            "         -464.7350])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1219.8461, -1563.5145,  -490.5180,  ...,  -926.1415,  -790.2735,\n",
            "        -1383.8796])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1365.4343,  -934.3125,  -785.3076,  ...,  -422.3734,  -949.4885,\n",
            "         -786.1607])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -906.8151,  -908.5663, -1281.3866,  ...,  -965.9617,  -875.4889,\n",
            "        -1248.6331])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -970.3029, -1282.9330, -1119.7866,  ...,  -867.1575,  -711.7747,\n",
            "        -1442.1844])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1246.0790,  -726.7777,  -962.5469,  ..., -1166.6105, -1363.4836,\n",
            "         -703.5192])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1051.9928,  -512.8756,  -660.8549,  ...,  -736.4156, -1134.4393,\n",
            "        -1132.2853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -888.2852, -1082.4144, -1206.7704,  ...,  -557.9396,  -681.1132,\n",
            "         -991.8795])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -953.5604,  -884.8868,  -936.8489,  ...,  -769.9882,  -817.4798,\n",
            "        -1253.0636])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1069.1227,  -377.4828,  -981.8946,  ...,  -650.7134, -1101.7800,\n",
            "        -1455.7085])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-608.4471, -433.6111, -735.3502,  ..., -997.7735, -975.1986,\n",
            "        -737.0941])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1227.6254, -1327.6301, -1091.3438,  ...,  -784.0724, -1220.2153,\n",
            "        -1394.4525])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1072.5486, -1039.8043, -1102.8826,  ...,  -764.1956,  -956.5122,\n",
            "        -1243.8412])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -957.9733, -1056.4270, -1093.2799,  ...,  -567.6660, -1264.7157,\n",
            "        -1359.5619])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -938.1779,  -923.0464,  -823.1461,  ...,  -766.3899, -1076.4987,\n",
            "         -872.8730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -863.1932, -1008.8538,  -663.9274,  ..., -1112.2076, -1062.6504,\n",
            "        -1292.9248])\n",
            "actor loss: 29953.00013794704, critic loss: 27216593.9609375, entropy: 96736.26025390625, KL divergence: 0.0010933258459924506\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.0306411215074474], 離散行動：[0, 1], 連続行動：-0.3084709048271179\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "104エピソード目の累積報酬：-100000.00000015616, 一つ保全の回数：2, 二つ保全の回数：8190, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -971.2565,  -937.4613,  -369.9425,  ..., -1239.7786, -1025.5648,\n",
            "         -814.1221])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1360.4589, -1089.7410, -1389.7155,  ...,  -839.7538,  -920.2044,\n",
            "         -997.5379])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -774.5301, -1229.5112, -1184.3582,  ...,  -855.3925,  -809.9174,\n",
            "         -808.3722])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1504.0868,  -723.7529, -1158.3079,  ..., -1200.5319, -1232.1741,\n",
            "         -493.8344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1034.6744, -1497.3567,  -514.3257,  ..., -1028.8646,  -736.0285,\n",
            "        -1298.5729])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1049.7438,  -600.4180, -1032.2836,  ..., -1132.9934,  -829.5994,\n",
            "        -1532.4594])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -559.0452, -1068.5797,  -807.8641,  ...,  -877.0344, -1019.7164,\n",
            "         -513.1403])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1092.4717,  -489.5564,  -688.0341,  ..., -1047.8899,  -774.1402,\n",
            "        -1109.4214])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -815.3931,  -997.9354, -1328.7653,  ..., -1051.4083, -1008.7883,\n",
            "         -986.5275])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -798.1765,  -923.8666, -1072.7904,  ...,  -844.5270,  -981.7617,\n",
            "        -1200.4812])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1076.7574, -1256.9994,  -969.8459,  ...,  -597.4527,  -977.4696,\n",
            "         -944.0225])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1135.9570,  -481.1455, -1276.1318,  ..., -1068.1924,  -727.2442,\n",
            "         -635.3019])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -627.1809,  -976.5599,  -667.6464,  ...,  -957.2769, -1014.1713,\n",
            "         -753.9175])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1415.4216, -1272.1256,  -951.2249,  ...,  -919.9723,  -730.3866,\n",
            "         -996.2772])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1316.1949,  -710.6632,  -855.9396,  ...,  -887.0764, -1372.3899,\n",
            "         -985.2098])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1133.0052, -1323.2948, -1274.6378,  ..., -1111.2675,  -972.6597,\n",
            "         -899.1685])\n",
            "actor loss: 29942.615462772934, critic loss: 27270757.8046875, entropy: 97683.50146484375, KL divergence: 0.009512782850028653\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.3000914759137134], 離散行動：[0, 1], 連続行動：1.3868377208709717\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "105エピソード目の累積報酬：-99999.99858368233, 一つ保全の回数：4, 二つ保全の回数：8188, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1304.6823,  -635.5168,  -909.4780,  ...,  -799.8315, -1100.5828,\n",
            "         -984.8912])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1090.6261, -2874.1836, -1148.1210,  ..., -1042.1812,  -518.0758,\n",
            "         -942.9603])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -352.1069,  -750.1310, -1069.1777,  ..., -1065.1833,  -812.6992,\n",
            "         -818.4517])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1240.6311, -1125.4869,  -651.0159,  ...,  -570.6184,  -878.2760,\n",
            "         -877.7406])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -622.6489, -1566.9125, -1425.8435,  ...,  -721.4798,  -970.2194,\n",
            "        -1022.8492])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1173.2040,  -487.5316, -1154.2786,  ...,  -993.9485, -1271.1652,\n",
            "         -559.6985])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -553.4377, -1193.9398,  -934.8802,  ..., -1060.7775,  -873.8784,\n",
            "         -975.4553])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -957.6820, -1258.5258, -1460.4332,  ..., -1244.4958,  -855.9792,\n",
            "         -992.2899])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-720.8329, -325.2991, -779.0582,  ..., -971.4338, -866.4887,\n",
            "        -957.7509])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1338.2682,  -809.4407,  -585.3431,  ...,  -880.7473, -1470.5045,\n",
            "         -835.7034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-963.2515, -950.1288, -879.5667,  ..., -928.3724, -699.8778,\n",
            "        -998.5021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1243.1484,  -752.3284, -1493.8857,  ...,  -685.8589,  -652.9259,\n",
            "        -1046.0200])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -840.8906, -1165.7913,  -963.0571,  ...,  -821.8797, -1436.7288,\n",
            "        -1136.7209])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1137.3652, -1076.0480, -1155.0983,  ...,  -855.1126,  -634.7714,\n",
            "         -924.4922])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -510.0353,  -930.0284, -1019.8944,  ...,  -732.5912, -1025.4686,\n",
            "         -439.4434])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -856.7609,  -857.5419, -1133.0973,  ...,  -869.0659, -1205.2360,\n",
            "        -1411.0012])\n",
            "actor loss: 29956.674615919284, critic loss: 27491622.2421875, entropy: 99323.47021484375, KL divergence: 0.0028035865594658728\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.0583383002034683], 離散行動：[0, 1], 連続行動：-1.5668511390686035\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "106エピソード目の累積報酬：-100000.01881112583, 一つ保全の回数：7, 二つ保全の回数：8185, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -897.4584,  -745.4211,  -801.1100,  ..., -1041.3658, -1240.2467,\n",
            "        -1290.3688])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -609.1672, -1439.3528, -1064.0718,  ..., -1256.8488,  -906.9299,\n",
            "         -632.7363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1186.5519,  -849.6368, -1108.2898,  ...,  -517.5570,  -865.0032,\n",
            "         -919.8864])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -899.4317, -1362.8298,  -344.7898,  ...,  -962.2451,  -494.3784,\n",
            "         -741.6136])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -604.7990,  -904.2919, -1068.8901,  ...,  -902.7185, -1073.6024,\n",
            "        -1059.3374])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -884.5294,  -547.9894,  -561.5630,  ..., -1087.4093,  -995.2615,\n",
            "        -1078.0990])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -955.3168,  -976.4542,  -817.8116,  ..., -1285.6759,  -942.3717,\n",
            "         -574.4272])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -777.4738, -1048.0001,  -390.7413,  ...,  -794.4432, -1242.1283,\n",
            "        -1226.5887])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -906.3827,  -691.2828,  -848.7248,  ...,  -793.6110, -1285.2782,\n",
            "        -1017.2172])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -377.6428, -1101.5089,  -818.8441,  ...,  -820.3714,  -725.2477,\n",
            "         -954.9705])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -916.1826, -1049.9086,  -328.1673,  ...,  -543.0206, -1347.6218,\n",
            "         -921.4024])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -957.0303, -1327.9727, -1437.3033,  ..., -1024.1189,  -853.2133,\n",
            "        -1082.5184])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -486.5650,  -948.2906,  -832.7670,  ...,  -854.8423, -1036.1812,\n",
            "        -1265.4010])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1078.1409, -1042.9281,  -694.4538,  ..., -1136.8135,  -780.2665,\n",
            "        -1245.9753])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -765.2554, -1151.6626, -1074.3845,  ...,  -485.4095,  -987.0400,\n",
            "        -1344.9243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -978.3816, -1115.1305,  -690.0969,  ..., -1173.6473, -1035.8599,\n",
            "         -645.8742])\n",
            "actor loss: 29925.531143739387, critic loss: 27969258.4140625, entropy: 100465.185546875, KL divergence: 0.007440220129267665\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1012505604089156], 離散行動：[0, 1], 連続行動：-0.2727123498916626\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "107エピソード目の累積報酬：-100000.01104282297, 一つ保全の回数：5, 二つ保全の回数：8187, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -411.9034,  -651.3438,  -932.2405,  ...,  -881.1833,  -651.2465,\n",
            "        -1148.4878])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1273.9037, -1097.6399,  -609.7247,  ...,  -488.8463, -1164.1127,\n",
            "        -1156.8873])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1187.8002,  -996.6736,  -696.6278,  ..., -1266.5256,  -541.3654,\n",
            "         -470.3145])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -708.1865,  -833.5910, -1267.1348,  ..., -1329.2848,  -806.0364,\n",
            "        -1189.2366])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1226.2456,  -810.8452,  -993.4221,  ...,  -680.6000, -1126.2437,\n",
            "         -340.9707])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -720.1363, -1018.5826,  -522.4385,  ...,  -986.2260,  -671.8368,\n",
            "        -1063.0597])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -438.3431,  -683.3400,  -848.6061,  ..., -1052.1053,  -753.7498,\n",
            "         -724.1725])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1510.8425, -1083.9492, -1058.2860,  ...,  -848.0582,  -852.6110,\n",
            "        -1536.1178])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -792.2642,  -879.1243,  -735.8380,  ..., -1285.1716, -1025.9827,\n",
            "         -792.3181])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1014.2360,  -734.4138,  -777.8255,  ..., -1083.2571,  -656.5706,\n",
            "        -1294.9921])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -855.1830, -1205.6947,  -943.9636,  ...,  -764.4031,  -917.0219,\n",
            "        -1047.7311])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -946.7573,  -875.9675, -1017.2025,  ...,  -808.9269,  -817.0961,\n",
            "         -946.8741])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -905.7371,  -916.8954,  -874.5902,  ...,  -722.7932, -1029.1017,\n",
            "        -1000.2176])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1477.1760,  -612.4536, -1325.9427,  ...,  -947.9005,  -961.4663,\n",
            "         -974.6840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1269.7344,  -749.8507, -1300.4485,  ...,  -488.8463, -1128.2483,\n",
            "        -1308.1785])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1047.7722,  -583.2255, -1454.3004,  ..., -1195.9341,  -547.3520,\n",
            "         -972.3411])\n",
            "actor loss: 29938.314861090355, critic loss: 28008599.875, entropy: 101873.05322265625, KL divergence: 0.0034153615995088237\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1974980063560305], 離散行動：[0, 1], 連続行動：-0.12430769205093384\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "108エピソード目の累積報酬：-100007.36392238857, 一つ保全の回数：9, 二つ保全の回数：8183, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -467.3035, -1230.8682,  -684.2993,  ...,  -919.8934,  -812.0882,\n",
            "         -845.4440])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-757.6219, -894.0765, -874.1111,  ..., -868.2630, -872.7319,\n",
            "        -323.3680])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1024.0084,  -935.9546,  -554.1489,  ..., -1038.3243,  -666.8278,\n",
            "         -988.7809])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -762.9583, -1101.4242,  -830.3137,  ..., -1053.2645, -1008.9357,\n",
            "         -724.1230])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -804.4933, -1287.5326,  -765.9517,  ..., -1054.8081,  -649.8514,\n",
            "         -597.6210])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -699.7109, -1191.1538,  -847.6953,  ...,  -990.3486,  -824.5988,\n",
            "         -601.9621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -668.7697,  -457.5346, -1423.9775,  ...,  -868.4683,  -798.3204,\n",
            "         -555.3589])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-780.2514, -875.2441, -930.5529,  ..., -650.0817, -964.1973,\n",
            "        -878.6558])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1043.5436, -1320.1630,  -924.0271,  ...,  -857.9972,  -877.0650,\n",
            "         -875.0446])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1090.6659, -1007.9931, -1385.9454,  ..., -1126.1663,  -857.7721,\n",
            "         -651.1444])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1426.2358, -1053.8248,  -840.8553,  ..., -1215.3110, -1001.7626,\n",
            "         -569.6262])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -962.3220,  -991.4660,  -858.0505,  ..., -1055.2251,  -818.2556,\n",
            "         -796.9227])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -858.0505, -1127.3958, -1024.0084,  ...,  -888.0700,  -502.3134,\n",
            "        -1454.8063])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -857.0451, -1189.9637,  -711.3464,  ...,  -315.9810,  -968.4461,\n",
            "         -903.0783])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -826.5628, -1050.5344,  -722.7256,  ..., -1073.8267, -1145.2047,\n",
            "         -874.6703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -652.8089,  -906.9398,  -793.2500,  ..., -1043.9762, -1091.2216,\n",
            "         -903.9713])\n",
            "actor loss: 29894.189358430347, critic loss: 28514593.3125, entropy: 102028.15673828125, KL divergence: 0.002336157356413517\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9387718602067827], 離散行動：[0, 1], 連続行動：0.32603077590465546\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "109エピソード目の累積報酬：-100000.00000000063, 一つ保全の回数：4, 二つ保全の回数：8188, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -685.6817,  -689.6384, -1191.7156,  ...,  -814.7164,  -930.1751,\n",
            "         -923.7640])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -515.8300, -1045.2793, -1208.3909,  ...,  -845.3788, -1150.0878,\n",
            "         -746.1689])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-614.1538, -921.3382, -614.6890,  ..., -945.3419, -935.4907,\n",
            "        -716.8686])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -945.2431, -1330.8134,  -705.5025,  ...,  -499.6388,  -989.6307,\n",
            "        -1271.0585])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1340.6688, -1189.9667,  -572.5712,  ...,  -697.9839,  -821.7950,\n",
            "         -945.7535])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -753.2643,  -773.4033, -1316.6996,  ..., -1128.1908,  -736.8793,\n",
            "         -926.9011])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -868.0418, -1096.3203, -1006.2668,  ...,  -817.3694, -1095.0618,\n",
            "         -889.5126])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -931.0564,  -913.6675,  -971.0369,  ..., -1179.3564, -1001.5178,\n",
            "         -815.5091])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1146.8330, -1056.1711, -1012.6888,  ...,  -987.5054, -1036.5508,\n",
            "         -849.7682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-642.1777, -815.4709, -796.7939,  ..., -718.1857, -624.0146,\n",
            "        -888.5422])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1147.3260,  -665.1674, -1095.3007,  ...,  -672.5953, -1031.2937,\n",
            "         -646.3865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1257.5347,  -505.5383,  -888.0533,  ..., -1179.1835, -1231.2408,\n",
            "         -947.0095])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -640.9580, -1107.4915,  -875.5883,  ..., -1024.9531,  -903.0663,\n",
            "        -1019.6044])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-724.1533, -358.4288, -607.8190,  ..., -966.8524, -849.6547,\n",
            "        -523.8680])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-804.5058, -812.7774, -801.8005,  ..., -568.8332, -974.9078,\n",
            "        -577.3438])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -901.8743, -1245.9774,  -737.5291,  ...,  -387.9124,  -846.9445,\n",
            "         -942.8465])\n",
            "actor loss: 29915.10337499566, critic loss: 28657537.1171875, entropy: 102068.61328125, KL divergence: 0.0012428217890835046\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2411551930290012], 離散行動：[0, 1], 連続行動：0.5399225428700447\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "110エピソード目の累積報酬：-100018.07869148592, 一つ保全の回数：8, 二つ保全の回数：8184, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -944.2377, -1254.4486,  -683.3368,  ...,  -809.4493,  -826.0195,\n",
            "        -1382.3729])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1192.3893, -1250.2810, -1056.4930,  ..., -1231.9387,  -953.5927,\n",
            "         -877.3632])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1175.3159, -1257.8652, -1129.9995,  ...,  -739.1995,  -944.0687,\n",
            "         -685.9224])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1125.1704,  -830.9066, -1014.2810,  ..., -1253.7279,  -782.2809,\n",
            "         -540.6500])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1276.9459,  -941.4472,  -543.8753,  ..., -1161.9689, -1215.0471,\n",
            "        -1159.8723])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1323.2384,  -595.7618,  -956.1382,  ...,  -664.4199, -1158.2363,\n",
            "        -1061.3917])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -684.7468,  -888.6370,  -703.9283,  ..., -1022.1060,  -891.8815,\n",
            "         -855.6790])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -889.2747,  -841.2588,  -781.7791,  ..., -1234.0641,  -805.1484,\n",
            "        -1095.1183])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1452.5219,  -865.7680,  -953.8478,  ..., -1354.6360,  -953.5927,\n",
            "         -929.6666])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -837.4917,  -784.0968,  -938.4158,  ...,  -779.6860,  -894.3468,\n",
            "        -1247.8407])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1155.3768, -1056.2314,  -890.4734,  ..., -1130.1459, -1045.9296,\n",
            "        -1149.2012])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -502.6323,  -882.7197, -1131.0242,  ..., -1125.5555, -1272.0573,\n",
            "         -722.8807])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1115.6234,  -938.0787,  -680.8257,  ..., -1375.4282,  -563.7534,\n",
            "        -1020.4044])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -672.7581, -1226.1699, -1060.1560,  ..., -1195.8116, -1052.5902,\n",
            "        -1067.0703])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-937.3610, -884.8822, -891.8645,  ..., -911.5965, -916.6210,\n",
            "        -659.2682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -765.4033,  -757.6119, -1026.1902,  ..., -1156.9991,  -989.8623,\n",
            "         -793.4917])\n",
            "actor loss: 29835.787668810855, critic loss: 28960528.4296875, entropy: 103943.8369140625, KL divergence: 0.014116442124274447\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6851902331416548], 離散行動：[0, 1], 連続行動：-4.569624900817871\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "111エピソード目の累積報酬：-100478.13453837036, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1202.9028,  -646.2270,  -756.9193,  ...,  -718.9475,  -957.9208,\n",
            "        -1155.0859])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1129.9148,  -887.3944, -1050.7112,  ..., -1223.3250,  -974.5690,\n",
            "         -937.9379])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1033.2432,  -949.0511, -1264.8756,  ...,  -866.5602, -1260.8577,\n",
            "         -654.8768])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -640.6597,  -838.7432,  -957.0646,  ...,  -893.9098, -1210.0461,\n",
            "        -1004.6301])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1208.0504,  -721.3561,  -801.1189,  ..., -1489.2108, -1030.7594,\n",
            "         -618.0568])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -849.3409,  -919.3528, -1080.8824,  ...,  -735.1605, -1034.9231,\n",
            "         -940.2935])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -860.9839,  -591.4152, -1042.8842,  ..., -1327.6815, -1077.3224,\n",
            "         -974.7188])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -661.7640, -1155.5986, -1260.7794,  ...,  -473.7812, -1166.3428,\n",
            "         -976.0230])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -834.4622,  -822.4904,  -499.2082,  ...,  -526.4544,  -895.4497,\n",
            "        -1043.2585])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1179.8137,  -923.1213, -1290.4908,  ...,  -909.8766,  -963.7818,\n",
            "        -1034.6056])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1439.2949, -1158.7490, -1382.5240,  ..., -1064.3981, -1018.5006,\n",
            "        -1073.7086])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -679.9809, -1227.4498, -1251.7385,  ...,  -565.4618,  -871.1823,\n",
            "        -1310.7032])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1172.6193,  -975.0677,  -966.6467,  ..., -1281.1786, -1188.0151,\n",
            "         -700.3025])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -893.2635,  -740.8058, -1265.9852,  ...,  -692.7560,  -766.1468,\n",
            "        -1108.0829])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -823.9531,  -996.4552,  -649.5886,  ...,  -675.4915, -1163.3318,\n",
            "        -1074.6522])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -837.7256,  -539.6690, -1195.0449,  ..., -1164.6873,  -690.4647,\n",
            "         -496.1995])\n",
            "actor loss: 29851.59115591434, critic loss: 29283799.3828125, entropy: 105295.76953125, KL divergence: 0.008442353752178436\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9541955388286008], 離散行動：[0, 1], 連続行動：0.624963141977787\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "112エピソード目の累積報酬：-100000.00172024807, 一つ保全の回数：5, 二つ保全の回数：8187, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -661.4764, -1122.1962, -1065.2676,  ..., -1200.4272,  -686.3154,\n",
            "         -926.7643])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1005.1828,  -674.0854,  -856.3038,  ...,  -971.8611, -1094.6379,\n",
            "         -890.0079])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -846.9496,  -986.0799,  -840.7657,  ..., -1007.5543, -1414.5468,\n",
            "         -856.3519])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1140.4326,  -526.7930,  -935.4114,  ..., -1065.3777,  -523.5463,\n",
            "         -759.4848])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-820.5573, -997.6514, -902.5010,  ..., -776.8100, -794.5743,\n",
            "        -554.3512])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1186.6066,  -471.4690,  -977.4421,  ..., -1073.5927,  -961.2213,\n",
            "         -810.4051])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -903.5662,  -934.9501,  -807.7448,  ...,  -866.3085, -1352.0876,\n",
            "        -1062.9329])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -596.5896,  -864.0114,  -764.9300,  ..., -1038.8895, -1105.6282,\n",
            "         -732.7657])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1465.7207, -1102.7445,  -939.5524,  ...,  -812.2075, -1095.6713,\n",
            "        -1178.9672])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1078.4011, -1032.1851,  -826.7417,  ...,  -872.9019, -1323.9996,\n",
            "         -809.3908])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -907.8766,  -663.0515,  -551.6054,  ...,  -528.1414,  -820.0688,\n",
            "        -1126.7684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -792.0203,  -572.4243,  -984.2096,  ...,  -945.9427,  -899.7027,\n",
            "        -1037.9921])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1071.2158,  -920.6749,  -821.3380,  ..., -1283.6101,  -859.2119,\n",
            "        -1041.4706])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -642.5844, -1011.6917, -1035.2496,  ..., -1014.5618,  -787.2378,\n",
            "         -896.5350])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -889.3407, -1078.0981, -1302.9175,  ..., -1135.3240,  -843.5851,\n",
            "         -962.3118])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1236.2734, -1334.0343, -1509.6256,  ..., -1325.6765,  -699.5072,\n",
            "        -1070.4679])\n",
            "actor loss: 29845.119879170794, critic loss: 29323022.2265625, entropy: 106578.345703125, KL divergence: 0.0027198935208913337\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.3566111672752017], 離散行動：[0, 1], 連続行動：-0.3632035255432129\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "113エピソード目の累積報酬：-100000.07648980497, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -917.8189,  -696.5551,  -372.2823,  ..., -1029.8276,  -835.3083,\n",
            "         -817.2902])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -892.2898,  -722.7492, -1069.1913,  ...,  -842.5061,  -988.8074,\n",
            "        -1088.4543])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -987.2485, -1356.5868,  -701.7469,  ...,  -581.8638,  -473.3047,\n",
            "         -970.1064])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -908.8160,  -466.2107,  -930.3342,  ...,  -782.8548, -1286.0577,\n",
            "        -1017.9423])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1009.8980,  -988.3612, -1069.1375,  ..., -1006.0347,  -937.8172,\n",
            "         -911.7094])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1161.0752, -1197.4653,  -815.8881,  ...,  -917.8189,  -663.3156,\n",
            "         -817.9122])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1037.6453,  -782.2242,  -858.2926,  ...,  -794.8347,  -565.6144,\n",
            "        -1147.8762])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -602.0123, -1230.6240,  -990.5427,  ...,  -917.8655, -1358.9576,\n",
            "        -1177.5312])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-678.4371, -904.4026, -925.8107,  ..., -887.5674, -917.3836,\n",
            "        -772.5095])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -872.2227,  -740.6544, -1168.8251,  ..., -1236.5696, -1125.2438,\n",
            "         -786.5322])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1293.9265, -1188.7898, -1191.1578,  ..., -1005.7512,  -660.5015,\n",
            "         -878.7446])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -916.7872, -1161.7559,  -730.2994,  ...,  -688.6640,  -779.9423,\n",
            "         -952.2173])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -995.6011,  -978.7764, -1077.5399,  ...,  -704.0366,  -822.7328,\n",
            "        -1219.3850])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -941.1265,  -737.5477, -1197.8784,  ...,  -735.1904,  -925.9068,\n",
            "        -1081.5138])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1075.8955,  -920.2642,  -959.3422,  ..., -1022.8976, -1016.4244,\n",
            "        -1015.5862])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -956.0353,  -890.7921, -1187.9182,  ..., -1163.6448,  -810.0418,\n",
            "         -922.2540])\n",
            "actor loss: 29830.389653763872, critic loss: 29555849.72265625, entropy: 105915.2734375, KL divergence: 0.0009129480147141081\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1255159107914394], 離散行動：[0, 1], 連続行動：2.72399640083313\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "114エピソード目の累積報酬：-100297.65421546208, 一つ保全の回数：7, 二つ保全の回数：8184, 三つ保全の回数：1, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1299.2279,  -815.8056,  -949.4230,  ...,  -912.2003,  -857.2151,\n",
            "        -1476.1111])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-866.4966, -873.7677, -984.9250,  ..., -942.3509, -436.7542,\n",
            "        -838.8734])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -867.3970, -1004.0396, -1034.0470,  ..., -1257.1504,  -592.1980,\n",
            "         -748.2460])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -856.1151, -1275.7998,  -598.0046,  ...,  -815.6622, -1196.0072,\n",
            "         -820.5401])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1008.7491,  -533.7648,  -909.8926,  ...,  -896.7950,  -816.7502,\n",
            "         -617.4495])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -932.5121,  -933.3630, -1388.3154,  ...,  -730.9714,  -975.3402,\n",
            "         -956.8900])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -760.8460, -1138.6392, -1252.8059,  ...,  -590.0449,  -867.3970,\n",
            "         -698.7960])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -884.2270, -1138.5131,  -924.1654,  ..., -1084.2346, -1171.7191,\n",
            "         -954.0286])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1414.2938,  -930.7927,  -830.9849,  ...,  -772.8326, -1187.6246,\n",
            "         -748.7863])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1271.5223, -1179.0455, -1335.7302,  ...,  -550.6974,  -631.6342,\n",
            "        -1263.0995])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1179.2616, -1022.9752,  -963.0318,  ..., -1185.4167,  -949.4230,\n",
            "         -987.0501])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -874.4005,  -936.2419,  -822.4608,  ..., -1127.6388,  -826.8833,\n",
            "        -1376.5220])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1001.0278,  -703.4229,  -958.5453,  ...,  -857.9734, -1101.9373,\n",
            "         -954.6014])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-988.2833, -984.8707, -929.1265,  ..., -621.1515, -795.4706,\n",
            "        -815.0643])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -925.0692,  -694.9552,  -484.7821,  ...,  -814.1121, -1057.0054,\n",
            "         -691.7548])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -806.8680, -1032.5656, -1097.8760,  ...,  -773.2880, -1141.7791,\n",
            "         -663.9971])\n",
            "actor loss: 29808.121091056226, critic loss: 29750624.36328125, entropy: 105624.49462890625, KL divergence: 0.0006608735758824608\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2471250970464063], 離散行動：[0, 1], 連続行動：0.10020431876182556\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "115エピソード目の累積報酬：-100000.00000133278, 一つ保全の回数：5, 二つ保全の回数：8187, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -763.0349,  -788.7042, -1085.6245,  ...,  -592.4703, -1001.9096,\n",
            "        -1194.1272])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1113.5723, -1026.7783, -1007.5419,  ...,  -949.6576,  -936.3043,\n",
            "         -765.2698])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -666.1130,  -760.1826,  -913.8164,  ..., -1073.2578,  -876.6530,\n",
            "        -1183.2838])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1351.8098, -1265.6368, -1166.0604,  ...,  -904.8052, -1070.4713,\n",
            "        -1131.7280])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -872.5319,  -944.4304,  -905.8583,  ..., -1097.7206, -1106.4329,\n",
            "        -1309.9468])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1068.7631,  -860.5853, -1449.2975,  ...,  -625.6890, -1098.7322,\n",
            "         -724.1929])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-973.9500, -940.1334, -743.7364,  ..., -767.7985, -598.4905,\n",
            "        -995.2526])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -583.7492,  -594.7985,  -814.0961,  ...,  -645.2189, -1026.2620,\n",
            "         -665.3339])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -985.1088,  -635.7509, -1072.4698,  ...,  -939.9257,  -584.1288,\n",
            "         -772.3182])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -795.6902,  -891.6538,  -790.6935,  ..., -1102.9993, -1226.3340,\n",
            "         -637.0758])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1033.3414,  -900.3597, -1006.7078,  ..., -1029.4655,  -672.4056,\n",
            "         -923.5978])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -956.0940,  -758.3611, -1014.4257,  ...,  -656.6439, -1002.5630,\n",
            "         -769.2857])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -850.8832,  -591.0283, -1230.7028,  ..., -1094.9824,  -707.6052,\n",
            "         -667.1161])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -630.9280,  -816.7399, -1034.2651,  ...,  -665.8444, -1075.5017,\n",
            "        -1140.0476])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1141.7225,  -851.1611,  -895.5308,  ...,  -769.2857, -1120.3092,\n",
            "         -877.8951])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -602.6591, -1248.4208,  -867.1644,  ..., -1343.5502,  -413.7081,\n",
            "        -1074.2445])\n",
            "actor loss: 29783.27824759947, critic loss: 30238250.93359375, entropy: 105805.6357421875, KL divergence: 0.0007960333563434794\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.2673016999997246], 離散行動：[0, 1], 連続行動：-0.897759199142456\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "116エピソード目の累積報酬：-100000.11255298469, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1235.3562,  -818.7951, -1357.5883,  ...,  -631.8284, -1214.2415,\n",
            "         -847.1485])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1104.8774,  -893.5403, -1300.5653,  ...,  -902.0851,  -663.3339,\n",
            "         -686.1976])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -943.2402,  -762.0249,  -915.7936,  ..., -1068.1460,  -634.3379,\n",
            "        -1035.2810])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -768.4100, -1432.2246,  -962.7385,  ...,  -734.4617,  -976.5527,\n",
            "        -1206.6652])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -793.7469, -1290.5029,  -880.6649,  ..., -1016.9647,  -902.3905,\n",
            "         -358.3834])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1209.1494, -1041.6870,  -807.1782,  ...,  -565.4314,  -921.3971,\n",
            "         -768.5265])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -686.8325, -1135.5334, -1021.1450,  ..., -1362.5464, -1067.5112,\n",
            "         -688.4756])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -838.6935, -1052.8459, -1119.4086,  ..., -1461.9424, -1007.5454,\n",
            "        -1014.7914])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -727.4323,  -905.2823,  -444.3636,  ...,  -982.6956, -1110.7198,\n",
            "        -1202.1257])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -706.7706, -1254.4519,  -688.0071,  ..., -1037.3063, -1458.5469,\n",
            "         -899.6322])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1078.2137,  -736.2382, -1027.4701,  ...,  -842.0665,  -722.7943,\n",
            "         -979.3790])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -911.8662,  -825.0215,  -600.6686,  ..., -1154.7344,  -739.5649,\n",
            "         -677.4106])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1291.2408,  -865.0628,  -549.7097,  ...,  -872.0587, -1205.6619,\n",
            "         -836.0027])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -978.7839,  -791.5335, -1024.1407,  ..., -1058.9032, -1379.2151,\n",
            "        -1180.7001])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1300.1484, -1011.8433,  -710.8965,  ..., -1110.9207,  -960.3274,\n",
            "        -1294.9978])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1099.0851, -1377.4536,  -756.0748,  ...,  -801.2589,  -975.8679,\n",
            "         -606.3808])\n",
            "actor loss: 29780.71636578018, critic loss: 30535193.3984375, entropy: 105752.2802734375, KL divergence: 0.0010416960232819711\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.5203958774100403], 離散行動：[0, 1], 連続行動：-0.6840417385101318\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "117エピソード目の累積報酬：-100456.20519564953, 一つ保全の回数：4, 二つ保全の回数：8188, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -946.1637,  -752.4474,  -794.4334,  ..., -1012.0391,  -886.2903,\n",
            "         -949.1149])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -993.5563, -1183.8087,  -885.4563,  ...,  -921.5261,  -806.5219,\n",
            "        -1304.7782])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1250.1678,  -980.7883,  -982.6036,  ...,  -960.8633, -1207.2245,\n",
            "        -1443.6281])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1110.9459,  -770.7979,  -836.7415,  ...,  -890.7512,  -754.7043,\n",
            "         -806.0188])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -956.6319,  -913.9296,  -700.6564,  ..., -1150.8264,  -623.8282,\n",
            "         -634.0046])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1047.1150,  -957.8525,  -881.4361,  ...,  -973.9940,  -934.7513,\n",
            "         -895.9921])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -822.0841,  -795.7182, -1118.3910,  ...,  -916.9019,  -788.5264,\n",
            "        -1138.8345])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -960.2581,  -871.0428,  -753.1443,  ..., -1023.7892,  -590.7313,\n",
            "         -591.9712])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1429.4702,  -990.1409,  -901.5191,  ..., -1222.5236,  -542.7649,\n",
            "         -911.2987])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -648.3821,  -723.7833,  -930.7180,  ...,  -819.7223,  -865.1610,\n",
            "        -1039.0071])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -932.6091,  -653.9745,  -552.5386,  ...,  -978.0615, -1121.9490,\n",
            "        -1166.3392])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -918.3337, -1313.6082, -1164.2903,  ...,  -977.0051, -1145.3457,\n",
            "        -1336.3439])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1336.3439, -1085.6062,  -902.7471,  ..., -1280.7701,  -972.7786,\n",
            "        -1177.1865])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -850.9185,  -828.0411, -1334.6158,  ..., -1127.0488,  -947.3611,\n",
            "        -1142.8512])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -833.6700, -1235.2871,  -629.4477,  ...,  -932.9843,  -837.9608,\n",
            "         -410.0730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1257.4774, -1071.0562,  -765.1776,  ...,  -555.4612,  -885.1434,\n",
            "        -1193.4236])\n",
            "actor loss: 29753.274472578207, critic loss: 30731826.57421875, entropy: 106260.90576171875, KL divergence: 0.0024190742309857733\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.621067604808193], 離散行動：[0, 1], 連続行動：-2.080237627029419\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "118エピソード目の累積報酬：-100000.06607169783, 一つ保全の回数：2, 二つ保全の回数：8190, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1183.3992, -1049.8759,  -763.7667,  ..., -1093.4789,  -759.4241,\n",
            "        -1090.8688])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -950.5613, -1203.7014, -1094.5299,  ...,  -847.2449,  -567.1052,\n",
            "         -860.8194])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -924.4178,  -715.0120,  -694.4780,  ..., -1015.5117,  -797.5344,\n",
            "         -856.2859])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -851.1544, -1033.9086, -1302.4323,  ..., -1020.9902,  -894.3522,\n",
            "        -1017.8166])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1026.4713, -1137.2400,  -657.5945,  ..., -1142.0192,  -688.7295,\n",
            "         -922.3629])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1014.8073,  -457.8182,  -888.2884,  ...,  -854.4455, -1144.6799,\n",
            "         -991.3244])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -642.3268, -1158.5763,  -958.1596,  ...,  -722.3422,  -940.6122,\n",
            "         -823.8758])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1064.1808, -1061.3218, -1191.1669,  ..., -1140.8457,  -942.4654,\n",
            "         -751.5498])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -859.6341, -1121.7794,  -913.3256,  ..., -1036.1505,  -589.1255,\n",
            "        -1057.9803])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-657.2721, -788.3474, -690.2535,  ..., -483.9279, -784.9913,\n",
            "        -964.8105])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -950.6266,  -869.5552,  -645.4839,  ...,  -550.6937, -1100.9569,\n",
            "         -854.4455])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -838.0070,  -859.9872, -1005.7227,  ..., -1016.4609,  -705.5203,\n",
            "         -984.9175])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -755.0819,  -983.7651, -1012.1400,  ..., -1138.3043, -1121.7794,\n",
            "         -913.0514])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -715.0120,  -785.4315, -1173.2585,  ...,  -881.5383, -1046.6375,\n",
            "         -423.2976])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1238.1930,  -856.3190,  -727.4901,  ...,  -930.9528, -1048.3243,\n",
            "         -953.4013])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -890.4945, -1110.6639, -1221.4639,  ..., -1125.2274,  -801.1985,\n",
            "        -1381.2717])\n",
            "actor loss: 29695.60918674139, critic loss: 31153570.9296875, entropy: 108332.86572265625, KL divergence: 0.021815510740539064\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7542248800326403], 離散行動：[0, 1], 連続行動：-3.9494924545288086\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "119エピソード目の累積報酬：-100006.86001084013, 一つ保全の回数：6, 二つ保全の回数：8186, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-804.6296, -929.9084, -784.9575,  ..., -514.6698, -925.3760,\n",
            "        -830.5150])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -462.1108, -1343.9467, -1137.4004,  ...,  -988.2023, -1156.6854,\n",
            "         -874.2271])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -880.5677,  -969.7585,  -849.9188,  ...,  -653.5261,  -832.0406,\n",
            "        -1004.3241])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -623.0448,  -993.0501,  -965.3170,  ..., -1043.3539, -1077.9969,\n",
            "         -907.5340])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -970.1689,  -951.2750,  -931.0096,  ..., -1125.6403,  -771.4835,\n",
            "        -1028.6187])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -839.2021,  -707.6841, -1221.3345,  ..., -1267.0869,  -773.1415,\n",
            "        -1050.7749])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -928.8998,  -551.3488,  -807.2740,  ...,  -597.1285, -1110.8597,\n",
            "        -1198.9185])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -866.3306, -1235.2982,  -779.3319,  ...,  -837.8885,  -903.6606,\n",
            "         -759.5181])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-962.3840, -657.1138, -723.6329,  ..., -624.4889, -496.5193,\n",
            "        -861.2443])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-924.2915, -755.6317, -913.1534,  ..., -938.4431, -914.3484,\n",
            "        -916.0031])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -874.0949,  -964.0419,  -729.4959,  ...,  -620.7536,  -893.7832,\n",
            "        -1031.4363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -875.2104, -1040.0911,  -695.8333,  ...,  -989.4908, -1135.7094,\n",
            "        -1023.6686])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -892.1144, -1085.3507,  -815.8732,  ...,  -889.1908, -1137.5087,\n",
            "         -971.9207])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -891.8963,  -842.0750, -1249.4828,  ..., -1017.6755,  -797.2234,\n",
            "         -661.3777])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1041.2863,  -960.3007,  -804.3613,  ...,  -831.6380, -1213.9797,\n",
            "        -1072.5786])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -846.0565, -1032.7936, -1100.8053,  ..., -1125.4098, -1072.3259,\n",
            "         -866.1111])\n",
            "actor loss: 29732.01854087448, critic loss: 31370167.6484375, entropy: 108613.66259765625, KL divergence: 0.0014357897511414995\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.463559161511598], 離散行動：[0, 1], 連続行動：-1.3718271255493164\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "120エピソード目の累積報酬：-100000.00000171614, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1034.9301,  -953.9247, -1287.5798,  ...,  -851.9155,  -511.8238,\n",
            "        -1094.8792])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -564.6236,  -943.7256, -1144.7990,  ..., -1248.8936,  -613.1219,\n",
            "        -1167.8333])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1269.4696,  -789.5925, -1037.7687,  ..., -1256.1857,  -935.8654,\n",
            "         -912.8235])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1287.9449,  -920.3583, -1030.4053,  ...,  -727.4659,  -703.5535,\n",
            "         -929.0684])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1093.6793, -1069.8989,  -952.0609,  ..., -1022.4913,  -773.2841,\n",
            "         -923.4233])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1054.9698,  -665.0761,  -687.3634,  ...,  -806.4771,  -851.5086,\n",
            "         -953.3247])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -807.1197,  -950.8640, -1273.7994,  ...,  -896.4597,  -966.6311,\n",
            "        -1127.4406])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -919.3896, -1027.7240,  -914.6110,  ...,  -766.1782,  -775.9645,\n",
            "        -1139.0338])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -941.1868,  -674.6969, -1238.5547,  ..., -1301.5958,  -800.2421,\n",
            "         -917.6201])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1070.0388,  -821.1244,  -700.9941,  ...,  -850.3995, -1063.7397,\n",
            "         -625.0773])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1201.6183,  -693.7065, -1141.8073,  ..., -1126.2438,  -846.3781,\n",
            "        -1281.9015])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1166.3569,  -887.0042,  -886.2529,  ...,  -875.2745,  -998.6526,\n",
            "        -1286.7551])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1165.4752, -1075.7063,  -996.7554,  ..., -1157.3546,  -660.5538,\n",
            "        -1164.8317])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -809.1797,  -790.8569,  -800.8072,  ...,  -962.7847, -1177.9004,\n",
            "         -841.0178])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -986.2389,  -958.1513, -1069.0750,  ...,  -855.8739, -1069.6904,\n",
            "         -924.2786])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1338.4391,  -967.0184,  -953.9272,  ...,  -990.3595, -1014.1293,\n",
            "         -991.7926])\n",
            "actor loss: 29692.84777898035, critic loss: 31366071.46875, entropy: 110447.2900390625, KL divergence: 0.015192815903864964\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.1777029252832267], 離散行動：[0, 1], 連続行動：-5.102232933044434\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "121エピソード目の累積報酬：-100001.13536166537, 一つ保全の回数：4, 二つ保全の回数：8188, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -686.6547,  -755.6214,  -872.1243,  ...,  -803.4266, -1099.9067,\n",
            "         -959.3730])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -668.1445,  -913.9583,  -857.5867,  ...,  -912.1636, -1052.5516,\n",
            "        -1089.6158])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1068.0751, -1033.2064,  -711.0964,  ...,  -952.9169,  -955.3443,\n",
            "         -773.4452])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1037.3772,  -596.0959,  -988.8137,  ..., -1008.5428,  -767.7478,\n",
            "         -931.4245])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -929.8777, -1189.5879,  -663.4008,  ...,  -789.9106,  -773.3284,\n",
            "         -707.8243])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -959.4202,  -768.0797, -1215.6010,  ..., -1262.0728,  -909.6713,\n",
            "         -856.3553])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -554.6722,  -944.4086,  -901.3070,  ...,  -871.0563, -1371.3055,\n",
            "         -825.1602])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1265.1147,  -763.4283,  -954.4522,  ...,  -694.9429, -1069.6945,\n",
            "        -1121.8345])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -620.6467, -1341.0864,  -816.1606,  ...,  -876.3893,  -972.7632,\n",
            "         -891.9575])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -550.0428,  -827.4785, -1079.0729,  ...,  -876.9763, -1130.9735,\n",
            "        -1143.2510])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1130.2472, -1309.8375, -1013.3096,  ...,  -885.5035, -1024.9598,\n",
            "        -1080.3206])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -620.7651,  -890.6204, -1331.9562,  ...,  -914.1915, -1266.1593,\n",
            "         -660.0386])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1005.0587,  -813.1755, -1089.4181,  ...,  -890.6692,  -951.1503,\n",
            "         -971.9808])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1234.5624, -1092.0477, -1170.7596,  ..., -1000.4443,  -960.4536,\n",
            "         -754.5648])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1323.1305,  -900.9008, -1020.8709,  ...,  -627.1020, -1013.0008,\n",
            "         -876.9363])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -994.1780,  -951.4297,  -717.6926,  ..., -1088.0743,  -784.0379,\n",
            "         -848.8241])\n",
            "actor loss: 29699.041058256742, critic loss: 31877869.53125, entropy: 112137.66796875, KL divergence: 0.0001850982916148172\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.255157806130355], 離散行動：[0, 1], 連続行動：7.664968490600586\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "122エピソード目の累積報酬：-100000.50332547941, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -839.3265,  -719.1536,  -990.9709,  ...,  -607.3762, -1006.4114,\n",
            "         -858.9521])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1058.6837,  -746.2308,  -886.5629,  ..., -1026.1562,  -738.9651,\n",
            "         -778.4034])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -906.5781, -1058.5267,  -670.3883,  ...,  -757.9857,  -746.5817,\n",
            "        -1349.0792])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1066.9983, -1007.5506,  -911.9305,  ..., -1146.2288,  -898.4975,\n",
            "         -697.4980])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-865.4297, -708.7448, -947.4968,  ..., -815.5458, -967.6965,\n",
            "        -942.2070])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1060.1040,  -911.3536,  -692.9929,  ...,  -708.0929,  -868.5896,\n",
            "         -845.7665])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -899.8983,  -967.9833,  -728.2407,  ...,  -763.0793,  -952.8180,\n",
            "        -1023.0682])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1132.7098,  -650.2324,  -908.4925,  ..., -1078.5476,  -825.6425,\n",
            "         -606.9853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -630.6010,  -517.6059,  -822.5935,  ..., -1258.8710,  -721.6760,\n",
            "         -890.6406])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-791.7982, -742.9208, -935.8715,  ..., -492.9712, -800.3653,\n",
            "        -987.5986])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -531.7886,  -666.6650, -1125.8896,  ..., -1089.4154,  -816.7875,\n",
            "        -1039.7621])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1023.3188,  -621.6739,  -824.7838,  ..., -1024.5691,  -844.5331,\n",
            "         -881.0576])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1180.0575,  -986.8434,  -955.1186,  ..., -1058.2726,  -759.6440,\n",
            "         -711.3434])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -722.6720,  -554.0658, -1139.2307,  ...,  -600.1280,  -935.6763,\n",
            "         -715.3849])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -968.1576,  -869.3339,  -956.7473,  ...,  -506.9697, -1264.8411,\n",
            "         -926.6511])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -888.6888, -1233.4703, -1156.5782,  ..., -1005.7981, -1312.2889,\n",
            "         -753.6690])\n",
            "actor loss: 29688.672535149344, critic loss: 31902838.80859375, entropy: 112137.33740234375, KL divergence: 3.670368883927456e-05\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.9677786252127016], 離散行動：[0, 1], 連続行動：1.234071969985962\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "123エピソード目の累積報酬：-99771.76953986508, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1008.2556,  -554.0057,  -747.2897,  ..., -1158.8746,  -867.6952,\n",
            "         -882.0010])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1350.9712, -1298.2957, -1017.9791,  ...,  -934.7856,  -698.1707,\n",
            "         -931.8553])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -908.2974,  -744.0955, -1288.8352,  ...,  -922.2031,  -900.6727,\n",
            "        -1195.3267])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1315.9229,  -871.7198, -1068.0955,  ...,  -974.9581,  -805.5074,\n",
            "         -918.1021])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1102.9521,  -874.1313,  -631.6879,  ...,  -651.2607,  -806.5366,\n",
            "         -624.9156])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -868.4093,  -611.4791, -1026.2660,  ...,  -868.4723, -1097.4393,\n",
            "         -840.7744])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -821.9547,  -685.7882,  -541.2002,  ...,  -974.5219, -1170.8956,\n",
            "        -1081.6891])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1084.8683,  -936.7330,  -763.7926,  ..., -1133.5900,  -950.9059,\n",
            "         -878.2081])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1169.0038, -1029.5482,  -821.1396,  ..., -1240.8695, -1015.5989,\n",
            "         -685.3809])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1205.1183,  -843.3670,  -982.3272,  ...,  -853.9631,  -970.3089,\n",
            "         -913.2819])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -869.5350, -1273.1077, -1036.7773,  ..., -1042.3071,  -685.4155,\n",
            "         -479.7045])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -915.6867, -1017.1085, -1027.0139,  ..., -1265.6107,  -881.7397,\n",
            "        -1081.6230])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-837.5496, -988.3115, -739.2580,  ..., -792.2273, -949.0439,\n",
            "        -935.5792])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1098.6466,  -655.2915, -1119.0145,  ...,  -845.5668,  -985.6090,\n",
            "         -814.0059])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1272.7778,  -868.3609,  -992.2128,  ...,  -897.1760,  -851.9427,\n",
            "        -1027.9751])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -954.3747,  -622.6903,  -846.8218,  ...,  -981.8044, -1070.5559,\n",
            "        -1145.4319])\n",
            "actor loss: 29690.53472570001, critic loss: 32005998.1328125, entropy: 112135.60498046875, KL divergence: 3.444760655721178e-05\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.7371739931940575], 離散行動：[0, 1], 連続行動：-1.329595923423767\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "124エピソード目の累積報酬：-100006.39737671896, 一つ保全の回数：4, 二つ保全の回数：8188, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -850.5715,  -734.0938, -1207.9968,  ...,  -961.4076,  -832.0046,\n",
            "         -702.6447])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -662.4738, -1154.9830,  -843.7614,  ..., -1322.7521, -1235.3997,\n",
            "        -1053.9458])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -785.3882,  -800.7806,  -922.4037,  ...,  -950.9174, -1067.6335,\n",
            "        -1152.0189])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -747.2547, -1065.1289, -1034.8583,  ..., -1176.8319,  -885.8910,\n",
            "         -986.1801])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -735.4362, -1090.3668,  -638.7624,  ..., -1085.3173, -1045.9358,\n",
            "        -1036.4487])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -776.5383,  -856.5414,  -948.6715,  ...,  -856.6966,  -830.4350,\n",
            "        -1257.6879])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1211.9927, -1112.6886,  -650.4241,  ...,  -723.7638, -1005.3694,\n",
            "         -854.4695])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -981.0104,  -704.1705,  -839.6795,  ...,  -791.7340, -1249.7029,\n",
            "        -1235.2374])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -925.8489, -1031.1873,  -709.9108,  ...,  -924.6599,  -935.7731,\n",
            "        -1308.9933])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -608.0239, -1214.3330,  -594.7866,  ...,  -731.0482, -1106.8462,\n",
            "        -1048.6743])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1058.1367, -1064.9294,  -894.3812,  ..., -1278.3179,  -927.6995,\n",
            "        -1233.5853])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1000.9357, -1016.4281, -1027.3329,  ..., -1234.9049,  -664.9308,\n",
            "         -870.8276])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -737.2006,  -982.4085,  -858.9858,  ...,  -933.2744, -1053.1163,\n",
            "         -636.0125])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -608.5369,  -861.9225, -1197.6606,  ..., -1108.0857, -1083.4556,\n",
            "        -1205.4152])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -878.4835,  -726.2848,  -781.8178,  ...,  -826.1978, -1030.7068,\n",
            "        -1319.5950])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -617.3868, -1224.7990, -1257.3127,  ..., -1241.6360, -1108.6877,\n",
            "        -1078.4648])\n",
            "actor loss: 29661.997042188974, critic loss: 32502303.265625, entropy: 112131.6943359375, KL divergence: 4.61320247968951e-05\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.1156157078630304], 離散行動：[0, 1], 連続行動：-1.9033727645874023\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "125エピソード目の累積報酬：-100000.00000000063, 一つ保全の回数：3, 二つ保全の回数：8189, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -774.7055,  -848.7926, -1074.6719,  ..., -1085.2531, -1003.8159,\n",
            "        -1110.3302])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -843.5916,  -839.5085, -1175.5454,  ..., -1029.0844,  -785.7009,\n",
            "        -1127.7964])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -970.9081,  -953.5734,  -777.3463,  ..., -1101.9893,  -613.1727,\n",
            "        -1099.5153])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -975.4644,  -694.3488,  -914.2275,  ..., -1115.9678,  -837.1870,\n",
            "         -779.0635])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1057.7183,  -823.9683, -1263.8407,  ...,  -927.2195,  -818.8062,\n",
            "         -856.0464])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1032.1553, -1341.1666,  -865.9689,  ...,  -909.5507,  -766.9011,\n",
            "        -1246.7834])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -923.3239, -1097.7881,  -878.5614,  ...,  -733.9998,  -977.7131,\n",
            "         -666.4697])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -807.5369,  -988.1562,  -997.5407,  ...,  -763.3080, -1161.5591,\n",
            "        -1024.2441])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1009.4583,  -670.5356, -1225.2039,  ...,  -764.1005, -1027.1852,\n",
            "         -651.5162])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -955.5685,  -595.2637, -1179.3915,  ...,  -879.2769,  -696.8839,\n",
            "        -1019.4718])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1107.2451, -1006.6761, -1263.6655,  ...,  -919.7824,  -743.5259,\n",
            "         -813.5704])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1342.3324,  -799.7856,  -974.1661,  ...,  -930.9677,  -732.5190,\n",
            "         -988.7836])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -858.3259,  -655.9264,  -832.2222,  ...,  -822.7820, -1059.7194,\n",
            "         -779.8761])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -843.6630,  -936.3937,  -868.3921,  ..., -1263.6655,  -650.8918,\n",
            "         -965.8743])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1095.5503,  -953.6317,  -718.4190,  ...,  -870.4469,  -970.2847,\n",
            "         -962.8344])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -597.0609, -1022.6639, -1340.0038,  ...,  -904.5477,  -901.4322,\n",
            "        -1072.5192])\n",
            "actor loss: 29636.975666596736, critic loss: 32713118.78515625, entropy: 112129.25244140625, KL divergence: 6.337255146106536e-05\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 2.202378893172934], 離散行動：[0, 1], 連続行動：-1.0535681247711182\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "126エピソード目の累積報酬：-100000.00000000063, 一つ保全の回数：1, 二つ保全の回数：8191, 三つ保全の回数：0, 違反回数：0\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -725.7845, -1138.5135,  -962.8184,  ..., -1268.2899, -1303.0918,\n",
            "         -944.6306])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -854.8311, -1154.2311,  -996.3585,  ...,  -918.1967,  -961.3546,\n",
            "        -1027.8840])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -967.5338, -1125.7432,  -747.7718,  ..., -1259.2734,  -958.7125,\n",
            "        -1080.6721])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -803.0430,  -879.9587,  -959.0453,  ..., -1249.4299,  -946.2555,\n",
            "        -1155.5917])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -778.3924,  -786.5807,  -881.5745,  ...,  -979.5762,  -962.1366,\n",
            "        -1037.4026])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -824.1592,  -751.1074, -1004.1090,  ...,  -700.3036, -1200.8541,\n",
            "        -1043.3207])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -797.4165,  -831.4300,  -855.2040,  ...,  -949.4401, -1100.1158,\n",
            "         -918.6423])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -790.2031,  -978.5925, -1015.9616,  ..., -1177.4657,  -944.4108,\n",
            "        -1076.2938])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -871.0519,  -630.0411,  -751.4093,  ...,  -775.3254, -1219.1052,\n",
            "        -1048.6973])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1015.4732,  -870.4896, -1024.7639,  ..., -1121.4901, -1137.3156,\n",
            "        -1226.3599])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1127.4163,  -823.8530, -1028.7958,  ...,  -931.7269,  -837.3118,\n",
            "         -948.5347])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1187.6503,  -981.6389, -1096.5325,  ...,  -843.5452,  -540.9236,\n",
            "        -1240.2019])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -904.0656,  -983.8303,  -756.0823,  ...,  -933.8979,  -813.6666,\n",
            "        -1012.2301])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([ -542.9305,  -764.7311, -1083.2788,  ...,  -783.7264,  -934.7038,\n",
            "        -1041.9644])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-1104.1636,  -992.0869, -1038.7325,  ...,  -910.7211, -1154.0720,\n",
            "         -642.4572])\n",
            "FLAG\n",
            "advantage[batch].size(),advantage[batch]: torch.Size([2048]) tensor([-747.7718, -742.1445, -895.1055,  ..., -844.6668, -961.6195,\n",
            "        -879.1181])\n",
            "actor loss: 29624.110103856572, critic loss: 32845368.0390625, entropy: 112130.005859375, KL divergence: 7.662894081819937e-05\n",
            "... loading models ...\n",
            "... saving models ...\n",
            "状態[0, 0, 1.6553107235511217], 離散行動：[0, 1], 連続行動：-2.488241195678711\n",
            "[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [0, 0, 0, 0]\n",
            "127エピソード目の累積報酬：-100000.00000000063, 一つ保全の回数：1, 二つ保全の回数：8191, 三つ保全の回数：0, 違反回数：0\n"
          ]
        }
      ],
      "source": [
        "num_episode =128#8*16*4で90分\n",
        "threshold = 0 #cnt学習を開始するエピソード\n",
        "best_reward = -np.inf\n",
        "episode_reward_history = []\n",
        "avg_cost = 0\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    episode_reward = 0\n",
        "    operation_time = 0\n",
        "    one_action = 0\n",
        "    two_action = 0\n",
        "    three_action = 0\n",
        "    penalty_action = 0\n",
        "    #level_ohe, load_total = env.init_random()\n",
        "    levels, load_total = env.init_random()\n",
        "    if episode % 100 == 0:\n",
        "        interval_time_episode = time.time()\n",
        "    interval_time_episode = time.time()\n",
        "    for _ in range(1024*8):#1024*8\n",
        "        #state = level_ohe + list([inventory,demand])\n",
        "        state = levels + list([load_total])\n",
        "        #print(state)\n",
        "        act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action(state)\n",
        "        act_dsc_list = [int(bit) for bit in format(act_dsc.item(), f'0{env.n_units}b')]\n",
        "        if sum(act_dsc_list) == 2:\n",
        "            one_action += 1\n",
        "        elif sum(act_dsc_list) == 1:\n",
        "            two_action += 1\n",
        "        elif sum(act_dsc_list) == 0:\n",
        "            three_action += 1\n",
        "        act_cnt_np = act_cnt.squeeze().cpu().numpy().copy()\n",
        "        act_cnt_np = act_cnt_np * 0.5 + 0.5 #補正\n",
        "\n",
        "        if act_cnt_np<0.5:\n",
        "          env.cntCount[0]+=1\n",
        "        else:\n",
        "          env.cntCount[1]+=1\n",
        "        #print(act_dsc_list,act_cnt_np)\n",
        "        #reward, level_ohe_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "        reward, levels_next, load_total_next= env.operation(act_dsc_list,act_cnt_np)\n",
        "\n",
        "        episode_reward = episode_reward *0.99 + reward\n",
        "        #penalty_action += flag\n",
        "        #if remain_interval > remain_interval_next:\n",
        "            #operation_time += (remain_interval+1)/2*interval - (remain_interval_next+1)/2*interval\n",
        "        #else:\n",
        "            #operation_time += (remain_interval_next + 1) / 2 * interval\n",
        "        agent.remember(state, act_dsc.item(), act_cnt.squeeze().cpu().numpy().copy(), log_prob_dsc, log_prob_cnt, val, reward, operation_time)\n",
        "        levels = levels_next\n",
        "        #mstatus_ohe = mstatus_ohe_next\n",
        "        #inventory = inventory_next\n",
        "        #demand = demand_next\n",
        "        #remain_interval = remain_interval_next\n",
        "        load_total = load_total_next\n",
        "    #print(f'{episode}エピソード目の時間：{time.time()-interval_time_episode}')\n",
        "    interval_time_episode = time.time()\n",
        "    agent.learn(episode, threshold)\n",
        "\n",
        "    old_agent = Agent(n_units=n_units,\n",
        "                        input_dims=input_size,\n",
        "                        n_states=n_states,\n",
        "                        MAX_maintenance_time=MAX_maintenance_time,\n",
        "                        beta=beta,\n",
        "                        interval=interval,\n",
        "                        alpha_actor=alpha_actor,\n",
        "                        alpha_critic=alpha_critic,\n",
        "                        policy_clip=policy_clip,\n",
        "                        batch_size=batch_size,\n",
        "                        n_epochs=n_epochs)\n",
        "    if episode != 0:\n",
        "        old_agent.load_models()\n",
        "\n",
        "    #if Check_convergence(agent, old_agent, n_units, n_states, MAX_maintenance_time):\n",
        "    #    break\n",
        "\n",
        "\n",
        "\n",
        "    agent.save_models()\n",
        "    print(f'状態{state}, 離散行動：{act_dsc_list}, 連続行動：{act_cnt_np}')\n",
        "    print(f'[保全を選択できた回数,1個故障で保全を選ばない回数, 2個故障で保全を選ばない回数, 3個故障で保全を選ばない回数] = [{env.replace_chance}, {env.failure_keep1}, {env.failure_keep2}, {env.failure_keep3}]')\n",
        "    env.replace_chance = 0\n",
        "    env.failure_keep1 = 0\n",
        "    env.failure_keep2 = 0\n",
        "    env.failure_keep3 = 0\n",
        "    #print(f'{episode}エピソード目の学習時間：{time.time()-interval_time_episode}')\n",
        "    print(f'{episode}エピソード目の累積報酬：{episode_reward}, 一つ保全の回数：{one_action}, 二つ保全の回数：{two_action}, 三つ保全の回数：{three_action}, 違反回数：{penalty_action}')\n",
        "    episode_reward_history.append(episode_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncPGOL29TYPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fcb8be-23e2-4b76-8ae1-974f27e8faf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHLLAogeTYPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f32c885-e780-4aba-d98b-0300fb25a023"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_learning_curve(rewards, title=\"Learning Curve\", label=\"Total reward\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rewards, label='Episode Reward')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# エージェントの学習\n",
        "# (agent.train()の呼び出しなど)\n",
        "\n",
        "# 学習後のエージェントの評価\n",
        "#evaluate_agent(agent, env, num_episodes=10)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plot_learning_curve(episode_reward_history, title=\"PPO Learning Curve\", label=\"Total reward\")\n",
        "plot_learning_curve(agent.loss_history_detail, title=\"loss curve\", label=\"Total loss (actor loss +  critic loss) \")\n",
        "plot_learning_curve(agent.actor_loss_history, title=\"actor loss curve\", label=\"actor loss\")\n",
        "plot_learning_curve(agent.critic_loss_history, title=\"critic loss curve\", label=\"critic loss\")\n",
        "plot_learning_curve(agent.entropy_history, title=\"entropy curve\", label=\"entropy\")\n",
        "plot_learning_curve(agent.kl_divergence_history, title=\"kl divergence curve\", label=\"kl divergence\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3bLWs2RTYPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72dc9716-500e-4838-b4fa-b463b2236c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[3.9158e-04, 9.5505e-01, 1.1952e-03, 4.3363e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2888]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7386]], grad_fn=<ExpBackward0>)\n",
            "1 -0.28876916\n",
            "state: tensor([[0.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[5.9864e-04, 9.2874e-01, 1.6969e-03, 6.8963e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2732]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0298]], grad_fn=<ExpBackward0>)\n",
            "1 -0.27322748\n",
            "state: tensor([[0.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[6.7775e-04, 9.1853e-01, 1.8842e-03, 7.8910e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2806]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3733]], grad_fn=<ExpBackward0>)\n",
            "1 -0.2806462\n",
            "state: tensor([[0.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[5.5650e-04, 9.3441e-01, 1.6119e-03, 6.3418e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2938]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.8941]], grad_fn=<ExpBackward0>)\n",
            "1 -0.2938029\n",
            "state: tensor([[0.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[3.3170e-04, 9.6270e-01, 1.0591e-03, 3.5905e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3082]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.6647]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3081965\n",
            "state: tensor([[0.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6510e-04, 9.8253e-01, 5.9821e-04, 1.6704e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3207]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.6376]], grad_fn=<ExpBackward0>)\n",
            "1 -0.32067502\n",
            "state: tensor([[0.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[7.2191e-05, 9.9288e-01, 3.0314e-04, 6.7428e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3382]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.7235]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33824807\n",
            "state: tensor([[0.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[2.7847e-05, 9.9746e-01, 1.3836e-04, 2.3715e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3541]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.9651]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35412616\n",
            "state: tensor([[0.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[9.8533e-06, 9.9917e-01, 5.8804e-05, 7.5800e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3756]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3203]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37563398\n",
            "state: tensor([[0.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4824e-06, 9.9973e-01, 2.4962e-05, 2.4200e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3958]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7536]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39584246\n",
            "state: tensor([[0.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2303e-06, 9.9991e-01, 1.0593e-05, 7.7233e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4154]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2296]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4154287\n",
            "state: tensor([[0.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[4.3462e-07, 9.9997e-01, 4.4943e-06, 2.4645e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4345]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7479]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43449578\n",
            "state: tensor([[0.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5353e-07, 9.9999e-01, 1.9068e-06, 7.8641e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4526]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3151]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45262778\n",
            "state: tensor([[0.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[5.3835e-08, 1.0000e+00, 8.0415e-07, 2.4889e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4682]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9380]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4682022\n",
            "state: tensor([[0.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.8654e-08, 1.0000e+00, 3.3583e-07, 7.7727e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4832]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48315385\n",
            "state: tensor([[0.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[6.4638e-09, 1.0000e+00, 1.4025e-07, 2.4274e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4984]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49835333\n",
            "state: tensor([[0.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[2.2397e-09, 1.0000e+00, 5.8572e-08, 7.5807e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5131]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5130777\n",
            "state: tensor([[0.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[7.7608e-10, 1.0000e+00, 2.4461e-08, 2.3674e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5277]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5277356\n",
            "state: tensor([[0.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[2.6892e-10, 1.0000e+00, 1.0215e-08, 7.3934e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5429]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5428621\n",
            "state: tensor([[0.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[9.3180e-11, 1.0000e+00, 4.2662e-09, 2.3089e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5576]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5576443\n",
            "state: tensor([[ 0.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[3.2287e-11, 1.0000e+00, 1.7817e-09, 7.2107e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5721]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5720812\n",
            "state: tensor([[ 0.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.1118e-11, 1.0000e+00, 7.4022e-10, 2.2366e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5866]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5865718\n",
            "state: tensor([[0.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[2.2818e-04, 9.7505e-01, 7.6748e-04, 2.3951e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2942]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4017]], grad_fn=<ExpBackward0>)\n",
            "1 -0.29417765\n",
            "state: tensor([[0.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3386e-04, 9.6227e-01, 1.0525e-03, 3.6348e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2991]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5223]], grad_fn=<ExpBackward0>)\n",
            "1 -0.29908457\n",
            "state: tensor([[0.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[4.2580e-04, 9.5101e-01, 1.2916e-03, 4.7274e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3107]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7016]], grad_fn=<ExpBackward0>)\n",
            "1 -0.31071323\n",
            "state: tensor([[0.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3281e-04, 9.6263e-01, 1.0596e-03, 3.5977e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3400]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.2570]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3400465\n",
            "state: tensor([[0.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[2.0342e-04, 9.7813e-01, 7.0908e-04, 2.0959e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3582]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.9375]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35821\n",
            "state: tensor([[0.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1107e-04, 9.8866e-01, 4.3190e-04, 1.0798e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3688]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.7449]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36878896\n",
            "state: tensor([[0.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[4.7246e-05, 9.9551e-01, 2.1385e-04, 4.2284e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3832]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.8632]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38318512\n",
            "state: tensor([[0.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.8995e-05, 9.9832e-01, 1.0102e-04, 1.5560e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3973]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.0578]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39734298\n",
            "state: tensor([[0.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[7.5034e-06, 9.9938e-01, 4.7021e-05, 5.6130e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4112]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.2688]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41116622\n",
            "state: tensor([[0.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[2.8338e-06, 9.9978e-01, 2.1087e-05, 1.9275e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4271]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.5976]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42707464\n",
            "state: tensor([[0.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[9.8580e-07, 9.9993e-01, 8.8350e-06, 6.0458e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4455]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0694]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44547787\n",
            "state: tensor([[0.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4160e-07, 9.9998e-01, 3.6899e-06, 1.8882e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5942]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46339506\n",
            "state: tensor([[0.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1837e-07, 9.9999e-01, 1.5410e-06, 5.8968e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4792]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1637]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47916213\n",
            "state: tensor([[0.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[4.1016e-08, 1.0000e+00, 6.4356e-07, 1.8416e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4943]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7817]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49426606\n",
            "state: tensor([[0.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4212e-08, 1.0000e+00, 2.6876e-07, 5.7511e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5091]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5090773\n",
            "state: tensor([[0.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[4.9246e-09, 1.0000e+00, 1.1224e-07, 1.7961e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5236]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5235931\n",
            "state: tensor([[0.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.7064e-09, 1.0000e+00, 4.6875e-08, 5.6090e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5377]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53766656\n",
            "state: tensor([[0.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[5.9127e-10, 1.0000e+00, 1.9576e-08, 1.7517e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5522]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55224764\n",
            "state: tensor([[0.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[2.0488e-10, 1.0000e+00, 8.1754e-09, 5.4705e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5667]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56665623\n",
            "state: tensor([[0.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[7.0992e-11, 1.0000e+00, 3.4142e-09, 1.7084e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5807]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5807264\n",
            "state: tensor([[ 0.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[2.4599e-11, 1.0000e+00, 1.4258e-09, 5.3353e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5945]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.594458\n",
            "state: tensor([[ 0.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[8.5237e-12, 1.0000e+00, 5.9547e-10, 1.6662e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6079]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6078513\n",
            "state: tensor([[1.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0042e-04, 9.8975e-01, 3.9032e-04, 9.7600e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3050]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.30496737\n",
            "state: tensor([[1.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6330e-04, 9.8262e-01, 5.8452e-04, 1.6636e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3103]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0052]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3103086\n",
            "state: tensor([[1.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.7647e-04, 9.8114e-01, 6.2570e-04, 1.8059e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3322]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3255]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33223343\n",
            "state: tensor([[1.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5466e-04, 9.8370e-01, 5.6341e-04, 1.5585e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3643]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7619]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36430877\n",
            "state: tensor([[1.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1478e-04, 9.8823e-01, 4.4224e-04, 1.1213e-02]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3910]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.2397]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3909646\n",
            "state: tensor([[1.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[6.4851e-05, 9.9367e-01, 2.7706e-04, 5.9867e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4085]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[3.9670]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40845716\n",
            "state: tensor([[1.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[2.8614e-05, 9.9739e-01, 1.4150e-04, 2.4359e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4206]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.0535]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4205728\n",
            "state: tensor([[1.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1431e-05, 9.9903e-01, 6.6494e-05, 8.8968e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4365]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.2684]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43653885\n",
            "state: tensor([[1.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[4.5173e-06, 9.9964e-01, 3.0964e-05, 3.2094e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4513]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.4920]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45127764\n",
            "state: tensor([[1.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7538e-06, 9.9987e-01, 1.4209e-05, 1.1354e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4651]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7590]], grad_fn=<ExpBackward0>)\n",
            "1 -0.465055\n",
            "state: tensor([[1.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[6.8080e-07, 9.9995e-01, 6.5190e-06, 4.0162e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4794]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0750]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47937512\n",
            "state: tensor([[1.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[2.5880e-07, 9.9998e-01, 2.9395e-06, 1.3884e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4932]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4432]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49323705\n",
            "state: tensor([[1.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[9.0087e-08, 9.9999e-01, 1.2322e-06, 4.3581e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5082]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9934]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50816226\n",
            "state: tensor([[1.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[3.1249e-08, 1.0000e+00, 5.1504e-07, 1.3626e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5226]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5961]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5226435\n",
            "state: tensor([[1.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0828e-08, 1.0000e+00, 2.1509e-07, 4.2554e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5368]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2584]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53678656\n",
            "state: tensor([[1.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[3.7519e-09, 1.0000e+00, 8.9827e-08, 1.3289e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5506]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55063426\n",
            "state: tensor([[1.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3001e-09, 1.0000e+00, 3.7514e-08, 4.1502e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5645]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5645027\n",
            "state: tensor([[1.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[4.5048e-10, 1.0000e+00, 1.5667e-08, 1.2961e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5785]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57853544\n",
            "state: tensor([[1.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5609e-10, 1.0000e+00, 6.5427e-09, 4.0477e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5922]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59223384\n",
            "state: tensor([[1.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[5.4087e-11, 1.0000e+00, 2.7324e-09, 1.2641e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6056]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60559845\n",
            "state: tensor([[ 1.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8741e-11, 1.0000e+00, 1.1411e-09, 3.9476e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6186]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61862975\n",
            "state: tensor([[ 1.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[6.4940e-12, 1.0000e+00, 4.7655e-10, 1.2328e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6313]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6313294\n",
            "state: tensor([[1.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[3.3715e-05, 9.9684e-01, 1.5847e-04, 2.9669e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3081]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3080919\n",
            "state: tensor([[1.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[5.7414e-05, 9.9439e-01, 2.4679e-04, 5.3024e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3139]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9456]], grad_fn=<ExpBackward0>)\n",
            "1 -0.31393716\n",
            "state: tensor([[1.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[6.5078e-05, 9.9358e-01, 2.7429e-04, 6.0856e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3425]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9456]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34246755\n",
            "state: tensor([[1.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[5.8107e-05, 9.9433e-01, 2.5077e-04, 5.3600e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3788]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3853]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37882668\n",
            "state: tensor([[1.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[4.6500e-05, 9.9557e-01, 2.0961e-04, 4.1752e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4134]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8389]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41339272\n",
            "state: tensor([[1.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[3.1834e-05, 9.9707e-01, 1.5394e-04, 2.7460e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4351]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3799]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43510264\n",
            "state: tensor([[1.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5094e-05, 9.9869e-01, 8.3422e-05, 1.2083e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4514]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3529]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45141307\n",
            "state: tensor([[1.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[6.3732e-06, 9.9948e-01, 4.1063e-05, 4.6849e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4653]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.5050]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46531093\n",
            "state: tensor([[1.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[2.5373e-06, 9.9981e-01, 1.9238e-05, 1.7041e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4823]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7481]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48227766\n",
            "state: tensor([[1.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0057e-06, 9.9993e-01, 8.9806e-06, 6.1669e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4975]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.9906]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4975294\n",
            "state: tensor([[1.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[3.9119e-07, 9.9997e-01, 4.1279e-06, 2.1854e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5117]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2833]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5117128\n",
            "state: tensor([[1.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5216e-07, 9.9999e-01, 1.8974e-06, 7.7446e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5252]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6285]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52516204\n",
            "state: tensor([[1.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[5.9185e-08, 1.0000e+00, 8.7209e-07, 2.7445e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5384]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9962]], grad_fn=<ExpBackward0>)\n",
            "1 -0.538354\n",
            "state: tensor([[1.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[2.3005e-08, 1.0000e+00, 4.0058e-07, 9.7192e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5508]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3874]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5507552\n",
            "state: tensor([[1.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[8.0406e-09, 1.0000e+00, 1.6849e-07, 3.0639e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5651]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0257]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56512105\n",
            "state: tensor([[1.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[2.8065e-09, 1.0000e+00, 7.0793e-08, 9.6442e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5800]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5799631\n",
            "state: tensor([[1.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[9.7960e-10, 1.0000e+00, 2.9743e-08, 3.0357e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5944]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5943957\n",
            "state: tensor([[1.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4058e-10, 1.0000e+00, 1.2457e-08, 9.5140e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6081]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6081433\n",
            "state: tensor([[1.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1834e-10, 1.0000e+00, 5.2143e-09, 2.9796e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6215]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62150663\n",
            "state: tensor([[1.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[4.1117e-11, 1.0000e+00, 2.1826e-09, 9.3317e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6345]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6345179\n",
            "state: tensor([[ 1.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4279e-11, 1.0000e+00, 9.1322e-10, 2.9209e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6471]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6471049\n",
            "state: tensor([[ 1.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[4.9476e-12, 1.0000e+00, 3.8138e-10, 9.1219e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6591]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6590561\n",
            "state: tensor([[2.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0385e-05, 9.9911e-01, 5.9891e-05, 8.2009e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2993]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.29930595\n",
            "state: tensor([[2.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5608e-05, 9.9862e-01, 8.4065e-05, 1.2792e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3275]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.32749543\n",
            "state: tensor([[2.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.7479e-05, 9.9844e-01, 9.2568e-05, 1.4458e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3591]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1311]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35912254\n",
            "state: tensor([[2.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7672e-05, 9.9843e-01, 9.3734e-05, 1.4591e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3906]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1869]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39064285\n",
            "state: tensor([[2.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5602e-05, 9.9863e-01, 8.4913e-05, 1.2674e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4251]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5800]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42509916\n",
            "state: tensor([[2.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1964e-05, 9.9898e-01, 6.8534e-05, 9.4114e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4630]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0595]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46302775\n",
            "state: tensor([[2.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[7.3315e-06, 9.9940e-01, 4.5927e-05, 5.4791e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4833]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7579]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48326078\n",
            "state: tensor([[2.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3854e-06, 9.9974e-01, 2.4345e-05, 2.3417e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4942]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8233]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49424165\n",
            "state: tensor([[2.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4187e-06, 9.9990e-01, 1.1908e-05, 9.0043e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5042]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.9823]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5041765\n",
            "state: tensor([[2.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[5.6275e-07, 9.9996e-01, 5.5620e-06, 3.2615e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5186]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2300]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51860803\n",
            "state: tensor([[2.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[2.2362e-07, 9.9999e-01, 2.6018e-06, 1.1836e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5336]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4937]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53364754\n",
            "state: tensor([[2.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[8.6982e-08, 9.9999e-01, 1.1959e-06, 4.1945e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5491]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8367]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5490624\n",
            "state: tensor([[2.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[3.3833e-08, 1.0000e+00, 5.4966e-07, 1.4864e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5642]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.2270]], grad_fn=<ExpBackward0>)\n",
            "1 -0.564232\n",
            "state: tensor([[2.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.3160e-08, 1.0000e+00, 2.5264e-07, 5.2673e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5776]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6325]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57763803\n",
            "state: tensor([[2.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[5.1186e-09, 1.0000e+00, 1.1612e-07, 1.8666e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5904]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0600]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5904258\n",
            "state: tensor([[2.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.9910e-09, 1.0000e+00, 5.3373e-08, 6.6146e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6029]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6028923\n",
            "state: tensor([[2.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[7.1232e-10, 1.0000e+00, 2.2891e-08, 2.1385e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6174]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61744374\n",
            "state: tensor([[2.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[2.4915e-10, 1.0000e+00, 9.6358e-09, 6.7441e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6321]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6320766\n",
            "state: tensor([[2.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[8.7108e-11, 1.0000e+00, 4.0544e-09, 2.1260e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6460]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.645981\n",
            "state: tensor([[2.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0405e-11, 1.0000e+00, 1.7035e-09, 6.6921e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6585]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6585216\n",
            "state: tensor([[ 2.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0613e-11, 1.0000e+00, 7.1571e-10, 2.1065e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6707]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67070675\n",
            "state: tensor([[ 2.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[3.6882e-12, 1.0000e+00, 2.9964e-10, 6.5987e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68236613\n",
            "state: tensor([[2.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[2.6645e-06, 9.9979e-01, 1.9454e-05, 1.8546e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2972]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.29716843\n",
            "state: tensor([[2.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[3.6267e-06, 9.9971e-01, 2.5157e-05, 2.5948e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3339]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3339374\n",
            "state: tensor([[2.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[4.1342e-06, 9.9967e-01, 2.8107e-05, 2.9908e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3709]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37088227\n",
            "state: tensor([[2.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[4.5985e-06, 9.9963e-01, 3.0783e-05, 3.3546e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4059]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3818]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40587354\n",
            "state: tensor([[2.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[4.5062e-06, 9.9964e-01, 3.0404e-05, 3.2636e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4411]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4540]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44114724\n",
            "state: tensor([[2.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[3.8333e-06, 9.9970e-01, 2.6712e-05, 2.7188e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4769]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8360]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4769135\n",
            "state: tensor([[2.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[2.8645e-06, 9.9978e-01, 2.1105e-05, 1.9609e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5104]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3390]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51037264\n",
            "state: tensor([[2.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5888e-06, 9.9988e-01, 1.3026e-05, 1.0225e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5253]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2559]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52527654\n",
            "state: tensor([[2.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[7.5868e-07, 9.9995e-01, 7.0988e-06, 4.5342e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5359]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3096]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53593755\n",
            "state: tensor([[2.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[3.1622e-07, 9.9998e-01, 3.4568e-06, 1.7325e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5433]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4921]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5433103\n",
            "state: tensor([[2.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2520e-07, 9.9999e-01, 1.6122e-06, 6.2633e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5567]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7552]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55667204\n",
            "state: tensor([[2.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[4.9606e-08, 1.0000e+00, 7.5230e-07, 2.2658e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5707]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0285]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5707218\n",
            "state: tensor([[2.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.9340e-08, 1.0000e+00, 3.4644e-07, 8.0504e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5854]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4085]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58540094\n",
            "state: tensor([[2.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[7.5226e-09, 1.0000e+00, 1.5924e-07, 2.8528e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5995]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.8337]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5995286\n",
            "state: tensor([[2.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[2.9260e-09, 1.0000e+00, 7.3190e-08, 1.0109e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6133]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2872]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61329234\n",
            "state: tensor([[2.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1381e-09, 1.0000e+00, 3.3640e-08, 3.5824e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6267]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6266934\n",
            "state: tensor([[2.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[4.4268e-10, 1.0000e+00, 1.5462e-08, 1.2695e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6389]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6389159\n",
            "state: tensor([[2.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7219e-10, 1.0000e+00, 7.1068e-09, 4.4987e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6502]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.650231\n",
            "state: tensor([[2.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[6.2929e-11, 1.0000e+00, 3.1022e-09, 1.4888e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6629]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6628668\n",
            "state: tensor([[2.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[2.2011e-11, 1.0000e+00, 1.3058e-09, 4.6952e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6762]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6762276\n",
            "state: tensor([[ 2.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[7.6989e-12, 1.0000e+00, 5.4967e-10, 1.4807e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6888]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6887703\n",
            "state: tensor([[ 2.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[2.6929e-12, 1.0000e+00, 2.3138e-10, 4.6698e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7008]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7008474\n",
            "state: tensor([[3.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[6.3568e-07, 9.9995e-01, 5.9537e-06, 3.8689e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.2983]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.29832655\n",
            "state: tensor([[3.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[8.2613e-07, 9.9994e-01, 7.4070e-06, 5.1499e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3354]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33537194\n",
            "state: tensor([[3.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[9.7693e-07, 9.9993e-01, 8.5264e-06, 6.1808e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3760]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37603173\n",
            "state: tensor([[3.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0883e-06, 9.9992e-01, 9.3493e-06, 6.9449e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4119]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41192403\n",
            "state: tensor([[3.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1691e-06, 9.9991e-01, 9.9577e-06, 7.4813e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4486]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44856668\n",
            "state: tensor([[3.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0663e-06, 9.9992e-01, 9.2717e-06, 6.7220e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4854]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7840]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48539278\n",
            "state: tensor([[3.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[8.8503e-07, 9.9994e-01, 7.9825e-06, 5.4477e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5230]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1554]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52297086\n",
            "state: tensor([[3.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[6.1961e-07, 9.9996e-01, 5.9761e-06, 3.6553e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5509]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8880]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55086595\n",
            "state: tensor([[3.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[3.4432e-07, 9.9998e-01, 3.6944e-06, 1.9086e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5651]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7886]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5651057\n",
            "state: tensor([[3.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6859e-07, 9.9999e-01, 2.0557e-06, 8.6950e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5745]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8080]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5744731\n",
            "state: tensor([[3.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[7.0477e-08, 1.0000e+00, 1.0034e-06, 3.3334e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5834]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0265]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5834264\n",
            "state: tensor([[3.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[2.7905e-08, 1.0000e+00, 4.6798e-07, 1.2050e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5936]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3233]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5935713\n",
            "state: tensor([[3.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1049e-08, 1.0000e+00, 2.1825e-07, 4.3563e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6065]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6209]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60646677\n",
            "state: tensor([[3.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[4.3021e-09, 1.0000e+00, 1.0040e-07, 1.5458e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6193]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0311]], grad_fn=<ExpBackward0>)\n",
            "1 -0.619326\n",
            "state: tensor([[3.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6726e-09, 1.0000e+00, 4.6130e-08, 5.4753e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6325]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6324812\n",
            "state: tensor([[3.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[6.5059e-10, 1.0000e+00, 2.1203e-08, 1.9402e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6454]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6453635\n",
            "state: tensor([[3.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[2.5305e-10, 1.0000e+00, 9.7455e-09, 6.8756e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6579]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6578887\n",
            "state: tensor([[3.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[9.8428e-11, 1.0000e+00, 4.4793e-09, 2.4365e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6701]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6700605\n",
            "state: tensor([[3.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[3.8285e-11, 1.0000e+00, 2.0588e-09, 8.6342e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6819]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6818822\n",
            "state: tensor([[3.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[1.4891e-11, 1.0000e+00, 9.4629e-10, 3.0597e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6928]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69279015\n",
            "state: tensor([[ 3.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[5.5593e-12, 1.0000e+00, 4.2041e-10, 1.0365e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7031]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7030812\n",
            "state: tensor([[ 3.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.9445e-12, 1.0000e+00, 1.7697e-10, 3.2688e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7148]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71482414\n",
            "state: tensor([[3.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4915e-07, 9.9999e-01, 1.7968e-06, 7.9275e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3006]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.30058214\n",
            "state: tensor([[3.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.8815e-07, 9.9999e-01, 2.1805e-06, 1.0219e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3378]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33780822\n",
            "state: tensor([[3.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[2.3128e-07, 9.9998e-01, 2.5907e-06, 1.2801e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3771]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3771062\n",
            "state: tensor([[3.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[2.5727e-07, 9.9998e-01, 2.8369e-06, 1.4359e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41435498\n",
            "state: tensor([[3.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[2.7307e-07, 9.9998e-01, 2.9907e-06, 1.5282e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4545]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45454973\n",
            "state: tensor([[3.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[2.7768e-07, 9.9998e-01, 3.0452e-06, 1.5480e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4877]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48773405\n",
            "state: tensor([[3.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[2.4629e-07, 9.9998e-01, 2.7710e-06, 1.3481e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5270]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1359]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52697223\n",
            "state: tensor([[3.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.8754e-07, 9.9999e-01, 2.2225e-06, 9.9310e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5648]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7228]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5648473\n",
            "state: tensor([[3.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3124e-07, 9.9999e-01, 1.6631e-06, 6.6587e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5882]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4950]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5881798\n",
            "state: tensor([[3.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[7.4675e-08, 1.0000e+00, 1.0484e-06, 3.5670e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6015]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3771]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60153794\n",
            "state: tensor([[3.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[3.6605e-08, 1.0000e+00, 5.8397e-07, 1.6259e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6118]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3973]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61182326\n",
            "state: tensor([[3.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5707e-08, 1.0000e+00, 2.9127e-07, 6.4133e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6198]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5954]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61975837\n",
            "state: tensor([[3.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[6.2192e-09, 1.0000e+00, 1.3584e-07, 2.3184e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6298]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9297]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6297985\n",
            "state: tensor([[3.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[2.4624e-09, 1.0000e+00, 6.3353e-08, 8.3813e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6399]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2915]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6399032\n",
            "state: tensor([[3.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[9.6137e-10, 1.0000e+00, 2.9207e-08, 2.9829e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6519]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65190977\n",
            "state: tensor([[3.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[3.7248e-10, 1.0000e+00, 1.3381e-08, 1.0526e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6636]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66356534\n",
            "state: tensor([[3.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4466e-10, 1.0000e+00, 6.1424e-09, 3.7238e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6753]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67531013\n",
            "state: tensor([[3.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[5.6265e-11, 1.0000e+00, 2.8232e-09, 1.3196e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6870]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.686979\n",
            "state: tensor([[3.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[2.1885e-11, 1.0000e+00, 1.2976e-09, 4.6763e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6983]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69830406\n",
            "state: tensor([[3.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[8.5125e-12, 1.0000e+00, 5.9643e-10, 1.6571e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7093]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70929\n",
            "state: tensor([[ 3.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3110e-12, 1.0000e+00, 2.7414e-10, 5.8723e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7194]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7194446\n",
            "state: tensor([[ 3.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.2879e-12, 1.0000e+00, 1.2600e-10, 2.0810e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7291]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72906923\n",
            "state: tensor([[4.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[3.3969e-08, 1.0000e+00, 5.2894e-07, 1.5730e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3048]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.30481213\n",
            "state: tensor([[4.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[4.2850e-08, 1.0000e+00, 6.4189e-07, 2.0276e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3415]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34147558\n",
            "state: tensor([[4.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[5.4052e-08, 1.0000e+00, 7.7895e-07, 2.6137e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3780]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.377958\n",
            "state: tensor([[4.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[6.0890e-08, 1.0000e+00, 8.6176e-07, 2.9730e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4143]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41429174\n",
            "state: tensor([[4.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[6.4022e-08, 1.0000e+00, 9.0133e-07, 3.1310e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4564]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4564116\n",
            "state: tensor([[4.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[6.4729e-08, 1.0000e+00, 9.1289e-07, 3.1568e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4937]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4937213\n",
            "state: tensor([[4.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[6.4224e-08, 1.0000e+00, 9.1098e-07, 3.1104e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5259]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5259089\n",
            "state: tensor([[4.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[5.4903e-08, 1.0000e+00, 8.0429e-07, 2.5999e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5657]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5656679\n",
            "state: tensor([[4.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[3.8537e-08, 1.0000e+00, 6.0331e-07, 1.7499e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6023]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6023307\n",
            "state: tensor([[4.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[2.6967e-08, 1.0000e+00, 4.5146e-07, 1.1733e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6226]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1847]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6226373\n",
            "state: tensor([[4.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6176e-08, 1.0000e+00, 2.9725e-07, 6.6558e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6353]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0243]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63529104\n",
            "state: tensor([[4.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[7.9577e-09, 1.0000e+00, 1.6606e-07, 3.0455e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6451]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0415]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6450535\n",
            "state: tensor([[4.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[3.5070e-09, 1.0000e+00, 8.4666e-08, 1.2369e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6547]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2411]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65475\n",
            "state: tensor([[4.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.3862e-09, 1.0000e+00, 3.9434e-08, 4.4612e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6630]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66304547\n",
            "state: tensor([[4.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[5.4880e-10, 1.0000e+00, 1.8390e-08, 1.6125e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6722]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67224973\n",
            "state: tensor([[4.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[2.1483e-10, 1.0000e+00, 8.4966e-09, 5.7560e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6818]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68179923\n",
            "state: tensor([[4.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[8.3235e-11, 1.0000e+00, 3.8926e-09, 2.0311e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6931]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69308007\n",
            "state: tensor([[4.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[3.2249e-11, 1.0000e+00, 1.7834e-09, 7.1675e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7036]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70360446\n",
            "state: tensor([[4.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2510e-11, 1.0000e+00, 8.1788e-10, 2.5327e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7140]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7140223\n",
            "state: tensor([[4.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[4.8661e-12, 1.0000e+00, 3.7592e-10, 8.9749e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7245]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7245287\n",
            "state: tensor([[ 4.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.8927e-12, 1.0000e+00, 1.7279e-10, 3.1804e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7347]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7346537\n",
            "state: tensor([[ 4.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[7.3620e-13, 1.0000e+00, 7.9417e-11, 1.1270e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7438]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74383444\n",
            "state: tensor([[4.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[7.7181e-09, 1.0000e+00, 1.5540e-07, 3.1134e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3090]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.30899844\n",
            "state: tensor([[4.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[9.7585e-09, 1.0000e+00, 1.8895e-07, 4.0233e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3451]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34513247\n",
            "state: tensor([[4.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2310e-08, 1.0000e+00, 2.2930e-07, 5.1861e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3799]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3798953\n",
            "state: tensor([[4.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.4410e-08, 1.0000e+00, 2.6177e-07, 6.1551e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4136]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41356945\n",
            "state: tensor([[4.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4990e-08, 1.0000e+00, 2.7136e-07, 6.4069e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4542]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45423642\n",
            "state: tensor([[4.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5146e-08, 1.0000e+00, 2.7462e-07, 6.4581e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4958]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49576458\n",
            "state: tensor([[4.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4908e-08, 1.0000e+00, 2.7213e-07, 6.3148e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5309]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5309278\n",
            "state: tensor([[4.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.4752e-08, 1.0000e+00, 2.7102e-07, 6.2004e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5648]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5647735\n",
            "state: tensor([[4.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1322e-08, 1.0000e+00, 2.1894e-07, 4.6013e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6017]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60168713\n",
            "state: tensor([[4.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[7.9187e-09, 1.0000e+00, 1.6377e-07, 3.0833e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6336]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63356316\n",
            "state: tensor([[4.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[5.5285e-09, 1.0000e+00, 1.2233e-07, 2.0613e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6539]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65392625\n",
            "state: tensor([[4.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4328e-09, 1.0000e+00, 8.2863e-08, 1.2142e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6666]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6665678\n",
            "state: tensor([[4.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.7299e-09, 1.0000e+00, 4.7219e-08, 5.7048e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6759]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67589813\n",
            "state: tensor([[4.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[7.7419e-10, 1.0000e+00, 2.4382e-08, 2.3566e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6861]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68612593\n",
            "state: tensor([[4.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[3.0968e-10, 1.0000e+00, 1.1468e-08, 8.6113e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6959]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69589543\n",
            "state: tensor([[4.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.2241e-10, 1.0000e+00, 5.3413e-09, 3.1058e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7030]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70301735\n",
            "state: tensor([[4.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[4.8007e-11, 1.0000e+00, 2.4717e-09, 1.1107e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7109]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7109431\n",
            "state: tensor([[4.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[1.8600e-11, 1.0000e+00, 1.1324e-09, 3.9194e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7196]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7195893\n",
            "state: tensor([[4.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[7.2065e-12, 1.0000e+00, 5.1879e-10, 1.3831e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7300]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7299797\n",
            "state: tensor([[4.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[2.7921e-12, 1.0000e+00, 2.3768e-10, 4.8806e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7396]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7396243\n",
            "state: tensor([[ 4.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0820e-12, 1.0000e+00, 1.0890e-10, 1.7225e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7488]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7488109\n",
            "state: tensor([[ 4.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[4.2084e-13, 1.0000e+00, 5.0055e-11, 6.1041e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7579]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75790757\n",
            "state: tensor([[5.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.7467e-09, 1.0000e+00, 4.5507e-08, 6.1361e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3129]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.31287214\n",
            "state: tensor([[5.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[2.2224e-09, 1.0000e+00, 5.5622e-08, 7.9829e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3488]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.348779\n",
            "state: tensor([[5.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[2.8034e-09, 1.0000e+00, 6.7499e-08, 1.0290e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3814]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38140833\n",
            "state: tensor([[5.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3663e-09, 1.0000e+00, 7.8671e-08, 1.2561e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4127]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4126766\n",
            "state: tensor([[5.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[3.5119e-09, 1.0000e+00, 8.1737e-08, 1.3119e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4509]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45089883\n",
            "state: tensor([[5.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[3.5510e-09, 1.0000e+00, 8.2775e-08, 1.3232e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4951]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49514738\n",
            "state: tensor([[5.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[3.4644e-09, 1.0000e+00, 8.1371e-08, 1.2834e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5343]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53426266\n",
            "state: tensor([[5.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4250e-09, 1.0000e+00, 8.0959e-08, 1.2595e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5693]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56929463\n",
            "state: tensor([[5.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[3.1219e-09, 1.0000e+00, 7.5364e-08, 1.1290e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6009]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60090405\n",
            "state: tensor([[5.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[2.3347e-09, 1.0000e+00, 5.9592e-08, 8.1423e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6355]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6354984\n",
            "state: tensor([[5.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6252e-09, 1.0000e+00, 4.4414e-08, 5.4248e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6622]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6622215\n",
            "state: tensor([[5.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1334e-09, 1.0000e+00, 3.3147e-08, 3.6213e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6831]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6831242\n",
            "state: tensor([[5.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[7.0632e-10, 1.0000e+00, 2.2520e-08, 2.1412e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6954]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6953652\n",
            "state: tensor([[5.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[3.7608e-10, 1.0000e+00, 1.3427e-08, 1.0686e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7045]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70454377\n",
            "state: tensor([[5.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6780e-10, 1.0000e+00, 6.9157e-09, 4.4001e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7142]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.714209\n",
            "state: tensor([[5.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[6.9181e-11, 1.0000e+00, 3.3351e-09, 1.6622e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7260]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7259609\n",
            "state: tensor([[5.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[2.7345e-11, 1.0000e+00, 1.5533e-09, 5.9951e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7329]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73289037\n",
            "state: tensor([[5.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0744e-11, 1.0000e+00, 7.1987e-10, 2.1476e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7393]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73928505\n",
            "state: tensor([[5.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[4.1564e-12, 1.0000e+00, 3.2942e-10, 7.5631e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7458]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7457848\n",
            "state: tensor([[5.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6104e-12, 1.0000e+00, 1.5092e-10, 2.6688e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7535]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.753549\n",
            "state: tensor([[ 5.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[6.2394e-13, 1.0000e+00, 6.9142e-11, 9.4178e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7629]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7628546\n",
            "state: tensor([[ 5.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[2.4174e-13, 1.0000e+00, 3.1677e-11, 3.3233e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7719]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.771853\n",
            "state: tensor([[5.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[3.9532e-10, 1.0000e+00, 1.3326e-08, 1.2094e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3172]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.31716415\n",
            "state: tensor([[5.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[5.0613e-10, 1.0000e+00, 1.6373e-08, 1.5840e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3522]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35215926\n",
            "state: tensor([[5.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[6.3845e-10, 1.0000e+00, 1.9870e-08, 2.0418e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3825]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3824769\n",
            "state: tensor([[5.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[7.8371e-10, 1.0000e+00, 2.3578e-08, 2.5538e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4133]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4132663\n",
            "state: tensor([[5.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[8.2224e-10, 1.0000e+00, 2.4608e-08, 2.6843e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4483]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4483333\n",
            "state: tensor([[5.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[8.3274e-10, 1.0000e+00, 2.4956e-08, 2.7117e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4933]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49329162\n",
            "state: tensor([[5.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[8.1058e-10, 1.0000e+00, 2.4483e-08, 2.6243e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5342]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5342354\n",
            "state: tensor([[5.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[7.9498e-10, 1.0000e+00, 2.4183e-08, 2.5570e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5727]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5726523\n",
            "state: tensor([[5.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[7.6019e-10, 1.0000e+00, 2.3411e-08, 2.4182e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6051]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60513324\n",
            "state: tensor([[5.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[6.4658e-10, 1.0000e+00, 2.0589e-08, 2.0074e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6346]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6346336\n",
            "state: tensor([[5.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[4.8108e-10, 1.0000e+00, 1.6211e-08, 1.4396e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6654]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6654368\n",
            "state: tensor([[5.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3319e-10, 1.0000e+00, 1.2035e-08, 9.5303e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6890]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68895346\n",
            "state: tensor([[5.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[2.3236e-10, 1.0000e+00, 8.9816e-09, 6.3619e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7103]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71029013\n",
            "state: tensor([[5.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.4466e-10, 1.0000e+00, 6.0971e-09, 3.7575e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7220]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7220331\n",
            "state: tensor([[5.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[8.0144e-11, 1.0000e+00, 3.7560e-09, 1.9583e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7309]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7308914\n",
            "state: tensor([[5.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[3.6369e-11, 1.0000e+00, 1.9616e-09, 8.2155e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7402]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7401666\n",
            "state: tensor([[5.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5455e-11, 1.0000e+00, 9.6990e-10, 3.2086e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7513]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7513118\n",
            "state: tensor([[5.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[6.1089e-12, 1.0000e+00, 4.5173e-10, 1.1572e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7602]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7601816\n",
            "state: tensor([[5.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[2.4064e-12, 1.0000e+00, 2.0981e-10, 4.1578e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7660]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7660059\n",
            "state: tensor([[5.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[9.3081e-13, 1.0000e+00, 9.5994e-11, 1.4638e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7717]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77174264\n",
            "state: tensor([[ 5.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6003e-13, 1.0000e+00, 4.3920e-11, 5.1533e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7774]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77735645\n",
            "state: tensor([[ 5.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.3943e-13, 1.0000e+00, 2.0114e-11, 1.8173e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7839]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7839127\n",
            "state: tensor([[6.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[8.9467e-11, 1.0000e+00, 3.9022e-09, 2.3835e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3212]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.32121712\n",
            "state: tensor([[6.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1470e-10, 1.0000e+00, 4.7999e-09, 3.1262e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3532]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35322657\n",
            "state: tensor([[6.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4540e-10, 1.0000e+00, 5.8490e-09, 4.0514e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3835]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38354445\n",
            "state: tensor([[6.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7610e-10, 1.0000e+00, 6.8645e-09, 4.9925e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4149]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.414929\n",
            "state: tensor([[6.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.9234e-10, 1.0000e+00, 7.4036e-09, 5.4870e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4469]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44690657\n",
            "state: tensor([[6.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.9528e-10, 1.0000e+00, 7.5243e-09, 5.5576e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4913]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49132162\n",
            "state: tensor([[6.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[1.9009e-10, 1.0000e+00, 7.3814e-09, 5.3784e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5325]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5324785\n",
            "state: tensor([[6.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.8503e-10, 1.0000e+00, 7.2412e-09, 5.2049e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5730]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57296646\n",
            "state: tensor([[6.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[1.8265e-10, 1.0000e+00, 7.1948e-09, 5.1003e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6091]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6090835\n",
            "state: tensor([[6.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5753e-10, 1.0000e+00, 6.3973e-09, 4.3038e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6384]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63835496\n",
            "state: tensor([[6.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3391e-10, 1.0000e+00, 5.6246e-09, 3.5692e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6661]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66610706\n",
            "state: tensor([[6.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[9.8743e-11, 1.0000e+00, 4.3964e-09, 2.5327e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6919]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6919022\n",
            "state: tensor([[6.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[6.8308e-11, 1.0000e+00, 3.2609e-09, 1.6743e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7138]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71375453\n",
            "state: tensor([[6.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[4.7636e-11, 1.0000e+00, 2.4337e-09, 1.1177e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7338]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7338136\n",
            "state: tensor([[6.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[2.9412e-11, 1.0000e+00, 1.6407e-09, 6.5424e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7462]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7462253\n",
            "state: tensor([[6.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6549e-11, 1.0000e+00, 1.0238e-09, 3.4668e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7547]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75472647\n",
            "state: tensor([[6.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[7.8828e-12, 1.0000e+00, 5.5638e-10, 1.5339e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7640]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76395214\n",
            "state: tensor([[6.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[3.3919e-12, 1.0000e+00, 2.7796e-10, 6.0738e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7744]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7744376\n",
            "state: tensor([[6.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3647e-12, 1.0000e+00, 1.3137e-10, 2.2338e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7836]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7836154\n",
            "state: tensor([[6.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[5.3902e-13, 1.0000e+00, 6.1149e-11, 8.0496e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7901]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7901181\n",
            "state: tensor([[ 6.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0849e-13, 1.0000e+00, 2.7977e-11, 2.8339e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7953]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79532105\n",
            "state: tensor([[ 6.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[8.0644e-14, 1.0000e+00, 1.2800e-11, 9.9768e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8004]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80040944\n",
            "state: tensor([[6.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[2.0248e-11, 1.0000e+00, 1.1427e-09, 4.6976e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3249]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3248644\n",
            "state: tensor([[6.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[2.5957e-11, 1.0000e+00, 1.4056e-09, 6.1614e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3543]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35428563\n",
            "state: tensor([[6.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[3.3113e-11, 1.0000e+00, 1.7218e-09, 8.0387e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3846]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38461098\n",
            "state: tensor([[6.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[3.9570e-11, 1.0000e+00, 1.9985e-09, 9.7602e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4166]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.416589\n",
            "state: tensor([[6.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[4.4994e-11, 1.0000e+00, 2.2274e-09, 1.1216e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4479]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4479215\n",
            "state: tensor([[6.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[4.5795e-11, 1.0000e+00, 2.2685e-09, 1.1390e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4889]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4888511\n",
            "state: tensor([[6.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[4.4577e-11, 1.0000e+00, 2.2255e-09, 1.1023e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5307]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5307169\n",
            "state: tensor([[6.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[4.3391e-11, 1.0000e+00, 2.1832e-09, 1.0667e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5708]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57075876\n",
            "state: tensor([[6.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[4.2426e-11, 1.0000e+00, 2.1505e-09, 1.0363e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6100]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6099549\n",
            "state: tensor([[6.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[3.8320e-11, 1.0000e+00, 1.9861e-09, 9.2016e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6423]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6422616\n",
            "state: tensor([[6.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[3.2646e-11, 1.0000e+00, 1.7482e-09, 7.6599e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6688]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66877085\n",
            "state: tensor([[6.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[2.7701e-11, 1.0000e+00, 1.5351e-09, 6.3361e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6947]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6946513\n",
            "state: tensor([[6.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[2.0267e-11, 1.0000e+00, 1.1923e-09, 4.4558e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7158]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71578836\n",
            "state: tensor([[6.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.4009e-11, 1.0000e+00, 8.8382e-10, 2.9427e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7361]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73614746\n",
            "state: tensor([[6.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[9.7660e-12, 1.0000e+00, 6.5943e-10, 1.9635e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7554]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75536376\n",
            "state: tensor([[6.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[5.9801e-12, 1.0000e+00, 4.4150e-10, 1.1391e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7686]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7686409\n",
            "state: tensor([[6.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[3.4173e-12, 1.0000e+00, 2.7908e-10, 6.1373e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7764]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.776384\n",
            "state: tensor([[6.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6759e-12, 1.0000e+00, 1.5533e-10, 2.8040e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7855]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78546786\n",
            "state: tensor([[6.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[7.3516e-13, 1.0000e+00, 7.8841e-11, 1.1341e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7952]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7951889\n",
            "state: tensor([[6.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0487e-13, 1.0000e+00, 3.8205e-11, 4.3118e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8036]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8035822\n",
            "state: tensor([[ 6.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2051e-13, 1.0000e+00, 1.7794e-11, 1.5551e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8115]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81151545\n",
            "state: tensor([[ 6.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[4.6700e-14, 1.0000e+00, 8.1541e-12, 5.4864e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8163]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8163131\n",
            "state: tensor([[7.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[4.5825e-12, 1.0000e+00, 3.3461e-10, 9.2584e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3270]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3269504\n",
            "state: tensor([[7.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[5.8746e-12, 1.0000e+00, 4.1159e-10, 1.2143e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3553]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3553439\n",
            "state: tensor([[7.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[7.4539e-12, 1.0000e+00, 5.0200e-10, 1.5747e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3862]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38616312\n",
            "state: tensor([[7.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[8.8915e-12, 1.0000e+00, 5.8184e-10, 1.9081e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4182]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4182463\n",
            "state: tensor([[7.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0525e-11, 1.0000e+00, 6.7015e-10, 2.2926e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4493]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44930604\n",
            "state: tensor([[7.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0698e-11, 1.0000e+00, 6.8170e-10, 2.3249e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4869]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4869052\n",
            "state: tensor([[7.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0454e-11, 1.0000e+00, 6.7097e-10, 2.2591e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5288]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5288183\n",
            "state: tensor([[7.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0175e-11, 1.0000e+00, 6.5823e-10, 2.1862e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5685]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56854284\n",
            "state: tensor([[7.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[9.9047e-12, 1.0000e+00, 6.4573e-10, 2.1157e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6080]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6079934\n",
            "state: tensor([[7.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[9.3261e-12, 1.0000e+00, 6.1692e-10, 1.9685e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6433]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6432949\n",
            "state: tensor([[7.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[7.9410e-12, 1.0000e+00, 5.4275e-10, 1.6377e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6722]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67220914\n",
            "state: tensor([[7.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[6.7644e-12, 1.0000e+00, 4.7768e-10, 1.3631e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6972]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69718176\n",
            "state: tensor([[7.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[5.7242e-12, 1.0000e+00, 4.1861e-10, 1.1232e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7191]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7190915\n",
            "state: tensor([[7.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[4.1599e-12, 1.0000e+00, 3.2337e-10, 7.8391e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7387]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7386875\n",
            "state: tensor([[7.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[2.8753e-12, 1.0000e+00, 2.3970e-10, 5.1771e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7576]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7576379\n",
            "state: tensor([[7.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.9906e-12, 1.0000e+00, 1.7782e-10, 3.4282e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7754]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77536577\n",
            "state: tensor([[7.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2159e-12, 1.0000e+00, 1.1880e-10, 1.9834e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7883]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7883113\n",
            "state: tensor([[7.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[7.0566e-13, 1.0000e+00, 7.6072e-11, 1.0865e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7967]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7966761\n",
            "state: tensor([[7.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[3.4502e-13, 1.0000e+00, 4.2233e-11, 4.9478e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8052]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8051559\n",
            "state: tensor([[7.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5934e-13, 1.0000e+00, 2.2362e-11, 2.1174e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8142]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81423104\n",
            "state: tensor([[ 7.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[6.8107e-14, 1.0000e+00, 1.1111e-11, 8.3231e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8219]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8218904\n",
            "state: tensor([[ 7.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[2.6921e-14, 1.0000e+00, 5.1748e-12, 3.0018e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8292]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8291543\n",
            "state: tensor([[7.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0371e-12, 1.0000e+00, 9.7985e-11, 1.8247e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3287]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.32867965\n",
            "state: tensor([[7.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.3295e-12, 1.0000e+00, 1.2053e-10, 2.3933e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3564]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35640115\n",
            "state: tensor([[7.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6644e-12, 1.0000e+00, 1.4539e-10, 3.0578e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3878]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38783503\n",
            "state: tensor([[7.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.9979e-12, 1.0000e+00, 1.6940e-10, 3.7302e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4199]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4199006\n",
            "state: tensor([[7.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[2.3801e-12, 1.0000e+00, 1.9612e-10, 4.5135e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4509]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4509119\n",
            "state: tensor([[7.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[2.4977e-12, 1.0000e+00, 2.0476e-10, 4.7424e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4871]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48712838\n",
            "state: tensor([[7.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[2.4514e-12, 1.0000e+00, 2.0230e-10, 4.6298e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5273]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52731067\n",
            "state: tensor([[7.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[2.3862e-12, 1.0000e+00, 1.9845e-10, 4.4805e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5660]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5659587\n",
            "state: tensor([[7.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[2.3227e-12, 1.0000e+00, 1.9469e-10, 4.3360e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6054]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60541934\n",
            "state: tensor([[7.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[2.2669e-12, 1.0000e+00, 1.9145e-10, 4.2057e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6418]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.64177626\n",
            "state: tensor([[7.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.9322e-12, 1.0000e+00, 1.6855e-10, 3.5027e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6747]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6747066\n",
            "state: tensor([[7.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[1.6456e-12, 1.0000e+00, 1.4832e-10, 2.9147e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7007]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7006515\n",
            "state: tensor([[7.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3979e-12, 1.0000e+00, 1.3027e-10, 2.4168e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7232]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72319955\n",
            "state: tensor([[7.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.1828e-12, 1.0000e+00, 1.1415e-10, 1.9913e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7409]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7408775\n",
            "state: tensor([[7.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[8.5382e-13, 1.0000e+00, 8.7701e-11, 1.3791e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7595]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7595293\n",
            "state: tensor([[7.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[5.9017e-13, 1.0000e+00, 6.5008e-11, 9.1081e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7776]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77760226\n",
            "state: tensor([[7.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[4.0472e-13, 1.0000e+00, 4.7850e-11, 5.9690e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7939]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7938794\n",
            "state: tensor([[7.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[2.4721e-13, 1.0000e+00, 3.1969e-11, 3.4535e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8061]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80611765\n",
            "state: tensor([[7.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4571e-13, 1.0000e+00, 2.0736e-11, 1.9234e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8152]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81522405\n",
            "state: tensor([[7.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[7.1032e-14, 1.0000e+00, 1.1483e-11, 8.7309e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8232]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8232159\n",
            "state: tensor([[ 7.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[3.3897e-14, 1.0000e+00, 6.2462e-12, 3.8732e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8316]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8316178\n",
            "state: tensor([[ 7.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.4860e-14, 1.0000e+00, 3.1688e-12, 1.5654e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8389]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83889776\n",
            "state: tensor([[8.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[2.3471e-13, 1.0000e+00, 2.8693e-11, 3.5963e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3304]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33040667\n",
            "state: tensor([[8.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0089e-13, 1.0000e+00, 3.5294e-11, 4.7169e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3579]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.357875\n",
            "state: tensor([[8.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[3.7167e-13, 1.0000e+00, 4.2107e-11, 5.9377e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3895]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38950428\n",
            "state: tensor([[8.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[4.4894e-13, 1.0000e+00, 4.9318e-11, 7.2924e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4216]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4215523\n",
            "state: tensor([[8.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[5.3480e-13, 1.0000e+00, 5.7098e-11, 8.8236e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4525]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4525092\n",
            "state: tensor([[8.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[5.8247e-13, 1.0000e+00, 6.1446e-11, 9.6612e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4886]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48857716\n",
            "state: tensor([[8.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[5.7488e-13, 1.0000e+00, 6.0991e-11, 9.4886e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5277]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5276541\n",
            "state: tensor([[8.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[5.5958e-13, 1.0000e+00, 5.9833e-11, 9.1826e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5635]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.563518\n",
            "state: tensor([[8.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[5.4469e-13, 1.0000e+00, 5.8697e-11, 8.8864e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6028]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6028325\n",
            "state: tensor([[8.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[5.3020e-13, 1.0000e+00, 5.7582e-11, 8.5998e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63970834\n",
            "state: tensor([[8.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[4.7104e-13, 1.0000e+00, 5.2432e-11, 7.5077e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6735]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67347974\n",
            "state: tensor([[8.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[4.0033e-13, 1.0000e+00, 4.6051e-11, 6.2325e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7040]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70397377\n",
            "state: tensor([[8.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[3.4058e-13, 1.0000e+00, 4.0489e-11, 5.1789e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7263]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7262869\n",
            "state: tensor([[8.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[2.8886e-13, 1.0000e+00, 3.5523e-11, 4.2844e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7473]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74725056\n",
            "state: tensor([[8.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[2.4440e-13, 1.0000e+00, 3.1127e-11, 3.5297e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7613]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7613398\n",
            "state: tensor([[8.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7525e-13, 1.0000e+00, 2.3785e-11, 2.4263e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7789]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7788886\n",
            "state: tensor([[8.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2113e-13, 1.0000e+00, 1.7631e-11, 1.6024e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7961]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7960894\n",
            "state: tensor([[8.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[8.2288e-14, 1.0000e+00, 1.2876e-11, 1.0393e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8110]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8110295\n",
            "state: tensor([[8.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[5.0262e-14, 1.0000e+00, 8.6027e-12, 6.0131e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8226]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8225749\n",
            "state: tensor([[8.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0089e-14, 1.0000e+00, 5.6523e-12, 3.4050e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8312]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8312278\n",
            "state: tensor([[ 8.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[1.4624e-14, 1.0000e+00, 3.1222e-12, 1.5406e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8396656\n",
            "state: tensor([[ 8.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[6.9785e-15, 1.0000e+00, 1.6983e-12, 6.8346e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8474]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84743243\n",
            "state: tensor([[8.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[5.3119e-14, 1.0000e+00, 8.4022e-12, 7.0879e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3321]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33213142\n",
            "state: tensor([[8.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[6.8098e-14, 1.0000e+00, 1.0335e-11, 9.2966e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3596]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3595631\n",
            "state: tensor([[8.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[8.2993e-14, 1.0000e+00, 1.2195e-11, 1.1530e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3912]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3911711\n",
            "state: tensor([[8.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0048e-13, 1.0000e+00, 1.4311e-11, 1.4195e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4232]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42317992\n",
            "state: tensor([[8.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2017e-13, 1.0000e+00, 1.6623e-11, 1.7250e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4541]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45410383\n",
            "state: tensor([[8.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[1.3584e-13, 1.0000e+00, 1.8439e-11, 1.9681e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4900]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49002337\n",
            "state: tensor([[8.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[1.3481e-13, 1.0000e+00, 1.8389e-11, 1.9446e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5272]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5271555\n",
            "state: tensor([[8.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[1.3123e-13, 1.0000e+00, 1.8040e-11, 1.8819e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5635]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5634694\n",
            "state: tensor([[8.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2773e-13, 1.0000e+00, 1.7697e-11, 1.8212e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6002]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60020405\n",
            "state: tensor([[8.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[1.2434e-13, 1.0000e+00, 1.7361e-11, 1.7625e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6373]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6372956\n",
            "state: tensor([[8.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[1.1485e-13, 1.0000e+00, 1.6312e-11, 1.6094e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6723]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67231673\n",
            "state: tensor([[8.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[9.7592e-14, 1.0000e+00, 1.4325e-11, 1.3359e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7036]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7036291\n",
            "state: tensor([[8.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[8.2937e-14, 1.0000e+00, 1.2581e-11, 1.1089e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7295]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7294766\n",
            "state: tensor([[8.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[7.0417e-14, 1.0000e+00, 1.1044e-11, 9.1901e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7500]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74996704\n",
            "state: tensor([[8.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[5.9689e-14, 1.0000e+00, 9.6872e-12, 7.5953e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7681]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7681251\n",
            "state: tensor([[8.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[5.0464e-14, 1.0000e+00, 8.4829e-12, 6.2518e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7807]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7807158\n",
            "state: tensor([[8.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[3.5970e-14, 1.0000e+00, 6.4506e-12, 4.2687e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7969]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7968692\n",
            "state: tensor([[8.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[2.4863e-14, 1.0000e+00, 4.7815e-12, 2.8191e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8128]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81281847\n",
            "state: tensor([[8.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.6731e-14, 1.0000e+00, 3.4649e-12, 1.8096e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8269]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8268904\n",
            "state: tensor([[8.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0219e-14, 1.0000e+00, 2.3149e-12, 1.0470e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8378]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8377606\n",
            "state: tensor([[ 8.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[6.2133e-15, 1.0000e+00, 1.5407e-12, 6.0278e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8460]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84596294\n",
            "state: tensor([[ 8.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[3.0107e-15, 1.0000e+00, 8.4893e-13, 2.7186e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8538]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8538486\n",
            "state: tensor([[9.0000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[1.2022e-14, 1.0000e+00, 2.4604e-12, 1.3969e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3339]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3338542\n",
            "state: tensor([[9.0000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[1.5307e-14, 1.0000e+00, 3.0096e-12, 1.8186e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3617]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3616816\n",
            "state: tensor([[9.0000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[1.8532e-14, 1.0000e+00, 3.5318e-12, 2.2389e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3928]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39283523\n",
            "state: tensor([[9.0000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[2.2436e-14, 1.0000e+00, 4.1446e-12, 2.7565e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4248]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4247927\n",
            "state: tensor([[9.0000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[2.7003e-14, 1.0000e+00, 4.8398e-12, 3.3723e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4557]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4556955\n",
            "state: tensor([[9.0000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0956e-14, 1.0000e+00, 5.4303e-12, 3.9086e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4908]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49082115\n",
            "state: tensor([[9.0000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[3.1615e-14, 1.0000e+00, 5.5441e-12, 3.9854e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5266]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5266332\n",
            "state: tensor([[9.0000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0774e-14, 1.0000e+00, 5.4389e-12, 3.8569e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56342065\n",
            "state: tensor([[9.0000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[2.9955e-14, 1.0000e+00, 5.3356e-12, 3.7325e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5994]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59936553\n",
            "state: tensor([[9.0000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[2.9158e-14, 1.0000e+00, 5.2343e-12, 3.6122e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6349]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6348702\n",
            "state: tensor([[9.0000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[2.8208e-14, 1.0000e+00, 5.1090e-12, 3.4721e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6696]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.669612\n",
            "state: tensor([[9.0000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[2.3791e-14, 1.0000e+00, 4.4562e-12, 2.8633e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7026]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70260197\n",
            "state: tensor([[9.0000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[2.0220e-14, 1.0000e+00, 3.9139e-12, 2.3770e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7306]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7306309\n",
            "state: tensor([[9.0000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[1.7148e-14, 1.0000e+00, 3.4319e-12, 1.9677e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7529]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75292015\n",
            "state: tensor([[9.0000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[1.4559e-14, 1.0000e+00, 3.0127e-12, 1.6308e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7719]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77186906\n",
            "state: tensor([[9.0000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[1.2334e-14, 1.0000e+00, 2.6417e-12, 1.3465e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7866]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7865804\n",
            "state: tensor([[9.0000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[1.0402e-14, 1.0000e+00, 2.3089e-12, 1.1050e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7985]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79849094\n",
            "state: tensor([[9.0000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[7.3829e-15, 1.0000e+00, 1.7494e-12, 7.5099e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8135]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.813541\n",
            "state: tensor([[9.0000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[5.1031e-15, 1.0000e+00, 1.2968e-12, 4.9597e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8283]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8283063\n",
            "state: tensor([[9.0000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4016e-15, 1.0000e+00, 9.3238e-13, 3.1508e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8414]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8413794\n",
            "state: tensor([[ 9.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0777e-15, 1.0000e+00, 6.2293e-13, 1.8230e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8518]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85175234\n",
            "state: tensor([[ 9.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.2691e-15, 1.0000e+00, 4.1618e-13, 1.0547e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8594]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85936064\n",
            "state: tensor([[9.5000, 0.0000, 0.2000]]) dist_dsc.probs: tensor([[2.7207e-15, 1.0000e+00, 7.2049e-13, 2.7532e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3356]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3355746\n",
            "state: tensor([[9.5000, 0.5000, 0.2000]]) dist_dsc.probs: tensor([[3.4181e-15, 1.0000e+00, 8.7163e-13, 3.5313e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3642]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36421928\n",
            "state: tensor([[9.5000, 1.0000, 0.2000]]) dist_dsc.probs: tensor([[4.1382e-15, 1.0000e+00, 1.0229e-12, 4.3477e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3945]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39449686\n",
            "state: tensor([[9.5000, 1.5000, 0.2000]]) dist_dsc.probs: tensor([[5.0099e-15, 1.0000e+00, 1.2003e-12, 5.3527e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4264]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42640293\n",
            "state: tensor([[9.5000, 2.0000, 0.2000]]) dist_dsc.probs: tensor([[6.0653e-15, 1.0000e+00, 1.4086e-12, 6.5901e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4573]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45728213\n",
            "state: tensor([[9.5000, 2.5000, 0.2000]]) dist_dsc.probs: tensor([[6.9441e-15, 1.0000e+00, 1.5787e-12, 7.6277e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4915]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49145567\n",
            "state: tensor([[9.5000, 3.0000, 0.2000]]) dist_dsc.probs: tensor([[7.3946e-15, 1.0000e+00, 1.6678e-12, 8.1456e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5261]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52606297\n",
            "state: tensor([[9.5000, 3.5000, 0.2000]]) dist_dsc.probs: tensor([[7.2166e-15, 1.0000e+00, 1.6398e-12, 7.9046e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56337196\n",
            "state: tensor([[9.5000, 4.0000, 0.2000]]) dist_dsc.probs: tensor([[7.0246e-15, 1.0000e+00, 1.6087e-12, 7.6497e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5988]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5987909\n",
            "state: tensor([[9.5000, 4.5000, 0.2000]]) dist_dsc.probs: tensor([[6.8377e-15, 1.0000e+00, 1.5781e-12, 7.4029e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6339]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63386947\n",
            "state: tensor([[9.5000, 5.0000, 0.2000]]) dist_dsc.probs: tensor([[6.6557e-15, 1.0000e+00, 1.5481e-12, 7.1642e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6671]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66713053\n",
            "state: tensor([[9.5000, 5.5000, 0.2000]]) dist_dsc.probs: tensor([[5.8062e-15, 1.0000e+00, 1.3876e-12, 6.1431e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7014]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7013696\n",
            "state: tensor([[9.5000, 6.0000, 0.2000]]) dist_dsc.probs: tensor([[4.9291e-15, 1.0000e+00, 1.2175e-12, 5.0948e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7300]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7299541\n",
            "state: tensor([[9.5000, 6.5000, 0.2000]]) dist_dsc.probs: tensor([[4.1835e-15, 1.0000e+00, 1.0682e-12, 4.2220e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7544]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7544243\n",
            "state: tensor([[9.5000, 7.0000, 0.2000]]) dist_dsc.probs: tensor([[3.5454e-15, 1.0000e+00, 9.3614e-13, 3.4917e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7746]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77459705\n",
            "state: tensor([[9.5000, 7.5000, 0.2000]]) dist_dsc.probs: tensor([[3.0102e-15, 1.0000e+00, 8.2178e-13, 2.8939e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7921]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79208064\n",
            "state: tensor([[9.5000, 8.0000, 0.2000]]) dist_dsc.probs: tensor([[2.5487e-15, 1.0000e+00, 7.2038e-13, 2.3870e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8037]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80372983\n",
            "state: tensor([[9.5000, 8.5000, 0.2000]]) dist_dsc.probs: tensor([[2.1443e-15, 1.0000e+00, 6.2842e-13, 1.9532e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8149]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8149163\n",
            "state: tensor([[9.5000, 9.0000, 0.2000]]) dist_dsc.probs: tensor([[1.5153e-15, 1.0000e+00, 4.7446e-13, 1.3212e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8290]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8289746\n",
            "state: tensor([[9.5000, 9.5000, 0.2000]]) dist_dsc.probs: tensor([[1.0474e-15, 1.0000e+00, 3.5169e-13, 8.7257e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8426]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8426238\n",
            "state: tensor([[ 9.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[6.9162e-16, 1.0000e+00, 2.5090e-13, 5.4861e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8546]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85458624\n",
            "state: tensor([[ 9.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[4.2244e-16, 1.0000e+00, 1.6763e-13, 3.1741e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8645]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86448425\n",
            "state: tensor([[10.0000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[6.1575e-16, 1.0000e+00, 2.1098e-13, 5.4263e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3373]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33729267\n",
            "state: tensor([[10.0000,  0.5000,  0.2000]]) dist_dsc.probs: tensor([[7.6327e-16, 1.0000e+00, 2.5244e-13, 6.8573e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3668]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36675185\n",
            "state: tensor([[10.0000,  1.0000,  0.2000]]) dist_dsc.probs: tensor([[9.2406e-16, 1.0000e+00, 2.9624e-13, 8.4425e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3970]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39695334\n",
            "state: tensor([[10.0000,  1.5000,  0.2000]]) dist_dsc.probs: tensor([[1.1187e-15, 1.0000e+00, 3.4764e-13, 1.0394e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4280]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4280104\n",
            "state: tensor([[10.0000,  2.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3516e-15, 1.0000e+00, 4.0727e-13, 1.2768e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4586]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4585793\n",
            "state: tensor([[10.0000,  2.5000,  0.2000]]) dist_dsc.probs: tensor([[1.5577e-15, 1.0000e+00, 4.5894e-13, 1.4886e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4921]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49208984\n",
            "state: tensor([[10.0000,  3.0000,  0.2000]]) dist_dsc.probs: tensor([[1.7245e-15, 1.0000e+00, 5.0050e-13, 1.6594e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5269]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52692014\n",
            "state: tensor([[10.0000,  3.5000,  0.2000]]) dist_dsc.probs: tensor([[1.6924e-15, 1.0000e+00, 4.9439e-13, 1.6200e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5633]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56331825\n",
            "state: tensor([[10.0000,  4.0000,  0.2000]]) dist_dsc.probs: tensor([[1.6473e-15, 1.0000e+00, 4.8501e-13, 1.5678e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5982]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5982157\n",
            "state: tensor([[10.0000,  4.5000,  0.2000]]) dist_dsc.probs: tensor([[1.6035e-15, 1.0000e+00, 4.7580e-13, 1.5172e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6333]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63333297\n",
            "state: tensor([[10.0000,  5.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5608e-15, 1.0000e+00, 4.6676e-13, 1.4683e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6660]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6659706\n",
            "state: tensor([[10.0000,  5.5000,  0.2000]]) dist_dsc.probs: tensor([[1.4273e-15, 1.0000e+00, 4.3493e-13, 1.3262e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6987]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6987257\n",
            "state: tensor([[10.0000,  6.0000,  0.2000]]) dist_dsc.probs: tensor([[1.2016e-15, 1.0000e+00, 3.7873e-13, 1.0920e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7293]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72927594\n",
            "state: tensor([[10.0000,  6.5000,  0.2000]]) dist_dsc.probs: tensor([[1.0210e-15, 1.0000e+00, 3.3257e-13, 9.0625e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7546]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7545985\n",
            "state: tensor([[10.0000,  7.0000,  0.2000]]) dist_dsc.probs: tensor([[8.6478e-16, 1.0000e+00, 2.9132e-13, 7.4902e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7761]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7761384\n",
            "state: tensor([[10.0000,  7.5000,  0.2000]]) dist_dsc.probs: tensor([[7.3303e-16, 1.0000e+00, 2.5536e-13, 6.1961e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7946]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79459494\n",
            "state: tensor([[10.0000,  8.0000,  0.2000]]) dist_dsc.probs: tensor([[6.2237e-16, 1.0000e+00, 2.2416e-13, 5.1353e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8096]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8095664\n",
            "state: tensor([[10.0000,  8.5000,  0.2000]]) dist_dsc.probs: tensor([[5.2666e-16, 1.0000e+00, 1.9645e-13, 4.2317e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8196]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81964046\n",
            "state: tensor([[10.0000,  9.0000,  0.2000]]) dist_dsc.probs: tensor([[4.4202e-16, 1.0000e+00, 1.7104e-13, 3.4522e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8301]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8301294\n",
            "state: tensor([[10.0000,  9.5000,  0.2000]]) dist_dsc.probs: tensor([[3.1103e-16, 1.0000e+00, 1.2868e-13, 2.3245e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8432]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8432413\n",
            "state: tensor([[10.0000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1498e-16, 1.0000e+00, 9.5381e-14, 1.5351e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8558]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85584164\n",
            "state: tensor([[10.0000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[1.4062e-16, 1.0000e+00, 6.7514e-14, 9.5523e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8668]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8667729\n",
            "state: tensor([[10.5000,  0.0000,  0.2000]]) dist_dsc.probs: tensor([[1.3936e-16, 1.0000e+00, 6.1782e-14, 1.0694e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3390]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33900857\n",
            "state: tensor([[10.5000,  0.5000,  0.2000]]) dist_dsc.probs: tensor([[1.7044e-16, 1.0000e+00, 7.3110e-14, 1.3316e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3693]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3692786\n",
            "state: tensor([[10.5000,  1.0000,  0.2000]]) dist_dsc.probs: tensor([[2.0634e-16, 1.0000e+00, 8.5795e-14, 1.6394e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3994]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3994128\n",
            "state: tensor([[10.5000,  1.5000,  0.2000]]) dist_dsc.probs: tensor([[2.4981e-16, 1.0000e+00, 1.0068e-13, 2.0184e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4294]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42944482\n",
            "state: tensor([[10.5000,  2.0000,  0.2000]]) dist_dsc.probs: tensor([[3.0089e-16, 1.0000e+00, 1.1765e-13, 2.4710e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4597]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45969665\n",
            "state: tensor([[10.5000,  2.5000,  0.2000]]) dist_dsc.probs: tensor([[3.4943e-16, 1.0000e+00, 1.3342e-13, 2.9051e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4927]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49272338\n",
            "state: tensor([[10.5000,  3.0000,  0.2000]]) dist_dsc.probs: tensor([[3.9677e-16, 1.0000e+00, 1.4855e-13, 3.3305e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5277]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52768886\n",
            "state: tensor([[10.5000,  3.5000,  0.2000]]) dist_dsc.probs: tensor([[3.9687e-16, 1.0000e+00, 1.4906e-13, 3.3201e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5628]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5628244\n",
            "state: tensor([[10.5000,  4.0000,  0.2000]]) dist_dsc.probs: tensor([[3.8631e-16, 1.0000e+00, 1.4623e-13, 3.2130e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5976]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5976399\n",
            "state: tensor([[10.5000,  4.5000,  0.2000]]) dist_dsc.probs: tensor([[3.7603e-16, 1.0000e+00, 1.4345e-13, 3.1094e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6328]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63279593\n",
            "state: tensor([[10.5000,  5.0000,  0.2000]]) dist_dsc.probs: tensor([[3.6602e-16, 1.0000e+00, 1.4073e-13, 3.0091e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6655]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6655233\n",
            "state: tensor([[10.5000,  5.5000,  0.2000]]) dist_dsc.probs: tensor([[3.5086e-16, 1.0000e+00, 1.3632e-13, 2.8631e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6970]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6969505\n",
            "state: tensor([[10.5000,  6.0000,  0.2000]]) dist_dsc.probs: tensor([[2.9379e-16, 1.0000e+00, 1.1813e-13, 2.3465e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7282]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72820723\n",
            "state: tensor([[10.5000,  6.5000,  0.2000]]) dist_dsc.probs: tensor([[2.4896e-16, 1.0000e+00, 1.0348e-13, 1.9431e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7540]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7539811\n",
            "state: tensor([[10.5000,  7.0000,  0.2000]]) dist_dsc.probs: tensor([[2.1105e-16, 1.0000e+00, 9.0701e-14, 1.6078e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7772]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77722675\n",
            "state: tensor([[10.5000,  7.5000,  0.2000]]) dist_dsc.probs: tensor([[1.7876e-16, 1.0000e+00, 7.9450e-14, 1.3288e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7962]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79615563\n",
            "state: tensor([[10.5000,  8.0000,  0.2000]]) dist_dsc.probs: tensor([[1.5156e-16, 1.0000e+00, 6.9655e-14, 1.0995e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8130]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8130055\n",
            "state: tensor([[10.5000,  8.5000,  0.2000]]) dist_dsc.probs: tensor([[1.2861e-16, 1.0000e+00, 6.1131e-14, 9.1049e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8250]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8250327\n",
            "state: tensor([[10.5000,  9.0000,  0.2000]]) dist_dsc.probs: tensor([[1.0883e-16, 1.0000e+00, 5.3570e-14, 7.5019e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8344]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83437955\n",
            "state: tensor([[10.5000,  9.5000,  0.2000]]) dist_dsc.probs: tensor([[9.1115e-17, 1.0000e+00, 4.6553e-14, 6.1019e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8442]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8441993\n",
            "state: tensor([[10.5000, 10.0000,  0.2000]]) dist_dsc.probs: tensor([[6.3839e-17, 1.0000e+00, 3.4898e-14, 4.0895e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8564]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85641116\n",
            "state: tensor([[10.5000, 10.5000,  0.2000]]) dist_dsc.probs: tensor([[4.4126e-17, 1.0000e+00, 2.5868e-14, 2.7008e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8680]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8680287\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 1.]]) dist_dsc.probs: tensor([[2.1380e-05, 9.9810e-01, 1.0964e-04, 1.7722e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3954]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39544967\n",
            "state: tensor([[0.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[4.1348e-05, 9.9613e-01, 1.8998e-04, 3.6354e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3767]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4152]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37673846\n",
            "state: tensor([[0., 1., 1.]]) dist_dsc.probs: tensor([[6.5155e-05, 9.9370e-01, 2.7796e-04, 5.9589e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3454]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2778]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3454243\n",
            "state: tensor([[0.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[7.4172e-05, 9.9276e-01, 3.1057e-04, 6.8562e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3386]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3521]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33856523\n",
            "state: tensor([[0., 2., 1.]]) dist_dsc.probs: tensor([[4.6145e-05, 9.9567e-01, 2.1045e-04, 4.0720e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3577]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.1665]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3577226\n",
            "state: tensor([[0.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1962e-05, 9.9806e-01, 1.1423e-04, 1.8044e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3763]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.2379]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37631348\n",
            "state: tensor([[0., 3., 1.]]) dist_dsc.probs: tensor([[9.0543e-06, 9.9925e-01, 5.5031e-05, 6.8348e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3974]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.4996]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39744073\n",
            "state: tensor([[0.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[3.6397e-06, 9.9972e-01, 2.5962e-05, 2.5182e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4189]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8211]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4188994\n",
            "state: tensor([[0., 4., 1.]]) dist_dsc.probs: tensor([[1.3654e-06, 9.9990e-01, 1.1571e-05, 8.5942e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2453]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43969962\n",
            "state: tensor([[0.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[5.0996e-07, 9.9997e-01, 5.1366e-06, 2.9208e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4628]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6773]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46278632\n",
            "state: tensor([[0., 5., 1.]]) dist_dsc.probs: tensor([[1.9043e-07, 9.9999e-01, 2.2800e-06, 9.9247e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4810]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1671]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48101944\n",
            "state: tensor([[0.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[7.1112e-08, 1.0000e+00, 1.0120e-06, 3.3724e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4968]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7087]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49677396\n",
            "state: tensor([[0., 6., 1.]]) dist_dsc.probs: tensor([[2.5245e-08, 1.0000e+00, 4.3109e-07, 1.0824e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5137]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3689]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51374614\n",
            "state: tensor([[0.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[8.9172e-09, 1.0000e+00, 1.8290e-07, 3.4539e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5299]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5299341\n",
            "state: tensor([[0., 7., 1.]]) dist_dsc.probs: tensor([[3.1499e-09, 1.0000e+00, 7.7596e-08, 1.1021e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5459]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5458742\n",
            "state: tensor([[0.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1126e-09, 1.0000e+00, 3.2921e-08, 3.5166e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5628]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56278807\n",
            "state: tensor([[0., 8., 1.]]) dist_dsc.probs: tensor([[3.9048e-10, 1.0000e+00, 1.3893e-08, 1.1141e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5794]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5793903\n",
            "state: tensor([[0.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.3353e-10, 1.0000e+00, 5.7385e-09, 3.4294e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5940]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5940091\n",
            "state: tensor([[0., 9., 1.]]) dist_dsc.probs: tensor([[4.5660e-11, 1.0000e+00, 2.3703e-09, 1.0556e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6077]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60767084\n",
            "state: tensor([[0.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5614e-11, 1.0000e+00, 9.7909e-10, 3.2494e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6218]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.621817\n",
            "state: tensor([[ 0., 10.,  1.]]) dist_dsc.probs: tensor([[5.3392e-12, 1.0000e+00, 4.0442e-10, 1.0002e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6354]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6354229\n",
            "state: tensor([[ 0.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.8258e-12, 1.0000e+00, 1.6705e-10, 3.0788e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6486]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6486454\n",
            "state: tensor([[0.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3127e-05, 9.9887e-01, 7.3209e-05, 1.0424e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3891]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38913462\n",
            "state: tensor([[0.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4312e-05, 9.9782e-01, 1.2249e-04, 2.0377e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3749]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7556]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37489542\n",
            "state: tensor([[0.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[4.1768e-05, 9.9609e-01, 1.9249e-04, 3.6708e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3608]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5114]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36081365\n",
            "state: tensor([[0.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[4.3626e-05, 9.9592e-01, 2.0028e-04, 3.8381e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3764]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.7605]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37644655\n",
            "state: tensor([[0.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[2.7310e-05, 9.9754e-01, 1.3638e-04, 2.2955e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3980]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.4854]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39802238\n",
            "state: tensor([[0.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5023e-05, 9.9871e-01, 8.3503e-05, 1.1896e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4115]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3770]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4114565\n",
            "state: tensor([[0.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[7.2904e-06, 9.9941e-01, 4.6051e-05, 5.3838e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4269]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.3960]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4268515\n",
            "state: tensor([[0.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[3.1457e-06, 9.9976e-01, 2.3037e-05, 2.1443e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4475]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.5533]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44752976\n",
            "state: tensor([[0.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1966e-06, 9.9991e-01, 1.0383e-05, 7.4371e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4723]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.9060]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47225538\n",
            "state: tensor([[0.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[4.4765e-07, 9.9997e-01, 4.6157e-06, 2.5321e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4927]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3609]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49266508\n",
            "state: tensor([[0.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[1.6357e-07, 9.9999e-01, 2.0127e-06, 8.3965e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5101]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8775]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5101485\n",
            "state: tensor([[0.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[5.7974e-08, 1.0000e+00, 8.5622e-07, 2.6910e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5279]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4623]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5278642\n",
            "state: tensor([[0.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[2.0541e-08, 1.0000e+00, 3.6414e-07, 8.6209e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5452]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1017]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54517645\n",
            "state: tensor([[0.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[7.1781e-09, 1.0000e+00, 1.5313e-07, 2.7178e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5601]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56008905\n",
            "state: tensor([[0.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[2.4873e-09, 1.0000e+00, 6.3951e-08, 8.4877e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5742]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57423943\n",
            "state: tensor([[0.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[8.6185e-10, 1.0000e+00, 2.6707e-08, 2.6507e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5887]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5886891\n",
            "state: tensor([[0.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[2.9864e-10, 1.0000e+00, 1.1154e-08, 8.2780e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6031]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60305417\n",
            "state: tensor([[0.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0348e-10, 1.0000e+00, 4.6580e-09, 2.5852e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6155]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61548823\n",
            "state: tensor([[0.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[3.5856e-11, 1.0000e+00, 1.9453e-09, 8.0734e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6283]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62827796\n",
            "state: tensor([[0.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2424e-11, 1.0000e+00, 8.1239e-10, 2.5213e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6409]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6409103\n",
            "state: tensor([[ 0.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[4.3051e-12, 1.0000e+00, 3.3927e-10, 7.8739e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6531]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65309316\n",
            "state: tensor([[ 0.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.4787e-12, 1.0000e+00, 1.4066e-10, 2.4356e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6654]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6654407\n",
            "state: tensor([[1., 0., 1.]]) dist_dsc.probs: tensor([[6.4052e-06, 9.9948e-01, 4.0430e-05, 4.7695e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3847]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3847181\n",
            "state: tensor([[1.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2389e-05, 9.9894e-01, 7.0167e-05, 9.7626e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3664]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9347]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3663959\n",
            "state: tensor([[1., 1., 1.]]) dist_dsc.probs: tensor([[1.7932e-05, 9.9843e-01, 9.5605e-05, 1.4589e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9998]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37403324\n",
            "state: tensor([[1.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[1.7874e-05, 9.9844e-01, 9.5697e-05, 1.4494e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4086]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3951]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40862423\n",
            "state: tensor([[1., 2., 1.]]) dist_dsc.probs: tensor([[1.4577e-05, 9.9875e-01, 8.1133e-05, 1.1562e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4395]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8448]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43945205\n",
            "state: tensor([[1.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[8.5801e-06, 9.9929e-01, 5.2499e-05, 6.4620e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4503]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.6433]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45033652\n",
            "state: tensor([[1., 3., 1.]]) dist_dsc.probs: tensor([[4.6173e-06, 9.9964e-01, 3.1558e-05, 3.2703e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4664]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.5394]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4664251\n",
            "state: tensor([[1.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1347e-06, 9.9984e-01, 1.6719e-05, 1.4033e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4838]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.6203]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48378268\n",
            "state: tensor([[1., 4., 1.]]) dist_dsc.probs: tensor([[9.0448e-07, 9.9994e-01, 8.2399e-06, 5.4770e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5013]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8019]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5012961\n",
            "state: tensor([[1.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[3.6872e-07, 9.9998e-01, 3.9353e-06, 2.0471e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5174]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0870]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5173764\n",
            "state: tensor([[1., 5., 1.]]) dist_dsc.probs: tensor([[1.2982e-07, 9.9999e-01, 1.6653e-06, 6.5089e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5372]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6384]], grad_fn=<ExpBackward0>)\n",
            "1 -0.537151\n",
            "state: tensor([[1.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[4.5232e-08, 1.0000e+00, 6.9862e-07, 2.0456e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5546]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.2488]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5545845\n",
            "state: tensor([[1., 6., 1.]]) dist_dsc.probs: tensor([[1.5759e-08, 1.0000e+00, 2.9308e-07, 6.4287e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5721]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9010]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5720877\n",
            "state: tensor([[1.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[5.4688e-09, 1.0000e+00, 1.2255e-07, 2.0110e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5876]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58755547\n",
            "state: tensor([[1., 7., 1.]]) dist_dsc.probs: tensor([[1.8950e-09, 1.0000e+00, 5.1180e-08, 6.2802e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6019]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6018614\n",
            "state: tensor([[1.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[6.5662e-10, 1.0000e+00, 2.1374e-08, 1.9613e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6156]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6155519\n",
            "state: tensor([[1., 8., 1.]]) dist_dsc.probs: tensor([[2.2752e-10, 1.0000e+00, 8.9262e-09, 6.1250e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6278]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6278238\n",
            "state: tensor([[1.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[7.8838e-11, 1.0000e+00, 3.7278e-09, 1.9128e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63974214\n",
            "state: tensor([[1., 9., 1.]]) dist_dsc.probs: tensor([[2.7318e-11, 1.0000e+00, 1.5568e-09, 5.9736e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6522]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6521818\n",
            "state: tensor([[1.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[9.4658e-12, 1.0000e+00, 6.5015e-10, 1.8656e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6641]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6641238\n",
            "state: tensor([[ 1., 10.,  1.]]) dist_dsc.probs: tensor([[3.2799e-12, 1.0000e+00, 2.7152e-10, 5.8260e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6756]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6756038\n",
            "state: tensor([[ 1.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.1365e-12, 1.0000e+00, 1.1339e-10, 1.8195e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6868]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6867639\n",
            "state: tensor([[1.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[2.5184e-06, 9.9981e-01, 1.8690e-05, 1.7208e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3734]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37336865\n",
            "state: tensor([[1.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[5.0289e-06, 9.9960e-01, 3.3236e-05, 3.6590e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3626]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3626147\n",
            "state: tensor([[1.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[6.9969e-06, 9.9943e-01, 4.3844e-05, 5.2317e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3779]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4834]], grad_fn=<ExpBackward0>)\n",
            "1 -0.377936\n",
            "state: tensor([[1.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[6.7852e-06, 9.9945e-01, 4.2868e-05, 5.0521e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4118]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9012]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41175675\n",
            "state: tensor([[1.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[5.9056e-06, 9.9952e-01, 3.8360e-05, 4.3254e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4532]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3172]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45315292\n",
            "state: tensor([[1.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[4.3839e-06, 9.9965e-01, 3.0100e-05, 3.1085e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4810]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0022]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48097146\n",
            "state: tensor([[1.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[2.5478e-06, 9.9981e-01, 1.9273e-05, 1.7130e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4975]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8355]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4974642\n",
            "state: tensor([[1.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2546e-06, 9.9991e-01, 1.0770e-05, 7.8562e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5141]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[4.8389]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5141005\n",
            "state: tensor([[1.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[5.3045e-07, 9.9996e-01, 5.3035e-06, 3.0508e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5331]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.0351]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53313845\n",
            "state: tensor([[1.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1061e-07, 9.9999e-01, 2.4789e-06, 1.1068e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5487]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3306]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5487206\n",
            "state: tensor([[1.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[8.3619e-08, 9.9999e-01, 1.1587e-06, 4.0156e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6833]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56344473\n",
            "state: tensor([[1.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[3.2840e-08, 1.0000e+00, 5.3675e-07, 1.4393e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5774]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0732]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5774351\n",
            "state: tensor([[1.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1743e-08, 1.0000e+00, 2.3009e-07, 4.6535e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5930]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6726]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59300625\n",
            "state: tensor([[1.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[4.0965e-09, 1.0000e+00, 9.6636e-08, 1.4637e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6089]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3755]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60894597\n",
            "state: tensor([[1.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[1.4263e-09, 1.0000e+00, 4.0527e-08, 4.5930e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6247]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62467027\n",
            "state: tensor([[1.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[4.9663e-10, 1.0000e+00, 1.6996e-08, 1.4412e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6386]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63863343\n",
            "state: tensor([[1.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[1.7281e-10, 1.0000e+00, 7.1241e-09, 4.5194e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6524]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.652384\n",
            "state: tensor([[1.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[6.0007e-11, 1.0000e+00, 2.9807e-09, 1.4141e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6661]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6660623\n",
            "state: tensor([[1.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[2.0813e-11, 1.0000e+00, 1.2459e-09, 4.4200e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6786]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67863077\n",
            "state: tensor([[1.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[7.2117e-12, 1.0000e+00, 5.2031e-10, 1.3803e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6899]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.689853\n",
            "state: tensor([[ 1.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[2.4989e-12, 1.0000e+00, 2.1729e-10, 4.3108e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7005]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.700451\n",
            "state: tensor([[ 1.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[8.6588e-13, 1.0000e+00, 9.0747e-11, 1.3462e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7107]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7107287\n",
            "state: tensor([[2., 0., 1.]]) dist_dsc.probs: tensor([[8.6918e-07, 9.9994e-01, 7.7569e-06, 5.3914e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3550]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35498735\n",
            "state: tensor([[2.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5405e-06, 9.9989e-01, 1.2474e-05, 1.0085e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3589]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35887015\n",
            "state: tensor([[2., 1., 1.]]) dist_dsc.probs: tensor([[2.2823e-06, 9.9983e-01, 1.7312e-05, 1.5468e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3789]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3097]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37890956\n",
            "state: tensor([[2.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[2.5264e-06, 9.9981e-01, 1.8897e-05, 1.7230e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4145]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41438577\n",
            "state: tensor([[2., 2., 1.]]) dist_dsc.probs: tensor([[2.1562e-06, 9.9984e-01, 1.6642e-05, 1.4435e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4555]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8559]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45548525\n",
            "state: tensor([[2.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.6769e-06, 9.9988e-01, 1.3574e-05, 1.0912e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4922]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5392]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49219278\n",
            "state: tensor([[2., 3., 1.]]) dist_dsc.probs: tensor([[1.1501e-06, 9.9992e-01, 9.9817e-06, 7.1779e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5222]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3485]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52220553\n",
            "state: tensor([[2.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[6.0155e-07, 9.9996e-01, 5.8634e-06, 3.5168e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5430]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3294]], grad_fn=<ExpBackward0>)\n",
            "1 -0.543001\n",
            "state: tensor([[2., 4., 1.]]) dist_dsc.probs: tensor([[2.8243e-07, 9.9998e-01, 3.1515e-06, 1.5292e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5580]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3978]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5579718\n",
            "state: tensor([[2.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1948e-07, 9.9999e-01, 1.5525e-06, 5.9420e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5759]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6163]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5758999\n",
            "state: tensor([[2., 5., 1.]]) dist_dsc.probs: tensor([[4.7437e-08, 1.0000e+00, 7.2566e-07, 2.1557e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5914]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9614]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59137344\n",
            "state: tensor([[2.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8834e-08, 1.0000e+00, 3.3918e-07, 7.8210e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6051]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3558]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6050608\n",
            "state: tensor([[2., 6., 1.]]) dist_dsc.probs: tensor([[7.4163e-09, 1.0000e+00, 1.5746e-07, 2.8116e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6179]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7856]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61792994\n",
            "state: tensor([[2.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8802e-09, 1.0000e+00, 7.2277e-08, 9.9517e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6295]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2501]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62952733\n",
            "state: tensor([[2., 7., 1.]]) dist_dsc.probs: tensor([[1.0485e-09, 1.0000e+00, 3.1449e-08, 3.2790e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6406]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.64061594\n",
            "state: tensor([[2.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[3.6506e-10, 1.0000e+00, 1.3189e-08, 1.0289e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6551]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65514797\n",
            "state: tensor([[2., 8., 1.]]) dist_dsc.probs: tensor([[1.2711e-10, 1.0000e+00, 5.5312e-09, 3.2285e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6693]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6692963\n",
            "state: tensor([[2.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[4.4257e-11, 1.0000e+00, 2.3196e-09, 1.0130e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6829]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6828904\n",
            "state: tensor([[2., 9., 1.]]) dist_dsc.probs: tensor([[1.5410e-11, 1.0000e+00, 9.7280e-10, 3.1787e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6955]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6954892\n",
            "state: tensor([[2.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[5.3654e-12, 1.0000e+00, 4.0797e-10, 9.9743e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7076]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7075517\n",
            "state: tensor([[ 2., 10.,  1.]]) dist_dsc.probs: tensor([[1.8681e-12, 1.0000e+00, 1.7109e-10, 3.1297e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7188]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71883976\n",
            "state: tensor([[ 2.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[6.5045e-13, 1.0000e+00, 7.1751e-11, 9.8205e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7298]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.729761\n",
            "state: tensor([[2.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[2.6092e-07, 9.9998e-01, 2.8667e-06, 1.4506e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3437]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34370023\n",
            "state: tensor([[2.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[4.0182e-07, 9.9997e-01, 4.1049e-06, 2.3248e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3637]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3636614\n",
            "state: tensor([[2.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[5.5468e-07, 9.9996e-01, 5.3663e-06, 3.3099e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3916]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3916033\n",
            "state: tensor([[2.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[7.4156e-07, 9.9995e-01, 6.8416e-06, 4.5345e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4186]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4185898\n",
            "state: tensor([[2.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[7.5677e-07, 9.9995e-01, 6.9878e-06, 4.6117e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4568]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4689]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45676297\n",
            "state: tensor([[2.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[5.8895e-07, 9.9996e-01, 5.7069e-06, 3.4842e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4958]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0541]], grad_fn=<ExpBackward0>)\n",
            "1 -0.495837\n",
            "state: tensor([[2.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[3.9454e-07, 9.9997e-01, 4.1171e-06, 2.2321e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5336]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9861]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5336251\n",
            "state: tensor([[2.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[2.5237e-07, 9.9998e-01, 2.8594e-06, 1.3588e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5643]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9277]], grad_fn=<ExpBackward0>)\n",
            "1 -0.564288\n",
            "state: tensor([[2.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3275e-07, 9.9999e-01, 1.6878e-06, 6.6975e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5838]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9226]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5838306\n",
            "state: tensor([[2.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[6.3577e-08, 1.0000e+00, 9.2213e-07, 2.9766e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5986]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9902]], grad_fn=<ExpBackward0>)\n",
            "1 -0.598621\n",
            "state: tensor([[2.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[2.6911e-08, 1.0000e+00, 4.5447e-07, 1.1573e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6149]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.2492]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6149156\n",
            "state: tensor([[2.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0684e-08, 1.0000e+00, 2.1242e-07, 4.1986e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6316]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6540]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63164294\n",
            "state: tensor([[2.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[4.2420e-09, 1.0000e+00, 9.9287e-08, 1.5233e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6437]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0805]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6437429\n",
            "state: tensor([[2.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[1.6748e-09, 1.0000e+00, 4.6195e-08, 5.4922e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6548]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6547626\n",
            "state: tensor([[2.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[6.4894e-10, 1.0000e+00, 2.1166e-08, 1.9382e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6646]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6646091\n",
            "state: tensor([[2.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[2.5127e-10, 1.0000e+00, 9.6923e-09, 6.8338e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67402256\n",
            "state: tensor([[2.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[9.3435e-11, 1.0000e+00, 4.2923e-09, 2.3049e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6850]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68497777\n",
            "state: tensor([[2.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[3.2533e-11, 1.0000e+00, 1.8001e-09, 7.2324e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6976]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6975949\n",
            "state: tensor([[2.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1327e-11, 1.0000e+00, 7.5491e-10, 2.2694e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7098]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7097938\n",
            "state: tensor([[2.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[3.9440e-12, 1.0000e+00, 3.1659e-10, 7.1210e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7216]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7215813\n",
            "state: tensor([[ 2.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[1.3732e-12, 1.0000e+00, 1.3277e-10, 2.2344e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7329]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73288226\n",
            "state: tensor([[ 2.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[4.7813e-13, 1.0000e+00, 5.5680e-11, 7.0112e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7434]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74335\n",
            "state: tensor([[3., 0., 1.]]) dist_dsc.probs: tensor([[6.8173e-08, 1.0000e+00, 9.4487e-07, 3.3493e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3345]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33448797\n",
            "state: tensor([[3.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0182e-07, 9.9999e-01, 1.3191e-06, 5.1903e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3663]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36627886\n",
            "state: tensor([[3., 1., 1.]]) dist_dsc.probs: tensor([[1.3504e-07, 9.9999e-01, 1.6677e-06, 7.0772e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4017]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40171134\n",
            "state: tensor([[3.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[1.7560e-07, 9.9999e-01, 2.0755e-06, 9.4312e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4303]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43028787\n",
            "state: tensor([[3., 2., 1.]]) dist_dsc.probs: tensor([[2.0282e-07, 9.9999e-01, 2.3490e-06, 1.0970e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4615]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46146816\n",
            "state: tensor([[3.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[2.0008e-07, 9.9999e-01, 2.3341e-06, 1.0734e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4985]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6646]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49851307\n",
            "state: tensor([[3., 3., 1.]]) dist_dsc.probs: tensor([[1.3242e-07, 9.9999e-01, 1.6680e-06, 6.7806e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6013]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5397492\n",
            "state: tensor([[3.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[8.4967e-08, 9.9999e-01, 1.1614e-06, 4.1399e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5744]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5634]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57443184\n",
            "state: tensor([[3., 4., 1.]]) dist_dsc.probs: tensor([[5.4599e-08, 1.0000e+00, 8.0964e-07, 2.5329e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6037]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5097]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60369545\n",
            "state: tensor([[3.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8896e-08, 1.0000e+00, 4.8034e-07, 1.2563e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6222]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5525]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6222479\n",
            "state: tensor([[3., 5., 1.]]) dist_dsc.probs: tensor([[1.4288e-08, 1.0000e+00, 2.6945e-07, 5.7836e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6362]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6138]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6362007\n",
            "state: tensor([[3.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[6.0611e-09, 1.0000e+00, 1.3304e-07, 2.2540e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6509]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.8822]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65085536\n",
            "state: tensor([[3., 6., 1.]]) dist_dsc.probs: tensor([[2.4065e-09, 1.0000e+00, 6.2182e-08, 8.1775e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6646]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3388]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66461855\n",
            "state: tensor([[3.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[9.5543e-10, 1.0000e+00, 2.9064e-08, 2.9668e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6777]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6776881\n",
            "state: tensor([[3., 7., 1.]]) dist_dsc.probs: tensor([[3.7763e-10, 1.0000e+00, 1.3535e-08, 1.0707e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6885]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6885039\n",
            "state: tensor([[3.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4622e-10, 1.0000e+00, 6.1982e-09, 3.7749e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6974]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69743794\n",
            "state: tensor([[3., 8., 1.]]) dist_dsc.probs: tensor([[5.6614e-11, 1.0000e+00, 2.8383e-09, 1.3310e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7060]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.705982\n",
            "state: tensor([[3.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1920e-11, 1.0000e+00, 1.2997e-09, 4.6928e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7143989\n",
            "state: tensor([[3., 9., 1.]]) dist_dsc.probs: tensor([[8.3265e-12, 1.0000e+00, 5.8582e-10, 1.6202e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7234]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72341883\n",
            "state: tensor([[3.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8991e-12, 1.0000e+00, 2.4568e-10, 5.0839e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7347]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7347387\n",
            "state: tensor([[ 3., 10.,  1.]]) dist_dsc.probs: tensor([[1.0094e-12, 1.0000e+00, 1.0303e-10, 1.5952e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7457]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7456637\n",
            "state: tensor([[ 3.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[3.5147e-13, 1.0000e+00, 4.3209e-11, 5.0055e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7561]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75610423\n",
            "state: tensor([[3.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[1.7422e-08, 1.0000e+00, 3.0581e-07, 7.5468e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3316]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33160743\n",
            "state: tensor([[3.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4844e-08, 1.0000e+00, 4.1073e-07, 1.1126e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3677]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36771634\n",
            "state: tensor([[3.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[3.1474e-08, 1.0000e+00, 5.0018e-07, 1.4411e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4068]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40683872\n",
            "state: tensor([[3.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[3.9777e-08, 1.0000e+00, 6.0801e-07, 1.8616e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4393]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43927726\n",
            "state: tensor([[3.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[4.8055e-08, 1.0000e+00, 7.1305e-07, 2.2816e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4702]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47019342\n",
            "state: tensor([[3.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[5.1457e-08, 1.0000e+00, 7.5847e-07, 2.4407e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5051]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5050774\n",
            "state: tensor([[3.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[4.2438e-08, 1.0000e+00, 6.5024e-07, 1.9597e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5430]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3806]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5429594\n",
            "state: tensor([[3.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8695e-08, 1.0000e+00, 4.7293e-07, 1.2669e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5811]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2724]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58106923\n",
            "state: tensor([[3.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.8336e-08, 1.0000e+00, 3.2818e-07, 7.7004e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6099]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1924]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6099139\n",
            "state: tensor([[3.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1755e-08, 1.0000e+00, 2.2835e-07, 4.6970e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6349]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1063]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63494086\n",
            "state: tensor([[3.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[6.2276e-09, 1.0000e+00, 1.3561e-07, 2.3307e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6548]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1832]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6548345\n",
            "state: tensor([[3.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[3.1683e-09, 1.0000e+00, 7.7874e-08, 1.1072e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6694]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2317]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66944754\n",
            "state: tensor([[3.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3550e-09, 1.0000e+00, 3.8707e-08, 4.3537e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6841]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6840529\n",
            "state: tensor([[3.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[5.4042e-10, 1.0000e+00, 1.8159e-08, 1.5875e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6961]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69607717\n",
            "state: tensor([[3.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[2.1509e-10, 1.0000e+00, 8.5048e-09, 5.7747e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7071]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70713073\n",
            "state: tensor([[3.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[8.5086e-11, 1.0000e+00, 3.9637e-09, 2.0853e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7164]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71638006\n",
            "state: tensor([[3.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[3.2945e-11, 1.0000e+00, 1.8151e-09, 7.3522e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7257]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7256843\n",
            "state: tensor([[3.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2756e-11, 1.0000e+00, 8.3118e-10, 2.5923e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7346]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7345895\n",
            "state: tensor([[3.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[4.9390e-12, 1.0000e+00, 3.8062e-10, 9.1399e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7426]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7426407\n",
            "state: tensor([[3.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.9123e-12, 1.0000e+00, 1.7429e-10, 3.2226e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7504]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7503705\n",
            "state: tensor([[ 3.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[7.4044e-13, 1.0000e+00, 7.9814e-11, 1.1362e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7579]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75790024\n",
            "state: tensor([[ 3.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[2.5836e-13, 1.0000e+00, 3.3531e-11, 3.5736e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7679]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7679438\n",
            "state: tensor([[4., 0., 1.]]) dist_dsc.probs: tensor([[4.3785e-09, 1.0000e+00, 9.7639e-08, 1.6689e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3292]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3292268\n",
            "state: tensor([[4.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[5.6822e-09, 1.0000e+00, 1.2133e-07, 2.2182e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3687]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36869055\n",
            "state: tensor([[4., 1., 1.]]) dist_dsc.probs: tensor([[7.1678e-09, 1.0000e+00, 1.4724e-07, 2.8593e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4081]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40807244\n",
            "state: tensor([[4.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[9.0418e-09, 1.0000e+00, 1.7868e-07, 3.6858e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4445]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44445127\n",
            "state: tensor([[4., 2., 1.]]) dist_dsc.probs: tensor([[1.1246e-08, 1.0000e+00, 2.1444e-07, 4.6737e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4782]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47820053\n",
            "state: tensor([[4.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2150e-08, 1.0000e+00, 2.2958e-07, 5.0575e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5127]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51265085\n",
            "state: tensor([[4., 3., 1.]]) dist_dsc.probs: tensor([[1.1337e-08, 1.0000e+00, 2.1802e-07, 4.6462e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5457]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5457122\n",
            "state: tensor([[4.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[8.7517e-09, 1.0000e+00, 1.7701e-07, 3.4683e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5819]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58186966\n",
            "state: tensor([[4., 4., 1.]]) dist_dsc.probs: tensor([[6.1700e-09, 1.0000e+00, 1.3324e-07, 2.3470e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6135]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61353153\n",
            "state: tensor([[4.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[3.9750e-09, 1.0000e+00, 9.3080e-08, 1.4394e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6407]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.640748\n",
            "state: tensor([[4., 5., 1.]]) dist_dsc.probs: tensor([[2.5192e-09, 1.0000e+00, 6.4147e-08, 8.6714e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6658]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66576463\n",
            "state: tensor([[4.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.3438e-09, 1.0000e+00, 3.8321e-08, 4.3314e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6837]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.683654\n",
            "state: tensor([[4., 6., 1.]]) dist_dsc.probs: tensor([[6.8388e-10, 1.0000e+00, 2.2012e-08, 2.0578e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6994]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6994177\n",
            "state: tensor([[4.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[3.0116e-10, 1.0000e+00, 1.1209e-08, 8.3559e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7135]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7134754\n",
            "state: tensor([[4., 7., 1.]]) dist_dsc.probs: tensor([[1.2012e-10, 1.0000e+00, 5.2586e-09, 3.0468e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7254]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7254061\n",
            "state: tensor([[4.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[4.7776e-11, 1.0000e+00, 2.4617e-09, 1.1070e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7341]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73406327\n",
            "state: tensor([[4., 8., 1.]]) dist_dsc.probs: tensor([[1.8990e-11, 1.0000e+00, 1.1518e-09, 4.0189e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7419]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7419145\n",
            "state: tensor([[4.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[7.4042e-12, 1.0000e+00, 5.3044e-10, 1.4280e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7506]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75063455\n",
            "state: tensor([[4., 9., 1.]]) dist_dsc.probs: tensor([[2.8741e-12, 1.0000e+00, 2.4340e-10, 5.0488e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7587]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7587122\n",
            "state: tensor([[4.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1128e-12, 1.0000e+00, 1.1146e-10, 1.7801e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7666]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76659083\n",
            "state: tensor([[ 4., 10.,  1.]]) dist_dsc.probs: tensor([[4.3087e-13, 1.0000e+00, 5.1040e-11, 6.2764e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7747]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7747151\n",
            "state: tensor([[ 4.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.6683e-13, 1.0000e+00, 2.3373e-11, 2.2129e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78238654\n",
            "state: tensor([[4.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[1.0243e-09, 1.0000e+00, 2.9395e-08, 3.4087e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3288]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.32881662\n",
            "state: tensor([[4.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2941e-09, 1.0000e+00, 3.5716e-08, 4.4014e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3696]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36964428\n",
            "state: tensor([[4.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[1.6324e-09, 1.0000e+00, 4.3343e-08, 5.6735e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4090]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4089924\n",
            "state: tensor([[4.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[2.0592e-09, 1.0000e+00, 5.2597e-08, 7.3134e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4459]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4459375\n",
            "state: tensor([[4.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[2.5663e-09, 1.0000e+00, 6.3219e-08, 9.2952e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4835]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4834665\n",
            "state: tensor([[4.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8264e-09, 1.0000e+00, 6.8684e-08, 1.0296e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5147]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5146566\n",
            "state: tensor([[4.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[2.7655e-09, 1.0000e+00, 6.7787e-08, 9.9825e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5493]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.549285\n",
            "state: tensor([[4.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[2.3480e-09, 1.0000e+00, 5.9561e-08, 8.2612e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5829]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5828855\n",
            "state: tensor([[4.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.8048e-09, 1.0000e+00, 4.8184e-08, 6.1381e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6167]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61673456\n",
            "state: tensor([[4.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2705e-09, 1.0000e+00, 3.6225e-08, 4.1461e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6435]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6435241\n",
            "state: tensor([[4.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[8.6171e-10, 1.0000e+00, 2.6400e-08, 2.6906e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6696]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.669645\n",
            "state: tensor([[4.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[5.4053e-10, 1.0000e+00, 1.8041e-08, 1.6026e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6933]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69327474\n",
            "state: tensor([[4.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[2.9113e-10, 1.0000e+00, 1.0865e-08, 8.0873e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7121]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71209276\n",
            "state: tensor([[4.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4723e-10, 1.0000e+00, 6.2081e-09, 3.8155e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7253]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72533035\n",
            "state: tensor([[4.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[6.6951e-11, 1.0000e+00, 3.2465e-09, 1.6042e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7404]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7403849\n",
            "state: tensor([[4.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[2.6654e-11, 1.0000e+00, 1.5208e-09, 5.8352e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7505]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7504889\n",
            "state: tensor([[4.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[1.0595e-11, 1.0000e+00, 7.1159e-10, 2.1184e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7593]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7593218\n",
            "state: tensor([[4.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[4.2112e-12, 1.0000e+00, 3.3295e-10, 7.6905e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7658]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76578206\n",
            "state: tensor([[4.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[1.6463e-12, 1.0000e+00, 1.5367e-10, 2.7406e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7731]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.773101\n",
            "state: tensor([[4.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[6.4034e-13, 1.0000e+00, 7.0630e-11, 9.7118e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7803]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.780325\n",
            "state: tensor([[ 4.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[2.4907e-13, 1.0000e+00, 3.2464e-11, 3.4416e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7876]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7876227\n",
            "state: tensor([[ 4.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[9.6879e-14, 1.0000e+00, 1.4921e-11, 1.2196e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7952]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7951871\n",
            "state: tensor([[5., 0., 1.]]) dist_dsc.probs: tensor([[2.3304e-10, 1.0000e+00, 8.6461e-09, 6.7559e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3301]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33006412\n",
            "state: tensor([[5.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[2.9471e-10, 1.0000e+00, 1.0514e-08, 8.7332e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3706]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3705973\n",
            "state: tensor([[5., 1., 1.]]) dist_dsc.probs: tensor([[3.7176e-10, 1.0000e+00, 1.2759e-08, 1.1257e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4099]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40991148\n",
            "state: tensor([[5.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[4.6895e-10, 1.0000e+00, 1.5483e-08, 1.4511e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4473]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44732302\n",
            "state: tensor([[5., 2., 1.]]) dist_dsc.probs: tensor([[5.8583e-10, 1.0000e+00, 1.8646e-08, 1.8493e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48241162\n",
            "state: tensor([[5.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[6.5715e-10, 1.0000e+00, 2.0547e-08, 2.0934e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5166]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5165768\n",
            "state: tensor([[5., 3., 1.]]) dist_dsc.probs: tensor([[6.4858e-10, 1.0000e+00, 2.0413e-08, 2.0514e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5503]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55026716\n",
            "state: tensor([[5.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[5.7276e-10, 1.0000e+00, 1.8519e-08, 1.7749e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5852]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5851604\n",
            "state: tensor([[5., 4., 1.]]) dist_dsc.probs: tensor([[4.8630e-10, 1.0000e+00, 1.6271e-08, 1.4689e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6170]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61698955\n",
            "state: tensor([[5.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[3.7220e-10, 1.0000e+00, 1.3116e-08, 1.0863e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6472]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6472057\n",
            "state: tensor([[5., 5., 1.]]) dist_dsc.probs: tensor([[2.6162e-10, 1.0000e+00, 9.8494e-09, 7.3248e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6717]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67166466\n",
            "state: tensor([[5.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8540e-10, 1.0000e+00, 7.4414e-09, 4.9878e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6966]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6965676\n",
            "state: tensor([[5., 6., 1.]]) dist_dsc.probs: tensor([[1.1597e-10, 1.0000e+00, 5.0735e-09, 2.9612e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7185]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7185151\n",
            "state: tensor([[5.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[6.3208e-11, 1.0000e+00, 3.0859e-09, 1.5135e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7364]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73643076\n",
            "state: tensor([[5., 7., 1.]]) dist_dsc.probs: tensor([[3.1836e-11, 1.0000e+00, 1.7573e-09, 7.1096e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7505]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75050586\n",
            "state: tensor([[5.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4889e-11, 1.0000e+00, 9.4052e-10, 3.0826e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7651]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76509327\n",
            "state: tensor([[5., 8., 1.]]) dist_dsc.probs: tensor([[5.9159e-12, 1.0000e+00, 4.3993e-10, 1.1180e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7739]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.773863\n",
            "state: tensor([[5.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[2.3494e-12, 1.0000e+00, 2.0570e-10, 4.0537e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7816]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7815514\n",
            "state: tensor([[5., 9., 1.]]) dist_dsc.probs: tensor([[9.3387e-13, 1.0000e+00, 9.6244e-11, 1.4716e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7880]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78804296\n",
            "state: tensor([[5.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[3.6605e-13, 1.0000e+00, 4.4517e-11, 5.2599e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7941]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7940607\n",
            "state: tensor([[ 5., 10.,  1.]]) dist_dsc.probs: tensor([[1.4238e-13, 1.0000e+00, 2.0461e-11, 1.8640e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8001]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8000782\n",
            "state: tensor([[ 5.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[5.5380e-14, 1.0000e+00, 9.4047e-12, 6.6053e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8068]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8067518\n",
            "state: tensor([[5.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[5.3020e-11, 1.0000e+00, 2.5431e-09, 1.3390e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3318]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3318089\n",
            "state: tensor([[5.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[6.7117e-11, 1.0000e+00, 3.0949e-09, 1.7328e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3715]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3715495\n",
            "state: tensor([[5.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[8.4664e-11, 1.0000e+00, 3.7558e-09, 2.2337e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4108]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4108298\n",
            "state: tensor([[5.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0680e-10, 1.0000e+00, 4.5578e-09, 2.8793e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4474]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44738367\n",
            "state: tensor([[5.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3360e-10, 1.0000e+00, 5.4947e-09, 3.6753e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4809]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4808637\n",
            "state: tensor([[5.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5409e-10, 1.0000e+00, 6.1942e-09, 4.2896e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5143784\n",
            "state: tensor([[5.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[1.5072e-10, 1.0000e+00, 6.1038e-09, 4.1691e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5522]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5521695\n",
            "state: tensor([[5.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[1.3962e-10, 1.0000e+00, 5.7556e-09, 3.8088e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5865]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58653784\n",
            "state: tensor([[5.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1863e-10, 1.0000e+00, 5.0591e-09, 3.1559e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6191]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6191232\n",
            "state: tensor([[5.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0072e-10, 1.0000e+00, 4.4452e-09, 2.6117e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6489]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.64892375\n",
            "state: tensor([[5.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[7.6757e-11, 1.0000e+00, 3.5705e-09, 1.9225e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67396915\n",
            "state: tensor([[5.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[5.3935e-11, 1.0000e+00, 2.6803e-09, 1.2958e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6981]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6980543\n",
            "state: tensor([[5.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[3.8176e-11, 1.0000e+00, 2.0232e-09, 8.8113e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7212]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72122943\n",
            "state: tensor([[5.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4879e-11, 1.0000e+00, 1.4268e-09, 5.4718e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7420]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7419949\n",
            "state: tensor([[5.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3723e-11, 1.0000e+00, 8.7646e-10, 2.8325e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7586]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7586178\n",
            "state: tensor([[5.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[6.8914e-12, 1.0000e+00, 4.9786e-10, 1.3262e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7727]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7726865\n",
            "state: tensor([[5.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[3.2272e-12, 1.0000e+00, 2.6677e-10, 5.7555e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7872]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78719145\n",
            "state: tensor([[5.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.3165e-12, 1.0000e+00, 1.2753e-10, 2.1491e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7964]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79638696\n",
            "state: tensor([[5.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[5.2181e-13, 1.0000e+00, 5.9534e-11, 7.7741e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8028]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8027769\n",
            "state: tensor([[5.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[2.0709e-13, 1.0000e+00, 2.7821e-11, 2.8161e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8081]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80807\n",
            "state: tensor([[ 5.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[8.1389e-14, 1.0000e+00, 1.2897e-11, 1.0095e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8134]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8133728\n",
            "state: tensor([[ 5.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[3.1657e-14, 1.0000e+00, 5.9276e-12, 3.5774e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8185]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8184564\n",
            "state: tensor([[6., 0., 1.]]) dist_dsc.probs: tensor([[1.2063e-11, 1.0000e+00, 7.4800e-10, 2.6539e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3356]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33560246\n",
            "state: tensor([[6.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5285e-11, 1.0000e+00, 9.1106e-10, 3.4383e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3725]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37248746\n",
            "state: tensor([[6., 1., 1.]]) dist_dsc.probs: tensor([[1.9281e-11, 1.0000e+00, 1.1056e-09, 4.4321e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4112]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41118878\n",
            "state: tensor([[6.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4322e-11, 1.0000e+00, 1.3417e-09, 5.7131e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4460]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44595063\n",
            "state: tensor([[6., 2., 1.]]) dist_dsc.probs: tensor([[3.0469e-11, 1.0000e+00, 1.6193e-09, 7.3045e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4791]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47911757\n",
            "state: tensor([[6.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[3.4755e-11, 1.0000e+00, 1.8091e-09, 8.4205e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5134]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5134021\n",
            "state: tensor([[6., 3., 1.]]) dist_dsc.probs: tensor([[3.5173e-11, 1.0000e+00, 1.8321e-09, 8.5079e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5511]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.551127\n",
            "state: tensor([[6.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[3.4003e-11, 1.0000e+00, 1.7888e-09, 8.1521e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5872]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58721\n",
            "state: tensor([[6., 4., 1.]]) dist_dsc.probs: tensor([[2.8934e-11, 1.0000e+00, 1.5728e-09, 6.7789e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6213]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62132996\n",
            "state: tensor([[6.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4569e-11, 1.0000e+00, 1.3821e-09, 5.6113e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6509]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6509179\n",
            "state: tensor([[6., 5., 1.]]) dist_dsc.probs: tensor([[2.0860e-11, 1.0000e+00, 1.2144e-09, 4.6438e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6787]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6787015\n",
            "state: tensor([[6.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5829e-11, 1.0000e+00, 9.7193e-10, 3.4025e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6989]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.698932\n",
            "state: tensor([[6., 6., 1.]]) dist_dsc.probs: tensor([[1.1119e-11, 1.0000e+00, 7.2939e-10, 2.2924e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7226]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7226496\n",
            "state: tensor([[6.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[7.8610e-12, 1.0000e+00, 5.5008e-10, 1.5566e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7442]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.744189\n",
            "state: tensor([[6., 7., 1.]]) dist_dsc.probs: tensor([[5.3008e-12, 1.0000e+00, 3.9896e-10, 1.0034e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7637]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76372606\n",
            "state: tensor([[6.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[2.9498e-12, 1.0000e+00, 2.4686e-10, 5.2444e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7789]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7789094\n",
            "state: tensor([[6., 8., 1.]]) dist_dsc.probs: tensor([[1.4892e-12, 1.0000e+00, 1.4086e-10, 2.4686e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7922]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7922278\n",
            "state: tensor([[6.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[6.9946e-13, 1.0000e+00, 7.5667e-11, 1.0746e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8067]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8066876\n",
            "state: tensor([[6., 9., 1.]]) dist_dsc.probs: tensor([[2.9410e-13, 1.0000e+00, 3.7088e-11, 4.1484e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8158]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81575936\n",
            "state: tensor([[6.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1625e-13, 1.0000e+00, 1.7274e-11, 1.4962e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8228]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8228097\n",
            "state: tensor([[ 6., 10.,  1.]]) dist_dsc.probs: tensor([[4.6027e-14, 1.0000e+00, 8.0564e-12, 5.4058e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8276]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8275895\n",
            "state: tensor([[ 6.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.8107e-14, 1.0000e+00, 3.7378e-12, 1.9391e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8314]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8313561\n",
            "state: tensor([[6.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[2.7444e-12, 1.0000e+00, 2.2001e-10, 5.2599e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3393]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.33930072\n",
            "state: tensor([[6.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[3.4810e-12, 1.0000e+00, 2.6819e-10, 6.8222e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3734]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37335494\n",
            "state: tensor([[6.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[4.3911e-12, 1.0000e+00, 3.2545e-10, 8.7941e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4109]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41090024\n",
            "state: tensor([[6.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[5.5392e-12, 1.0000e+00, 3.9495e-10, 1.1336e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4442]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44421154\n",
            "state: tensor([[6.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[6.8790e-12, 1.0000e+00, 4.7325e-10, 1.4357e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4776]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47755978\n",
            "state: tensor([[6.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[7.8280e-12, 1.0000e+00, 5.2777e-10, 1.6503e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5128]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51283973\n",
            "state: tensor([[6.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[8.2483e-12, 1.0000e+00, 5.5236e-10, 1.7436e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5489]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54886127\n",
            "state: tensor([[6.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[8.0507e-12, 1.0000e+00, 5.4322e-10, 1.6913e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5861]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5860512\n",
            "state: tensor([[6.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[7.0380e-12, 1.0000e+00, 4.8831e-10, 1.4493e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6225]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62250566\n",
            "state: tensor([[6.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[5.9933e-12, 1.0000e+00, 4.2971e-10, 1.2056e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6529]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6529031\n",
            "state: tensor([[6.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[5.0886e-12, 1.0000e+00, 3.7757e-10, 9.9772e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6806]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6805819\n",
            "state: tensor([[6.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[4.3204e-12, 1.0000e+00, 3.3175e-10, 8.2568e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7045]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70445436\n",
            "state: tensor([[6.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[3.2635e-12, 1.0000e+00, 2.6450e-10, 6.0197e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7228]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72282165\n",
            "state: tensor([[6.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[2.2923e-12, 1.0000e+00, 1.9849e-10, 4.0555e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7450]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74503493\n",
            "state: tensor([[6.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[1.6187e-12, 1.0000e+00, 1.4956e-10, 2.7498e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7653]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76525235\n",
            "state: tensor([[6.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0802e-12, 1.0000e+00, 1.0755e-10, 1.7523e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7831]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78305936\n",
            "state: tensor([[6.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[6.3246e-13, 1.0000e+00, 6.9390e-11, 9.6832e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7971]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79709154\n",
            "state: tensor([[6.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[3.2162e-13, 1.0000e+00, 3.9835e-11, 4.5913e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8097]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80970293\n",
            "state: tensor([[6.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[1.5160e-13, 1.0000e+00, 2.1462e-11, 2.0064e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8232]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8232343\n",
            "state: tensor([[6.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[6.5233e-14, 1.0000e+00, 1.0722e-11, 7.9448e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8326]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.832628\n",
            "state: tensor([[ 6.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[2.5970e-14, 1.0000e+00, 5.0235e-12, 2.8880e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8406]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8405924\n",
            "state: tensor([[ 6.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.0265e-14, 1.0000e+00, 2.3397e-12, 1.0416e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8451]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.845133\n",
            "state: tensor([[7., 0., 1.]]) dist_dsc.probs: tensor([[6.2305e-13, 1.0000e+00, 6.4597e-11, 1.0401e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3424]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34242922\n",
            "state: tensor([[7.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[7.9276e-13, 1.0000e+00, 7.8947e-11, 1.3537e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3763]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3763364\n",
            "state: tensor([[7., 1., 1.]]) dist_dsc.probs: tensor([[1.0000e-12, 1.0000e+00, 9.5804e-11, 1.7449e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4092]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40916985\n",
            "state: tensor([[7.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2615e-12, 1.0000e+00, 1.1626e-10, 2.2493e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4423]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.44234616\n",
            "state: tensor([[7., 2., 1.]]) dist_dsc.probs: tensor([[1.5479e-12, 1.0000e+00, 1.3793e-10, 2.8113e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4761]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47606334\n",
            "state: tensor([[7.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.7631e-12, 1.0000e+00, 1.5397e-10, 3.2344e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5123]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5122583\n",
            "state: tensor([[7., 3., 1.]]) dist_dsc.probs: tensor([[1.9343e-12, 1.0000e+00, 1.6654e-10, 3.5735e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5466]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54657567\n",
            "state: tensor([[7.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8828e-12, 1.0000e+00, 1.6337e-10, 3.4583e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5838]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5837524\n",
            "state: tensor([[7., 4., 1.]]) dist_dsc.probs: tensor([[1.7164e-12, 1.0000e+00, 1.5195e-10, 3.1070e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6215]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6215197\n",
            "state: tensor([[7.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4585e-12, 1.0000e+00, 1.3344e-10, 2.5795e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6551]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6551329\n",
            "state: tensor([[7., 5., 1.]]) dist_dsc.probs: tensor([[1.2413e-12, 1.0000e+00, 1.1739e-10, 2.1436e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6824309\n",
            "state: tensor([[7.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0539e-12, 1.0000e+00, 1.0315e-10, 1.7740e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7082]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7081709\n",
            "state: tensor([[7., 6., 1.]]) dist_dsc.probs: tensor([[8.9481e-13, 1.0000e+00, 9.0630e-11, 1.4681e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7274]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72736865\n",
            "state: tensor([[7.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[6.7278e-13, 1.0000e+00, 7.1979e-11, 1.0649e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7446]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7446376\n",
            "state: tensor([[7., 7., 1.]]) dist_dsc.probs: tensor([[4.7257e-13, 1.0000e+00, 5.4014e-11, 7.1746e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7650]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7649996\n",
            "state: tensor([[7.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[3.3331e-13, 1.0000e+00, 4.0662e-11, 4.8578e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7840]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78398407\n",
            "state: tensor([[7., 8., 1.]]) dist_dsc.probs: tensor([[2.2013e-13, 1.0000e+00, 2.8993e-11, 3.0601e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8006]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.800593\n",
            "state: tensor([[7.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.3453e-13, 1.0000e+00, 1.9378e-11, 1.7717e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8137]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81366825\n",
            "state: tensor([[7., 9., 1.]]) dist_dsc.probs: tensor([[6.9460e-14, 1.0000e+00, 1.1266e-11, 8.5395e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8258]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8257597\n",
            "state: tensor([[7.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[3.2859e-14, 1.0000e+00, 6.0875e-12, 3.7463e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8383]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8383116\n",
            "state: tensor([[ 7., 10.,  1.]]) dist_dsc.probs: tensor([[1.4139e-14, 1.0000e+00, 3.0412e-12, 1.4834e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8470]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84695005\n",
            "state: tensor([[ 7.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[5.8016e-15, 1.0000e+00, 1.4609e-12, 5.5747e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8553]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8552543\n",
            "state: tensor([[7.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[1.4087e-13, 1.0000e+00, 1.8900e-11, 2.0477e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3455]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34547797\n",
            "state: tensor([[7.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8054e-13, 1.0000e+00, 2.3240e-11, 2.6859e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3786]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3786003\n",
            "state: tensor([[7.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[2.2774e-13, 1.0000e+00, 2.8202e-11, 3.4623e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4077]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40770537\n",
            "state: tensor([[7.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8729e-13, 1.0000e+00, 3.4224e-11, 4.4630e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4405]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4404768\n",
            "state: tensor([[7.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[3.4829e-13, 1.0000e+00, 4.0202e-11, 5.5049e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4743]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47425795\n",
            "state: tensor([[7.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[3.9643e-13, 1.0000e+00, 4.4850e-11, 6.3276e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5114]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5113786\n",
            "state: tensor([[7.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[4.4097e-13, 1.0000e+00, 4.9063e-11, 7.0984e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5441]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.544117\n",
            "state: tensor([[7.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[4.4154e-13, 1.0000e+00, 4.9257e-11, 7.0876e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5816]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58157194\n",
            "state: tensor([[7.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[4.1903e-13, 1.0000e+00, 4.7326e-11, 6.6682e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6196]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61956054\n",
            "state: tensor([[7.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[3.5520e-13, 1.0000e+00, 4.1472e-11, 5.5226e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6547]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6547425\n",
            "state: tensor([[7.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[3.0224e-13, 1.0000e+00, 3.6464e-11, 4.5910e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6852]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6852326\n",
            "state: tensor([[7.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[2.5709e-13, 1.0000e+00, 3.2070e-11, 3.8114e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7099]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7098876\n",
            "state: tensor([[7.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[2.1828e-13, 1.0000e+00, 2.8178e-11, 3.1542e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7338]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73375434\n",
            "state: tensor([[7.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8533e-13, 1.0000e+00, 2.4759e-11, 2.6103e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7480]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7480116\n",
            "state: tensor([[7.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[1.3870e-13, 1.0000e+00, 1.9588e-11, 1.8840e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7645]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76451665\n",
            "state: tensor([[7.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[9.7423e-14, 1.0000e+00, 1.4699e-11, 1.2692e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7835]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7834902\n",
            "state: tensor([[7.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[6.8499e-14, 1.0000e+00, 1.1039e-11, 8.5611e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8011]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80112267\n",
            "state: tensor([[7.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[4.4780e-14, 1.0000e+00, 7.8048e-12, 5.3317e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8167]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81670415\n",
            "state: tensor([[7.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[2.7352e-14, 1.0000e+00, 5.2145e-12, 3.0848e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8289]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.82885164\n",
            "state: tensor([[7.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5001e-14, 1.0000e+00, 3.1859e-12, 1.5883e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8406]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84058094\n",
            "state: tensor([[ 7.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[7.1219e-15, 1.0000e+00, 1.7267e-12, 6.9948e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8522]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.852207\n",
            "state: tensor([[ 7.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[3.0645e-15, 1.0000e+00, 8.6262e-13, 2.7697e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8601]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86014\n",
            "state: tensor([[8., 0., 1.]]) dist_dsc.probs: tensor([[3.1849e-14, 1.0000e+00, 5.5301e-12, 4.0312e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3488]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.34884736\n",
            "state: tensor([[8.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[4.1116e-14, 1.0000e+00, 6.8410e-12, 5.3295e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3792]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3792165\n",
            "state: tensor([[8., 1., 1.]]) dist_dsc.probs: tensor([[5.1866e-14, 1.0000e+00, 8.3018e-12, 6.8699e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4083]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40830517\n",
            "state: tensor([[8.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[6.5426e-14, 1.0000e+00, 1.0074e-11, 8.8555e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4386]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43860373\n",
            "state: tensor([[8., 2., 1.]]) dist_dsc.probs: tensor([[7.8317e-14, 1.0000e+00, 1.1711e-11, 1.0771e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4724]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47235882\n",
            "state: tensor([[8.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[8.8928e-14, 1.0000e+00, 1.3039e-11, 1.2349e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5101]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.51009613\n",
            "state: tensor([[8., 3., 1.]]) dist_dsc.probs: tensor([[9.9388e-14, 1.0000e+00, 1.4321e-11, 1.3923e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5427]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5426942\n",
            "state: tensor([[8.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0354e-13, 1.0000e+00, 1.4851e-11, 1.4526e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5794]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57938117\n",
            "state: tensor([[8., 4., 1.]]) dist_dsc.probs: tensor([[1.0079e-13, 1.0000e+00, 1.4569e-11, 1.4057e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6170]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61696136\n",
            "state: tensor([[8.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[8.6688e-14, 1.0000e+00, 1.2914e-11, 1.1849e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6530]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65299875\n",
            "state: tensor([[8., 5., 1.]]) dist_dsc.probs: tensor([[7.3592e-14, 1.0000e+00, 1.1331e-11, 9.8266e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6853]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68533915\n",
            "state: tensor([[8.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[6.2634e-14, 1.0000e+00, 9.9645e-12, 8.1709e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7122]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71221316\n",
            "state: tensor([[8., 6., 1.]]) dist_dsc.probs: tensor([[5.3246e-14, 1.0000e+00, 8.7611e-12, 6.7769e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7353]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.735344\n",
            "state: tensor([[8.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[4.5208e-14, 1.0000e+00, 7.6980e-12, 5.6084e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7556]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.75558615\n",
            "state: tensor([[8., 7., 1.]]) dist_dsc.probs: tensor([[3.8383e-14, 1.0000e+00, 6.7639e-12, 4.6413e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7670]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7669612\n",
            "state: tensor([[8.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[2.8594e-14, 1.0000e+00, 5.3304e-12, 3.3329e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7830]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7830406\n",
            "state: tensor([[8., 8., 1.]]) dist_dsc.probs: tensor([[2.0084e-14, 1.0000e+00, 4.0000e-12, 2.2454e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8007]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80069023\n",
            "state: tensor([[8.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4066e-14, 1.0000e+00, 2.9949e-12, 1.5072e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8170]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8170019\n",
            "state: tensor([[8., 9., 1.]]) dist_dsc.probs: tensor([[9.1046e-15, 1.0000e+00, 2.1002e-12, 9.2834e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8313]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83129543\n",
            "state: tensor([[8.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[5.5611e-15, 1.0000e+00, 1.4032e-12, 5.3711e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8429]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8428569\n",
            "state: tensor([[ 8., 10.,  1.]]) dist_dsc.probs: tensor([[3.2215e-15, 1.0000e+00, 8.9679e-13, 2.9357e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8542]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8542239\n",
            "state: tensor([[ 8.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[1.5408e-15, 1.0000e+00, 4.8901e-13, 1.3034e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8650]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86496985\n",
            "state: tensor([[8.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[7.2007e-15, 1.0000e+00, 1.6181e-12, 7.9361e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3515]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35149363\n",
            "state: tensor([[8.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[9.3193e-15, 1.0000e+00, 2.0058e-12, 1.0521e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3798]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37975726\n",
            "state: tensor([[8.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1812e-14, 1.0000e+00, 2.4438e-12, 1.3631e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4089]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4089047\n",
            "state: tensor([[8.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[1.4803e-14, 1.0000e+00, 2.9497e-12, 1.7445e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4374]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.437417\n",
            "state: tensor([[8.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[1.7568e-14, 1.0000e+00, 3.4044e-12, 2.1020e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4706]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47057983\n",
            "state: tensor([[8.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.9949e-14, 1.0000e+00, 3.7905e-12, 2.4099e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5088]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50881153\n",
            "state: tensor([[8.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[2.2386e-14, 1.0000e+00, 4.1778e-12, 2.7288e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5421]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54213774\n",
            "state: tensor([[8.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[2.4282e-14, 1.0000e+00, 4.4774e-12, 2.9770e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5771]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5770748\n",
            "state: tensor([[8.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[2.3636e-14, 1.0000e+00, 4.3924e-12, 2.8809e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61443204\n",
            "state: tensor([[8.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1183e-14, 1.0000e+00, 4.0256e-12, 2.5451e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6511]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6510512\n",
            "state: tensor([[8.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[1.7940e-14, 1.0000e+00, 3.5247e-12, 2.1062e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6840]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6839883\n",
            "state: tensor([[8.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[1.5247e-14, 1.0000e+00, 3.0957e-12, 1.7485e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7137]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7136934\n",
            "state: tensor([[8.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[1.2980e-14, 1.0000e+00, 2.7230e-12, 1.4543e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7372]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7372417\n",
            "state: tensor([[8.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[1.1028e-14, 1.0000e+00, 2.3934e-12, 1.2050e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7589]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7588771\n",
            "state: tensor([[8.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[9.3632e-15, 1.0000e+00, 2.1030e-12, 9.9719e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7742]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77424866\n",
            "state: tensor([[8.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[7.9496e-15, 1.0000e+00, 1.8478e-12, 8.2525e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7847]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78466135\n",
            "state: tensor([[8.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[5.8948e-15, 1.0000e+00, 1.4505e-12, 5.8963e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8003]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8002724\n",
            "state: tensor([[8.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[4.1405e-15, 1.0000e+00, 1.0885e-12, 3.9723e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8167]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8166643\n",
            "state: tensor([[8.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[2.8837e-15, 1.0000e+00, 8.1151e-13, 2.6478e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8316]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83164626\n",
            "state: tensor([[8.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.8511e-15, 1.0000e+00, 5.6516e-13, 1.6164e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8448]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84482443\n",
            "state: tensor([[ 8.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[1.1307e-15, 1.0000e+00, 3.7759e-13, 9.3520e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8555]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85554194\n",
            "state: tensor([[ 8.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[6.6086e-16, 1.0000e+00, 2.4312e-13, 5.1603e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8665]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8665428\n",
            "state: tensor([[9., 0., 1.]]) dist_dsc.probs: tensor([[1.6280e-15, 1.0000e+00, 4.7343e-13, 1.5624e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3528]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35277507\n",
            "state: tensor([[9.0000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[2.1091e-15, 1.0000e+00, 5.8735e-13, 2.0735e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3803]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38027388\n",
            "state: tensor([[9., 1., 1.]]) dist_dsc.probs: tensor([[2.6900e-15, 1.0000e+00, 7.1938e-13, 2.7047e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4095]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40950397\n",
            "state: tensor([[9.0000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[3.3263e-15, 1.0000e+00, 8.5878e-13, 3.4104e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4385]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43850574\n",
            "state: tensor([[9., 2., 1.]]) dist_dsc.probs: tensor([[3.9354e-15, 1.0000e+00, 9.8862e-13, 4.0955e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4692]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46923172\n",
            "state: tensor([[9.0000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[4.4749e-15, 1.0000e+00, 1.1020e-12, 4.7030e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5075]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5075247\n",
            "state: tensor([[9., 3., 1.]]) dist_dsc.probs: tensor([[5.0420e-15, 1.0000e+00, 1.2188e-12, 5.3481e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5416]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5415807\n",
            "state: tensor([[9.0000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[5.5888e-15, 1.0000e+00, 1.3295e-12, 5.9764e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5747]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5747438\n",
            "state: tensor([[9., 4., 1.]]) dist_dsc.probs: tensor([[5.5427e-15, 1.0000e+00, 1.3243e-12, 5.9044e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6118]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6118309\n",
            "state: tensor([[9.0000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[5.2073e-15, 1.0000e+00, 1.2617e-12, 5.4946e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6484]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.64836234\n",
            "state: tensor([[9., 5., 1.]]) dist_dsc.probs: tensor([[4.3782e-15, 1.0000e+00, 1.0975e-12, 4.5190e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68237245\n",
            "state: tensor([[9.0000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[3.7169e-15, 1.0000e+00, 9.6301e-13, 3.7477e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7124]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71244353\n",
            "state: tensor([[9., 6., 1.]]) dist_dsc.probs: tensor([[3.1590e-15, 1.0000e+00, 8.4580e-13, 3.1112e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7396]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7395686\n",
            "state: tensor([[9.0000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[2.6898e-15, 1.0000e+00, 7.4410e-13, 2.5883e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7604]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7603982\n",
            "state: tensor([[9., 7., 1.]]) dist_dsc.probs: tensor([[2.2840e-15, 1.0000e+00, 6.5385e-13, 2.1425e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7796]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7795888\n",
            "state: tensor([[9.0000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[1.9392e-15, 1.0000e+00, 5.7451e-13, 1.7730e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7917]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7916551\n",
            "state: tensor([[9., 8., 1.]]) dist_dsc.probs: tensor([[1.6465e-15, 1.0000e+00, 5.0480e-13, 1.4673e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8012]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8011683\n",
            "state: tensor([[9.0000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2153e-15, 1.0000e+00, 3.9473e-13, 1.0431e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8163]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81627643\n",
            "state: tensor([[9., 9., 1.]]) dist_dsc.probs: tensor([[8.5202e-16, 1.0000e+00, 2.9579e-13, 7.0106e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8314]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8313802\n",
            "state: tensor([[9.0000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[5.9151e-16, 1.0000e+00, 2.1998e-13, 4.6549e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8452]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8452229\n",
            "state: tensor([[ 9., 10.,  1.]]) dist_dsc.probs: tensor([[3.7637e-16, 1.0000e+00, 1.5208e-13, 2.8144e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8574]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.857353\n",
            "state: tensor([[ 9.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[2.2989e-16, 1.0000e+00, 1.0161e-13, 1.6284e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8673]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86727667\n",
            "state: tensor([[9.5000, 0.0000, 1.0000]]) dist_dsc.probs: tensor([[3.6808e-16, 1.0000e+00, 1.3852e-13, 3.0758e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3541]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.35405496\n",
            "state: tensor([[9.5000, 0.5000, 1.0000]]) dist_dsc.probs: tensor([[4.7733e-16, 1.0000e+00, 1.7200e-13, 4.0866e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3808]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38079053\n",
            "state: tensor([[9.5000, 1.0000, 1.0000]]) dist_dsc.probs: tensor([[6.1193e-16, 1.0000e+00, 2.1156e-13, 5.3601e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4101]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41008514\n",
            "state: tensor([[9.5000, 1.5000, 1.0000]]) dist_dsc.probs: tensor([[7.4742e-16, 1.0000e+00, 2.5003e-13, 6.6671e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4396]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43959317\n",
            "state: tensor([[9.5000, 2.0000, 1.0000]]) dist_dsc.probs: tensor([[8.8158e-16, 1.0000e+00, 2.8709e-13, 7.9795e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4695]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4694983\n",
            "state: tensor([[9.5000, 2.5000, 1.0000]]) dist_dsc.probs: tensor([[1.0038e-15, 1.0000e+00, 3.2036e-13, 9.1782e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5059]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50587934\n",
            "state: tensor([[9.5000, 3.0000, 1.0000]]) dist_dsc.probs: tensor([[1.1356e-15, 1.0000e+00, 3.5556e-13, 1.0482e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5410]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54102314\n",
            "state: tensor([[9.5000, 3.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2607e-15, 1.0000e+00, 3.8830e-13, 1.1734e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5724]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5723871\n",
            "state: tensor([[9.5000, 4.0000, 1.0000]]) dist_dsc.probs: tensor([[1.2998e-15, 1.0000e+00, 3.9927e-13, 1.2101e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6091]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6091398\n",
            "state: tensor([[9.5000, 4.5000, 1.0000]]) dist_dsc.probs: tensor([[1.2652e-15, 1.0000e+00, 3.9169e-13, 1.1710e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6457]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6457409\n",
            "state: tensor([[9.5000, 5.0000, 1.0000]]) dist_dsc.probs: tensor([[1.0718e-15, 1.0000e+00, 3.4270e-13, 9.7217e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6803]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6803482\n",
            "state: tensor([[9.5000, 5.5000, 1.0000]]) dist_dsc.probs: tensor([[9.0611e-16, 1.0000e+00, 2.9957e-13, 8.0328e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7112]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.711189\n",
            "state: tensor([[9.5000, 6.0000, 1.0000]]) dist_dsc.probs: tensor([[7.7009e-16, 1.0000e+00, 2.6311e-13, 6.6685e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7387]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73873407\n",
            "state: tensor([[9.5000, 6.5000, 1.0000]]) dist_dsc.probs: tensor([[6.5449e-16, 1.0000e+00, 2.3109e-13, 5.5359e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7627]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7627341\n",
            "state: tensor([[9.5000, 7.0000, 1.0000]]) dist_dsc.probs: tensor([[5.5716e-16, 1.0000e+00, 2.0329e-13, 4.6031e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7813]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7812726\n",
            "state: tensor([[9.5000, 7.5000, 1.0000]]) dist_dsc.probs: tensor([[4.7305e-16, 1.0000e+00, 1.7862e-13, 3.8094e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7975]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7974856\n",
            "state: tensor([[9.5000, 8.0000, 1.0000]]) dist_dsc.probs: tensor([[4.0164e-16, 1.0000e+00, 1.5695e-13, 3.1526e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8079]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80786496\n",
            "state: tensor([[9.5000, 8.5000, 1.0000]]) dist_dsc.probs: tensor([[3.4092e-16, 1.0000e+00, 1.3787e-13, 2.6082e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8171]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81708986\n",
            "state: tensor([[9.5000, 9.0000, 1.0000]]) dist_dsc.probs: tensor([[2.5035e-16, 1.0000e+00, 1.0736e-13, 1.8436e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8311]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8310794\n",
            "state: tensor([[9.5000, 9.5000, 1.0000]]) dist_dsc.probs: tensor([[1.7525e-16, 1.0000e+00, 8.0347e-14, 1.2366e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8450]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8449941\n",
            "state: tensor([[ 9.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[1.2141e-16, 1.0000e+00, 5.9660e-14, 8.1894e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8577944\n",
            "state: tensor([[ 9.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[7.6522e-17, 1.0000e+00, 4.0923e-14, 4.9004e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8689]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86894184\n",
            "state: tensor([[10.,  0.,  1.]]) dist_dsc.probs: tensor([[8.3220e-17, 1.0000e+00, 4.0531e-14, 6.0552e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3553]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3553338\n",
            "state: tensor([[10.0000,  0.5000,  1.0000]]) dist_dsc.probs: tensor([[1.0803e-16, 1.0000e+00, 5.0366e-14, 8.0543e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3813]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3813068\n",
            "state: tensor([[10.,  1.,  1.]]) dist_dsc.probs: tensor([[1.3849e-16, 1.0000e+00, 6.1952e-14, 1.0564e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4106]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4105874\n",
            "state: tensor([[10.0000,  1.5000,  1.0000]]) dist_dsc.probs: tensor([[1.6795e-16, 1.0000e+00, 7.2792e-14, 1.3034e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4407]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4406794\n",
            "state: tensor([[10.,  2.,  1.]]) dist_dsc.probs: tensor([[1.9748e-16, 1.0000e+00, 8.3371e-14, 1.5547e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4707]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47069126\n",
            "state: tensor([[10.0000,  2.5000,  1.0000]]) dist_dsc.probs: tensor([[2.2518e-16, 1.0000e+00, 9.3133e-14, 1.7912e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5051]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5051178\n",
            "state: tensor([[10.,  3.,  1.]]) dist_dsc.probs: tensor([[2.5569e-16, 1.0000e+00, 1.0369e-13, 2.0535e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5404]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5404043\n",
            "state: tensor([[10.0000,  3.5000,  1.0000]]) dist_dsc.probs: tensor([[2.8422e-16, 1.0000e+00, 1.1336e-13, 2.3022e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5707]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57070625\n",
            "state: tensor([[10.,  4.,  1.]]) dist_dsc.probs: tensor([[3.0482e-16, 1.0000e+00, 1.2038e-13, 2.4800e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6064]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6064346\n",
            "state: tensor([[10.0000,  4.5000,  1.0000]]) dist_dsc.probs: tensor([[2.9671e-16, 1.0000e+00, 1.1809e-13, 2.4000e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6433]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.64328116\n",
            "state: tensor([[10.,  5.,  1.]]) dist_dsc.probs: tensor([[2.6348e-16, 1.0000e+00, 1.0741e-13, 2.0988e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6778]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.677841\n",
            "state: tensor([[10.0000,  5.5000,  1.0000]]) dist_dsc.probs: tensor([[2.2112e-16, 1.0000e+00, 9.3273e-14, 1.7234e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7097]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70969635\n",
            "state: tensor([[10.,  6.,  1.]]) dist_dsc.probs: tensor([[1.8773e-16, 1.0000e+00, 8.1846e-14, 1.4293e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7376]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7375767\n",
            "state: tensor([[10.0000,  6.5000,  1.0000]]) dist_dsc.probs: tensor([[1.5955e-16, 1.0000e+00, 7.1885e-14, 1.1866e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7630]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7629536\n",
            "state: tensor([[10.,  7.,  1.]]) dist_dsc.probs: tensor([[1.3560e-16, 1.0000e+00, 6.3136e-14, 9.8502e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7835]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78348327\n",
            "state: tensor([[10.0000,  7.5000,  1.0000]]) dist_dsc.probs: tensor([[1.1540e-16, 1.0000e+00, 5.5537e-14, 8.1846e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8003]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8002791\n",
            "state: tensor([[10.,  8.,  1.]]) dist_dsc.probs: tensor([[9.7975e-17, 1.0000e+00, 4.8798e-14, 6.7733e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8134]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81341344\n",
            "state: tensor([[10.0000,  8.5000,  1.0000]]) dist_dsc.probs: tensor([[8.3184e-17, 1.0000e+00, 4.2877e-14, 5.6054e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8234]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.82336646\n",
            "state: tensor([[10.,  9.,  1.]]) dist_dsc.probs: tensor([[7.0585e-17, 1.0000e+00, 3.7653e-14, 4.6356e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8323]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8323278\n",
            "state: tensor([[10.0000,  9.5000,  1.0000]]) dist_dsc.probs: tensor([[5.1492e-17, 1.0000e+00, 2.9162e-14, 3.2518e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8450]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8450491\n",
            "state: tensor([[10., 10.,  1.]]) dist_dsc.probs: tensor([[3.6046e-17, 1.0000e+00, 2.1826e-14, 2.1812e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8577577\n",
            "state: tensor([[10.0000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[2.4919e-17, 1.0000e+00, 1.6180e-14, 1.4408e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8695]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.869547\n",
            "state: tensor([[10.5000,  0.0000,  1.0000]]) dist_dsc.probs: tensor([[1.8815e-17, 1.0000e+00, 1.1859e-14, 1.1921e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3566]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3566111\n",
            "state: tensor([[10.5000,  0.5000,  1.0000]]) dist_dsc.probs: tensor([[2.4449e-17, 1.0000e+00, 1.4749e-14, 1.5874e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3818]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38182276\n",
            "state: tensor([[10.5000,  1.0000,  1.0000]]) dist_dsc.probs: tensor([[3.1248e-17, 1.0000e+00, 1.8097e-14, 2.0751e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4112]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41120616\n",
            "state: tensor([[10.5000,  1.5000,  1.0000]]) dist_dsc.probs: tensor([[3.7738e-17, 1.0000e+00, 2.1193e-14, 2.5481e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4418]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4417645\n",
            "state: tensor([[10.5000,  2.0000,  1.0000]]) dist_dsc.probs: tensor([[4.4238e-17, 1.0000e+00, 2.4211e-14, 3.0291e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4719]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47188243\n",
            "state: tensor([[10.5000,  2.5000,  1.0000]]) dist_dsc.probs: tensor([[5.0513e-17, 1.0000e+00, 2.7075e-14, 3.4956e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5057]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5057404\n",
            "state: tensor([[10.5000,  3.0000,  1.0000]]) dist_dsc.probs: tensor([[5.7357e-17, 1.0000e+00, 3.0146e-14, 4.0075e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5396948\n",
            "state: tensor([[10.5000,  3.5000,  1.0000]]) dist_dsc.probs: tensor([[6.4016e-17, 1.0000e+00, 3.3072e-14, 4.5120e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5702]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5701743\n",
            "state: tensor([[10.5000,  4.0000,  1.0000]]) dist_dsc.probs: tensor([[7.0833e-17, 1.0000e+00, 3.6025e-14, 5.0317e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6038]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6038409\n",
            "state: tensor([[10.5000,  4.5000,  1.0000]]) dist_dsc.probs: tensor([[6.9579e-17, 1.0000e+00, 3.5605e-14, 4.9187e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6408]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6407597\n",
            "state: tensor([[10.5000,  5.0000,  1.0000]]) dist_dsc.probs: tensor([[6.4769e-17, 1.0000e+00, 3.3666e-14, 4.5310e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6753]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67531806\n",
            "state: tensor([[10.5000,  5.5000,  1.0000]]) dist_dsc.probs: tensor([[5.4234e-17, 1.0000e+00, 2.9174e-14, 3.7134e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7076]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70762104\n",
            "state: tensor([[10.5000,  6.0000,  1.0000]]) dist_dsc.probs: tensor([[4.5765e-17, 1.0000e+00, 2.5460e-14, 3.0636e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7364]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7364151\n",
            "state: tensor([[10.5000,  6.5000,  1.0000]]) dist_dsc.probs: tensor([[3.8895e-17, 1.0000e+00, 2.2362e-14, 2.5433e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7619]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7618889\n",
            "state: tensor([[10.5000,  7.0000,  1.0000]]) dist_dsc.probs: tensor([[3.3057e-17, 1.0000e+00, 1.9640e-14, 2.1113e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7844]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78437895\n",
            "state: tensor([[10.5000,  7.5000,  1.0000]]) dist_dsc.probs: tensor([[2.8098e-17, 1.0000e+00, 1.7251e-14, 1.7529e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8023]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8022677\n",
            "state: tensor([[10.5000,  8.0000,  1.0000]]) dist_dsc.probs: tensor([[2.3900e-17, 1.0000e+00, 1.5172e-14, 1.4553e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8182]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81818706\n",
            "state: tensor([[10.5000,  8.5000,  1.0000]]) dist_dsc.probs: tensor([[2.0292e-17, 1.0000e+00, 1.3331e-14, 1.2043e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8286]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8286433\n",
            "state: tensor([[10.5000,  9.0000,  1.0000]]) dist_dsc.probs: tensor([[1.7229e-17, 1.0000e+00, 1.1713e-14, 9.9666e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8379]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8378566\n",
            "state: tensor([[10.5000,  9.5000,  1.0000]]) dist_dsc.probs: tensor([[1.4586e-17, 1.0000e+00, 1.0268e-14, 8.2187e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8464]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8463674\n",
            "state: tensor([[10.5000, 10.0000,  1.0000]]) dist_dsc.probs: tensor([[1.0591e-17, 1.0000e+00, 7.9217e-15, 5.7358e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8581]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85809195\n",
            "state: tensor([[10.5000, 10.5000,  1.0000]]) dist_dsc.probs: tensor([[7.3997e-18, 1.0000e+00, 5.9200e-15, 3.8381e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8698]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86981404\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[8.4006e-07, 9.9994e-01, 7.6286e-06, 5.0629e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4505]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4505089\n",
            "state: tensor([[0.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.7333e-06, 9.9987e-01, 1.3941e-05, 1.1162e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4307]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3839]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43074596\n",
            "state: tensor([[0.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[3.2634e-06, 9.9975e-01, 2.3639e-05, 2.2211e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4116]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1671]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41164172\n",
            "state: tensor([[0.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[4.2586e-06, 9.9967e-01, 2.9604e-05, 2.9562e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3864]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4916]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3864388\n",
            "state: tensor([[0.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[3.4170e-06, 9.9974e-01, 2.4755e-05, 2.3146e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3862]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3013]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38620198\n",
            "state: tensor([[0.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.8144e-06, 9.9987e-01, 1.4689e-05, 1.1577e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4099]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3539]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40985557\n",
            "state: tensor([[0.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[8.7898e-07, 9.9994e-01, 8.0768e-06, 5.2378e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4353]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5701]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4353394\n",
            "state: tensor([[0.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[3.9715e-07, 9.9997e-01, 4.1970e-06, 2.1913e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4614]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8475]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4613972\n",
            "state: tensor([[0.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.6269e-07, 9.9999e-01, 2.0109e-06, 8.2428e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4827]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.2521]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48265988\n",
            "state: tensor([[0.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[6.4188e-08, 1.0000e+00, 9.3412e-07, 2.9747e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5033]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7296]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50326794\n",
            "state: tensor([[0.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[2.4006e-08, 1.0000e+00, 4.1527e-07, 1.0118e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5226]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3451]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52264947\n",
            "state: tensor([[0.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[8.9671e-09, 1.0000e+00, 1.8441e-07, 3.4372e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5414]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5413825\n",
            "state: tensor([[0.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[3.3068e-09, 1.0000e+00, 8.1003e-08, 1.1521e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5628]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5628078\n",
            "state: tensor([[0.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2186e-09, 1.0000e+00, 3.5562e-08, 3.8584e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5844]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.58442366\n",
            "state: tensor([[0.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[4.4908e-10, 1.0000e+00, 1.5612e-08, 1.2922e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6009]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.600936\n",
            "state: tensor([[0.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6549e-10, 1.0000e+00, 6.8540e-09, 4.3280e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6162]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61617225\n",
            "state: tensor([[0.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[5.8983e-11, 1.0000e+00, 2.9282e-09, 1.3962e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6316]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6315712\n",
            "state: tensor([[0.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0692e-11, 1.0000e+00, 1.2350e-09, 4.4244e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6466]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6465643\n",
            "state: tensor([[0.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[7.2401e-12, 1.0000e+00, 5.1983e-10, 1.3974e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6605]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6605009\n",
            "state: tensor([[0.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5333e-12, 1.0000e+00, 2.1880e-10, 4.4133e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6739967\n",
            "state: tensor([[ 0.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[8.7129e-13, 1.0000e+00, 9.0804e-11, 1.3672e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6870]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6869516\n",
            "state: tensor([[ 0.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[2.9906e-13, 1.0000e+00, 3.7622e-11, 4.2263e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7003]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70034\n",
            "state: tensor([[0.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[5.1577e-07, 9.9996e-01, 5.0930e-06, 2.9785e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4222]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42223936\n",
            "state: tensor([[0.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0417e-06, 9.9993e-01, 9.1475e-06, 6.4118e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4026]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40256023\n",
            "state: tensor([[0.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9360e-06, 9.9986e-01, 1.5351e-05, 1.2576e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3867]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4348]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38665128\n",
            "state: tensor([[0.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[2.7597e-06, 9.9979e-01, 2.0675e-05, 1.8416e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3882]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6442]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38817638\n",
            "state: tensor([[0.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[2.6170e-06, 9.9980e-01, 1.9845e-05, 1.7322e-04]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4132]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.2552]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4131606\n",
            "state: tensor([[0.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5488e-06, 9.9989e-01, 1.2890e-05, 9.7527e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4361]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.1529]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43607274\n",
            "state: tensor([[0.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[7.4597e-07, 9.9995e-01, 7.0585e-06, 4.3793e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4586]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.3337]], grad_fn=<ExpBackward0>)\n",
            "1 -0.45862985\n",
            "state: tensor([[0.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[3.3745e-07, 9.9998e-01, 3.6697e-06, 1.8353e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4817]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.5893]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48169622\n",
            "state: tensor([[0.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4464e-07, 9.9999e-01, 1.8258e-06, 7.2470e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5038]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9142]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50376076\n",
            "state: tensor([[0.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[5.6743e-08, 1.0000e+00, 8.4410e-07, 2.5994e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5244]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3553]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52438134\n",
            "state: tensor([[0.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[2.1673e-08, 1.0000e+00, 3.8174e-07, 9.0541e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5457]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9103]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5456671\n",
            "state: tensor([[0.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[8.1102e-09, 1.0000e+00, 1.6976e-07, 3.0824e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5668]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5668342\n",
            "state: tensor([[0.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[3.0297e-09, 1.0000e+00, 7.5388e-08, 1.0474e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5853]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5853346\n",
            "state: tensor([[0.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1272e-09, 1.0000e+00, 3.3361e-08, 3.5441e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6029]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6028972\n",
            "state: tensor([[0.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[4.1703e-10, 1.0000e+00, 1.4697e-08, 1.1918e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6198]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61976224\n",
            "state: tensor([[0.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4781e-10, 1.0000e+00, 6.2520e-09, 3.8197e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6348]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6347589\n",
            "state: tensor([[0.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[5.1742e-11, 1.0000e+00, 2.6323e-09, 1.2075e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6494]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.649437\n",
            "state: tensor([[0.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.7754e-11, 1.0000e+00, 1.0902e-09, 3.7331e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6636]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66358376\n",
            "state: tensor([[0.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[6.0728e-12, 1.0000e+00, 4.5043e-10, 1.1496e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6766]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67660755\n",
            "state: tensor([[0.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0766e-12, 1.0000e+00, 1.8605e-10, 3.5385e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6894]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6893733\n",
            "state: tensor([[ 0.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[7.1011e-13, 1.0000e+00, 7.6851e-11, 1.0892e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7023]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70233184\n",
            "state: tensor([[ 0.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[2.4283e-13, 1.0000e+00, 3.1744e-11, 3.3528e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7136]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71355104\n",
            "state: tensor([[1.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[2.7466e-07, 9.9998e-01, 3.0194e-06, 1.5032e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4160]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41597852\n",
            "state: tensor([[1.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[6.0709e-07, 9.9996e-01, 5.8525e-06, 3.5585e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3886]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38858277\n",
            "state: tensor([[1.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0311e-06, 9.9993e-01, 9.1142e-06, 6.3210e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3842]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7354]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3842316\n",
            "state: tensor([[1.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4458e-06, 9.9990e-01, 1.2105e-05, 9.1032e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3931]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8645]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39308026\n",
            "state: tensor([[1.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2936e-06, 9.9991e-01, 1.1067e-05, 8.0407e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4343]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6192]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43425915\n",
            "state: tensor([[1.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[9.2044e-07, 9.9994e-01, 8.3818e-06, 5.5214e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4665]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4505]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46650046\n",
            "state: tensor([[1.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[4.9547e-07, 9.9997e-01, 5.0357e-06, 2.7987e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4894]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4597]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4894445\n",
            "state: tensor([[1.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5649e-07, 9.9998e-01, 2.9270e-06, 1.3609e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5043]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.4324]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5042931\n",
            "state: tensor([[1.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1816e-07, 9.9999e-01, 1.5447e-06, 5.8212e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5253]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6016]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5253296\n",
            "state: tensor([[1.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[4.8227e-08, 1.0000e+00, 7.3803e-07, 2.1786e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5479]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9928]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5478533\n",
            "state: tensor([[1.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.8373e-08, 1.0000e+00, 3.3308e-07, 7.5665e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5682]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5335]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5681511\n",
            "state: tensor([[1.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[6.9997e-09, 1.0000e+00, 1.5032e-07, 2.6279e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5877]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1192]], grad_fn=<ExpBackward0>)\n",
            "1 -0.587728\n",
            "state: tensor([[1.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[2.6101e-09, 1.0000e+00, 6.6657e-08, 8.9111e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6058]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.60582745\n",
            "state: tensor([[1.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[9.3469e-10, 1.0000e+00, 2.8597e-08, 2.8879e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6227]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62270015\n",
            "state: tensor([[1.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[3.2566e-10, 1.0000e+00, 1.1997e-08, 9.0758e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6389]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63893974\n",
            "state: tensor([[1.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1347e-10, 1.0000e+00, 5.0330e-09, 2.8523e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6554]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65540355\n",
            "state: tensor([[1.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[3.9533e-11, 1.0000e+00, 2.1114e-09, 8.9639e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6700]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6700343\n",
            "state: tensor([[1.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.3774e-11, 1.0000e+00, 8.8579e-10, 2.8171e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6841]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68411124\n",
            "state: tensor([[1.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[4.7809e-12, 1.0000e+00, 3.7043e-10, 8.8161e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6960]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69601893\n",
            "state: tensor([[1.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6566e-12, 1.0000e+00, 1.5470e-10, 2.7532e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7078]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7078479\n",
            "state: tensor([[ 1.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7402e-13, 1.0000e+00, 6.4606e-11, 8.5982e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7185]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71853286\n",
            "state: tensor([[ 1.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.9667e-13, 1.0000e+00, 2.6730e-11, 2.6523e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7285]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7284882\n",
            "state: tensor([[1.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2845e-07, 9.9999e-01, 1.6087e-06, 6.5754e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4153]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4153341\n",
            "state: tensor([[1.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[2.7773e-07, 9.9998e-01, 3.0614e-06, 1.5191e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3916]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3915571\n",
            "state: tensor([[1.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[4.9665e-07, 9.9997e-01, 4.9780e-06, 2.8489e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3847]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0315]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38466948\n",
            "state: tensor([[1.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[5.7963e-07, 9.9996e-01, 5.6749e-06, 3.3651e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4079]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3171]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40789482\n",
            "state: tensor([[1.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[4.9778e-07, 9.9997e-01, 5.0164e-06, 2.8427e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4410]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0753]], grad_fn=<ExpBackward0>)\n",
            "1 -0.440981\n",
            "state: tensor([[1.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[3.9591e-07, 9.9997e-01, 4.1651e-06, 2.2035e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4784]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.8578]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47841474\n",
            "state: tensor([[1.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[2.7535e-07, 9.9998e-01, 3.0959e-06, 1.4750e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5114]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7143]], grad_fn=<ExpBackward0>)\n",
            "1 -0.511407\n",
            "state: tensor([[1.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4885e-07, 9.9999e-01, 1.8662e-06, 7.5142e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5361]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6729]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5360841\n",
            "state: tensor([[1.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[7.6589e-08, 1.0000e+00, 1.0794e-06, 3.6290e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5542]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.6376]], grad_fn=<ExpBackward0>)\n",
            "1 -0.55415803\n",
            "state: tensor([[1.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[3.6025e-08, 1.0000e+00, 5.7988e-07, 1.5865e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5720]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.7815]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57195145\n",
            "state: tensor([[1.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5034e-08, 1.0000e+00, 2.8227e-07, 6.0802e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5906]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1650]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59059477\n",
            "state: tensor([[1.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[5.6338e-09, 1.0000e+00, 1.2569e-07, 2.0726e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6093]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7573]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6093084\n",
            "state: tensor([[1.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[2.0140e-09, 1.0000e+00, 5.3857e-08, 6.7020e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6271]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6271253\n",
            "state: tensor([[1.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[7.0375e-10, 1.0000e+00, 2.2648e-08, 2.1130e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6434]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6433689\n",
            "state: tensor([[1.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[2.4520e-10, 1.0000e+00, 9.5012e-09, 6.6406e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6590]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6589667\n",
            "state: tensor([[1.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[8.5430e-11, 1.0000e+00, 3.9860e-09, 2.0869e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6737]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67372364\n",
            "state: tensor([[1.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[2.9765e-11, 1.0000e+00, 1.6722e-09, 6.5587e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6879]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68787086\n",
            "state: tensor([[1.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0371e-11, 1.0000e+00, 7.0152e-10, 2.0612e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7011]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70106804\n",
            "state: tensor([[1.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[3.6101e-12, 1.0000e+00, 2.9410e-10, 6.4704e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7144]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71437746\n",
            "state: tensor([[1.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2535e-12, 1.0000e+00, 1.2305e-10, 2.0245e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7262]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7262114\n",
            "state: tensor([[ 1.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[4.3526e-13, 1.0000e+00, 5.1485e-11, 6.3345e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7364]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7363988\n",
            "state: tensor([[ 1.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.5114e-13, 1.0000e+00, 2.1542e-11, 1.9820e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7463]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74626297\n",
            "state: tensor([[2.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[5.5110e-08, 1.0000e+00, 7.9870e-07, 2.6135e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4118]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41177538\n",
            "state: tensor([[2.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0681e-07, 9.9999e-01, 1.3865e-06, 5.3667e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4022]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4021592\n",
            "state: tensor([[2.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.7733e-07, 9.9999e-01, 2.1188e-06, 9.2937e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3920]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39199662\n",
            "state: tensor([[2.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[2.1699e-07, 9.9999e-01, 2.5120e-06, 1.1562e-05]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4137]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7861]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41367072\n",
            "state: tensor([[2.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9028e-07, 9.9999e-01, 2.2600e-06, 9.9842e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4442]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4895]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4442073\n",
            "state: tensor([[2.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5689e-07, 9.9999e-01, 1.9329e-06, 8.0548e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4803]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.2875]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48026958\n",
            "state: tensor([[2.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1244e-07, 9.9999e-01, 1.4734e-06, 5.5645e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5155]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1550]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5155308\n",
            "state: tensor([[2.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[7.6806e-08, 1.0000e+00, 1.0792e-06, 3.6497e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5537]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.0114]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5537339\n",
            "state: tensor([[2.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[4.1277e-08, 1.0000e+00, 6.4751e-07, 1.8465e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5771]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[5.9977]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57707703\n",
            "state: tensor([[2.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.9684e-08, 1.0000e+00, 3.5180e-07, 8.1975e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5962]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.1040]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5961523\n",
            "state: tensor([[2.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[8.9245e-09, 1.0000e+00, 1.8343e-07, 3.4385e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6119]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.3452]], grad_fn=<ExpBackward0>)\n",
            "1 -0.61194384\n",
            "state: tensor([[2.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[3.7852e-09, 1.0000e+00, 9.0554e-08, 1.3398e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6275]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.6892]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6275084\n",
            "state: tensor([[2.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4442e-09, 1.0000e+00, 4.0953e-08, 4.6532e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6421]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2317]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6421411\n",
            "state: tensor([[2.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[5.1563e-10, 1.0000e+00, 1.7530e-08, 1.5024e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6587]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.65871304\n",
            "state: tensor([[2.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[1.8115e-10, 1.0000e+00, 7.4050e-09, 4.7643e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67403626\n",
            "state: tensor([[2.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[6.3287e-11, 1.0000e+00, 3.1138e-09, 1.5015e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6886]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.688617\n",
            "state: tensor([[2.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2111e-11, 1.0000e+00, 1.3093e-09, 4.7323e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7032]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70316565\n",
            "state: tensor([[2.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[7.7249e-12, 1.0000e+00, 5.5056e-10, 1.4915e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7170]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71704984\n",
            "state: tensor([[2.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[2.6989e-12, 1.0000e+00, 2.3151e-10, 4.7006e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7301]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73014563\n",
            "state: tensor([[2.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[9.3991e-13, 1.0000e+00, 9.7106e-11, 1.4754e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7405]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74047875\n",
            "state: tensor([[ 2.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[3.2726e-13, 1.0000e+00, 4.0724e-11, 4.6295e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7504]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7504039\n",
            "state: tensor([[ 2.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.1392e-13, 1.0000e+00, 1.7075e-11, 1.4523e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7598]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7598441\n",
            "state: tensor([[2.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.7632e-08, 1.0000e+00, 3.1117e-07, 7.5370e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4032]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.403224\n",
            "state: tensor([[2.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[3.5427e-08, 1.0000e+00, 5.5554e-07, 1.6154e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3988]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3987944\n",
            "state: tensor([[2.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[5.7202e-08, 1.0000e+00, 8.2850e-07, 2.7196e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4019]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4019337\n",
            "state: tensor([[2.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[6.8909e-08, 1.0000e+00, 9.7010e-07, 3.3193e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4222]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42217618\n",
            "state: tensor([[2.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[7.0091e-08, 1.0000e+00, 9.8746e-07, 3.3661e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4498]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.9743]], grad_fn=<ExpBackward0>)\n",
            "1 -0.449804\n",
            "state: tensor([[2.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[5.7671e-08, 1.0000e+00, 8.4329e-07, 2.7073e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4899]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.8288]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48994324\n",
            "state: tensor([[2.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[4.1620e-08, 1.0000e+00, 6.4658e-07, 1.8848e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5238]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7509]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5237769\n",
            "state: tensor([[2.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[2.9590e-08, 1.0000e+00, 4.8964e-07, 1.2905e-06]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5600]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5996]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5600105\n",
            "state: tensor([[2.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9681e-08, 1.0000e+00, 3.5086e-07, 8.2162e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5938]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.4796]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59381086\n",
            "state: tensor([[2.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0070e-08, 1.0000e+00, 2.0226e-07, 3.9340e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6151]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.5373]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6151188\n",
            "state: tensor([[2.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[4.5613e-09, 1.0000e+00, 1.0538e-07, 1.6488e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6357]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[6.7778]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6356577\n",
            "state: tensor([[2.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0029e-09, 1.0000e+00, 5.3536e-08, 6.6712e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6491]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.0917]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6490675\n",
            "state: tensor([[2.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[8.5255e-10, 1.0000e+00, 2.6508e-08, 2.6095e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66344136\n",
            "state: tensor([[2.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[3.3849e-10, 1.0000e+00, 1.2390e-08, 9.4674e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6762]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67622596\n",
            "state: tensor([[2.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[1.3192e-10, 1.0000e+00, 5.7029e-09, 3.3656e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6885]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68850625\n",
            "state: tensor([[2.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[4.6470e-11, 1.0000e+00, 2.4142e-09, 1.0705e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7037]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.70365095\n",
            "state: tensor([[2.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[1.6235e-11, 1.0000e+00, 1.0152e-09, 3.3737e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7180]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7179548\n",
            "state: tensor([[2.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[5.6721e-12, 1.0000e+00, 4.2687e-10, 1.0633e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7310]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7310305\n",
            "state: tensor([[2.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9817e-12, 1.0000e+00, 1.7950e-10, 3.3511e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7422]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7422093\n",
            "state: tensor([[2.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[6.9091e-13, 1.0000e+00, 7.5356e-11, 1.0533e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7525]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7524978\n",
            "state: tensor([[ 2.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4056e-13, 1.0000e+00, 3.1602e-11, 3.3051e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7620]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7619816\n",
            "state: tensor([[ 2.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[8.3760e-14, 1.0000e+00, 1.3253e-11, 1.0371e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7716]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7715925\n",
            "state: tensor([[3.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[5.5003e-09, 1.0000e+00, 1.1873e-07, 2.1143e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3891]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38907504\n",
            "state: tensor([[3.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[9.5858e-09, 1.0000e+00, 1.8829e-07, 3.8835e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3989]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39885226\n",
            "state: tensor([[3.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4108e-08, 1.0000e+00, 2.5977e-07, 5.9214e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4130]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.41296393\n",
            "state: tensor([[3.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.8821e-08, 1.0000e+00, 3.3080e-07, 8.0836e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4422]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4421717\n",
            "state: tensor([[3.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2342e-08, 1.0000e+00, 3.8258e-07, 9.7037e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4646]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46464455\n",
            "state: tensor([[3.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0673e-08, 1.0000e+00, 3.6024e-07, 8.8579e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4953]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49527118\n",
            "state: tensor([[3.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5393e-08, 1.0000e+00, 2.8351e-07, 6.3775e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5367]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3706]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5366626\n",
            "state: tensor([[3.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0832e-08, 1.0000e+00, 2.1293e-07, 4.3169e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5674]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2244]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5673719\n",
            "state: tensor([[3.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[7.0231e-09, 1.0000e+00, 1.4954e-07, 2.6674e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5992]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1698]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5992279\n",
            "state: tensor([[3.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[4.2996e-09, 1.0000e+00, 1.0012e-07, 1.5487e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6300]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.1414]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6299776\n",
            "state: tensor([[3.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2505e-09, 1.0000e+00, 5.8825e-08, 7.5990e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6525]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.2083]], grad_fn=<ExpBackward0>)\n",
            "1 -0.652525\n",
            "state: tensor([[3.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0244e-09, 1.0000e+00, 3.0771e-08, 3.2016e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6725]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6724638\n",
            "state: tensor([[3.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[4.4951e-10, 1.0000e+00, 1.5625e-08, 1.2943e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6878]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68783087\n",
            "state: tensor([[3.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.9202e-10, 1.0000e+00, 7.7595e-09, 5.0825e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6994]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69943476\n",
            "state: tensor([[3.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[7.6239e-11, 1.0000e+00, 3.6268e-09, 1.8439e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7120]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7119855\n",
            "state: tensor([[3.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[3.0269e-11, 1.0000e+00, 1.6952e-09, 6.6897e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7224]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72237295\n",
            "state: tensor([[3.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1888e-11, 1.0000e+00, 7.8532e-10, 2.3979e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7312]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7311734\n",
            "state: tensor([[3.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[4.1648e-12, 1.0000e+00, 3.3097e-10, 7.5801e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7429]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7428592\n",
            "state: tensor([[3.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4551e-12, 1.0000e+00, 1.3917e-10, 2.3890e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7540]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7540038\n",
            "state: tensor([[3.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[5.0787e-13, 1.0000e+00, 5.8478e-11, 7.5199e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7642]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.764204\n",
            "state: tensor([[ 3.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7683e-13, 1.0000e+00, 2.4524e-11, 2.3596e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7732]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77317655\n",
            "state: tensor([[ 3.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[6.1570e-14, 1.0000e+00, 1.0285e-11, 7.4040e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7827]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7827168\n",
            "state: tensor([[3.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5577e-09, 1.0000e+00, 4.1827e-08, 5.3333e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3816]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.38163406\n",
            "state: tensor([[3.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[2.4778e-09, 1.0000e+00, 6.1493e-08, 8.8659e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3994]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39936695\n",
            "state: tensor([[3.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[3.4637e-09, 1.0000e+00, 8.1236e-08, 1.2795e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4241]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.424063\n",
            "state: tensor([[3.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[4.6109e-09, 1.0000e+00, 1.0314e-07, 1.7471e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4532]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4531885\n",
            "state: tensor([[3.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[5.9743e-09, 1.0000e+00, 1.2822e-07, 2.3089e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4825]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.48249793\n",
            "state: tensor([[3.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[6.6114e-09, 1.0000e+00, 1.3994e-07, 2.5644e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5055]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50553757\n",
            "state: tensor([[3.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[5.5775e-09, 1.0000e+00, 1.2218e-07, 2.1123e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5388]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53876406\n",
            "state: tensor([[3.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[3.7467e-09, 1.0000e+00, 8.8410e-08, 1.3557e-07]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5758]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.57576054\n",
            "state: tensor([[3.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[2.3942e-09, 1.0000e+00, 6.1359e-08, 8.2400e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6061]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6060909\n",
            "state: tensor([[3.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5315e-09, 1.0000e+00, 4.2615e-08, 5.0160e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6349]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63486713\n",
            "state: tensor([[3.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[9.2984e-10, 1.0000e+00, 2.8336e-08, 2.8863e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6617]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66170114\n",
            "state: tensor([[3.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[4.9254e-10, 1.0000e+00, 1.6814e-08, 1.4344e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6835]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6835012\n",
            "state: tensor([[3.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2990e-10, 1.0000e+00, 8.9810e-09, 6.2115e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7048]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7048412\n",
            "state: tensor([[3.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0088e-10, 1.0000e+00, 4.5604e-09, 2.5111e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7224]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72239006\n",
            "state: tensor([[3.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[4.3250e-11, 1.0000e+00, 2.2714e-09, 9.8988e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7320]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7320483\n",
            "state: tensor([[3.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.7171e-11, 1.0000e+00, 1.0617e-09, 3.5913e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7412]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7412195\n",
            "state: tensor([[3.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[6.8176e-12, 1.0000e+00, 4.9623e-10, 1.3029e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7499]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7498652\n",
            "state: tensor([[3.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[2.6847e-12, 1.0000e+00, 2.3039e-10, 4.6840e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7578378\n",
            "state: tensor([[3.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0431e-12, 1.0000e+00, 1.0578e-10, 1.6588e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7654]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76538444\n",
            "state: tensor([[3.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[3.7327e-13, 1.0000e+00, 4.5374e-11, 5.3676e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7753]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7752757\n",
            "state: tensor([[ 3.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.2999e-13, 1.0000e+00, 1.9031e-11, 1.6846e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7850]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7850065\n",
            "state: tensor([[ 3.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[4.5259e-14, 1.0000e+00, 7.9812e-12, 5.2860e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7933]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.793332\n",
            "state: tensor([[4.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[4.1141e-10, 1.0000e+00, 1.3904e-08, 1.2472e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3776]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37757912\n",
            "state: tensor([[4.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[6.3977e-10, 1.0000e+00, 2.0068e-08, 2.0208e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3942]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39417547\n",
            "state: tensor([[4.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[8.5586e-10, 1.0000e+00, 2.5552e-08, 2.7816e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4297]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42970774\n",
            "state: tensor([[4.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1294e-09, 1.0000e+00, 3.2171e-08, 3.7717e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4619]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46191618\n",
            "state: tensor([[4.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4642e-09, 1.0000e+00, 3.9999e-08, 4.9910e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4928]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49284557\n",
            "state: tensor([[4.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6209e-09, 1.0000e+00, 4.3682e-08, 5.5421e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5184]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5184145\n",
            "state: tensor([[4.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.6512e-09, 1.0000e+00, 4.4558e-08, 5.6103e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5455]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.54550487\n",
            "state: tensor([[4.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2930e-09, 1.0000e+00, 3.6611e-08, 4.2537e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5819]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5819423\n",
            "state: tensor([[4.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[8.2001e-10, 1.0000e+00, 2.5269e-08, 2.5618e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6123]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6122856\n",
            "state: tensor([[4.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[5.2329e-10, 1.0000e+00, 1.7518e-08, 1.5545e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6434]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6433957\n",
            "state: tensor([[4.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[3.3417e-10, 1.0000e+00, 1.2150e-08, 9.4406e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6677]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6676855\n",
            "state: tensor([[4.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0109e-10, 1.0000e+00, 8.0195e-09, 5.3792e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6893]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6892718\n",
            "state: tensor([[4.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0780e-10, 1.0000e+00, 4.8062e-09, 2.7075e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7081]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.708071\n",
            "state: tensor([[4.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[5.1596e-11, 1.0000e+00, 2.6212e-09, 1.2051e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7275]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7275499\n",
            "state: tensor([[4.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2641e-11, 1.0000e+00, 1.3310e-09, 4.8719e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7449]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7448775\n",
            "state: tensor([[4.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[9.7412e-12, 1.0000e+00, 6.6491e-10, 1.9279e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7583]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.758317\n",
            "state: tensor([[4.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[3.8676e-12, 1.0000e+00, 3.1078e-10, 6.9946e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7669]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76691633\n",
            "state: tensor([[4.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5355e-12, 1.0000e+00, 1.4526e-10, 2.5376e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7749]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.774949\n",
            "state: tensor([[4.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[6.0630e-13, 1.0000e+00, 6.7588e-11, 9.1499e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7826]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78256863\n",
            "state: tensor([[4.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.3555e-13, 1.0000e+00, 3.1033e-11, 3.2403e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7899]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78990114\n",
            "state: tensor([[ 4.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[9.1321e-14, 1.0000e+00, 1.4225e-11, 1.1444e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7966]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79658294\n",
            "state: tensor([[ 4.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[3.3269e-14, 1.0000e+00, 6.1935e-12, 3.7738e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8043]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8043412\n",
            "state: tensor([[4.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0559e-10, 1.0000e+00, 4.5161e-09, 2.8232e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3700]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36998644\n",
            "state: tensor([[4.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5748e-10, 1.0000e+00, 6.2945e-09, 4.3736e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3956]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39563218\n",
            "state: tensor([[4.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[2.1128e-10, 1.0000e+00, 8.0323e-09, 6.0402e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4319]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43191513\n",
            "state: tensor([[4.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[2.7306e-10, 1.0000e+00, 9.9420e-09, 8.0007e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4672]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4671663\n",
            "state: tensor([[4.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[3.3788e-10, 1.0000e+00, 1.1885e-08, 1.0078e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4997]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49966863\n",
            "state: tensor([[4.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[3.8966e-10, 1.0000e+00, 1.3421e-08, 1.1714e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5274]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5274262\n",
            "state: tensor([[4.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[3.9619e-10, 1.0000e+00, 1.3667e-08, 1.1839e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5563]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5562523\n",
            "state: tensor([[4.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[3.6181e-10, 1.0000e+00, 1.2748e-08, 1.0619e-08]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5869]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.586855\n",
            "state: tensor([[4.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[2.8304e-10, 1.0000e+00, 1.0466e-08, 8.0397e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6196]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6196056\n",
            "state: tensor([[4.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.7948e-10, 1.0000e+00, 7.2225e-09, 4.8412e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6462]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.646202\n",
            "state: tensor([[4.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1438e-10, 1.0000e+00, 5.0016e-09, 2.9328e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6727]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67266256\n",
            "state: tensor([[4.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[7.3041e-11, 1.0000e+00, 3.4691e-09, 1.7811e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6957]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6957009\n",
            "state: tensor([[4.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[4.3545e-11, 1.0000e+00, 2.2720e-09, 1.0044e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7148]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71476954\n",
            "state: tensor([[4.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[2.3592e-11, 1.0000e+00, 1.3738e-09, 5.1107e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7309]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7308954\n",
            "state: tensor([[4.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1406e-11, 1.0000e+00, 7.5555e-10, 2.2995e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7486]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7485718\n",
            "state: tensor([[4.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[5.0814e-12, 1.0000e+00, 3.8848e-10, 9.4522e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7656]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7655921\n",
            "state: tensor([[4.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[2.1940e-12, 1.0000e+00, 1.9464e-10, 3.7550e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7807]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7806636\n",
            "state: tensor([[4.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[8.7110e-13, 1.0000e+00, 9.0975e-11, 1.3623e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7903]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7902838\n",
            "state: tensor([[4.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[3.4585e-13, 1.0000e+00, 4.2522e-11, 4.9424e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7979]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.797945\n",
            "state: tensor([[4.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[1.3692e-13, 1.0000e+00, 1.9828e-11, 1.7873e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8056]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8055976\n",
            "state: tensor([[ 4.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[5.3141e-14, 1.0000e+00, 9.0968e-12, 6.3213e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8122]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81215644\n",
            "state: tensor([[ 4.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[2.0576e-14, 1.0000e+00, 4.1656e-12, 2.2288e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8180]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8179694\n",
            "state: tensor([[5.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[2.7029e-11, 1.0000e+00, 1.4638e-09, 6.3723e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3647]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36468348\n",
            "state: tensor([[5.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[3.8457e-11, 1.0000e+00, 1.9614e-09, 9.3829e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3978]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39775905\n",
            "state: tensor([[5.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[4.9298e-11, 1.0000e+00, 2.4117e-09, 1.2315e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4333]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43333995\n",
            "state: tensor([[5.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[6.2187e-11, 1.0000e+00, 2.9266e-09, 1.5875e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4682]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4681675\n",
            "state: tensor([[5.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[7.7134e-11, 1.0000e+00, 3.5039e-09, 2.0070e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5051]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5050917\n",
            "state: tensor([[5.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[9.2420e-11, 1.0000e+00, 4.0786e-09, 2.4388e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5333]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53331804\n",
            "state: tensor([[5.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[9.5257e-11, 1.0000e+00, 4.1993e-09, 2.5030e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5643]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5642625\n",
            "state: tensor([[5.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[8.9220e-11, 1.0000e+00, 3.9982e-09, 2.3093e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5957]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5956975\n",
            "state: tensor([[5.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[7.5555e-11, 1.0000e+00, 3.5057e-09, 1.9056e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6233]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6232521\n",
            "state: tensor([[5.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[5.9138e-11, 1.0000e+00, 2.8793e-09, 1.4438e-09]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6488]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6487991\n",
            "state: tensor([[5.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[3.9277e-11, 1.0000e+00, 2.0640e-09, 9.1466e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6736]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67357284\n",
            "state: tensor([[5.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5001e-11, 1.0000e+00, 1.4280e-09, 5.5331e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6983]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69833505\n",
            "state: tensor([[5.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5965e-11, 1.0000e+00, 9.9048e-10, 3.3603e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7216]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72159934\n",
            "state: tensor([[5.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[9.4394e-12, 1.0000e+00, 6.4420e-10, 1.8780e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7394]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73935676\n",
            "state: tensor([[5.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[5.1736e-12, 1.0000e+00, 3.9330e-10, 9.6736e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7546]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7545625\n",
            "state: tensor([[5.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[2.4844e-12, 1.0000e+00, 2.1513e-10, 4.3173e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7699]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7699376\n",
            "state: tensor([[5.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1404e-12, 1.0000e+00, 1.1338e-10, 1.8338e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7853]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7853184\n",
            "state: tensor([[5.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[4.9417e-13, 1.0000e+00, 5.6976e-11, 7.3133e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7998596\n",
            "state: tensor([[5.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9620e-13, 1.0000e+00, 2.6631e-11, 2.6533e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8109]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8109152\n",
            "state: tensor([[5.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[7.7897e-14, 1.0000e+00, 1.2447e-11, 9.6260e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8187]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8186839\n",
            "state: tensor([[ 5.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[3.0921e-14, 1.0000e+00, 5.8169e-12, 3.4914e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8247]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.82470703\n",
            "state: tensor([[ 5.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.1973e-14, 1.0000e+00, 2.6639e-12, 1.2312e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8310]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8309765\n",
            "state: tensor([[5.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[6.6670e-12, 1.0000e+00, 4.6016e-10, 1.3809e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3633]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3633351\n",
            "state: tensor([[5.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[8.7824e-12, 1.0000e+00, 5.7869e-10, 1.8675e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3989]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39889425\n",
            "state: tensor([[5.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1227e-11, 1.0000e+00, 7.0993e-10, 2.4436e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4343]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4343136\n",
            "state: tensor([[5.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4162e-11, 1.0000e+00, 8.6151e-10, 3.1499e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4691]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4691035\n",
            "state: tensor([[5.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.7645e-11, 1.0000e+00, 1.0353e-09, 4.0017e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5059]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5058585\n",
            "state: tensor([[5.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0918e-11, 1.0000e+00, 1.1937e-09, 4.8135e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5380]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5380456\n",
            "state: tensor([[5.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2687e-11, 1.0000e+00, 1.2805e-09, 5.2356e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5665]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5664669\n",
            "state: tensor([[5.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[2.1764e-11, 1.0000e+00, 1.2431e-09, 4.9616e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6016]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6016127\n",
            "state: tensor([[5.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.8479e-11, 1.0000e+00, 1.0923e-09, 4.1061e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6290]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6289868\n",
            "state: tensor([[5.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5689e-11, 1.0000e+00, 9.5973e-10, 3.3981e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6545]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6544501\n",
            "state: tensor([[5.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2235e-11, 1.0000e+00, 7.8578e-10, 2.5647e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6747]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.67471147\n",
            "state: tensor([[5.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[8.5891e-12, 1.0000e+00, 5.8949e-10, 1.7268e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6992]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6991969\n",
            "state: tensor([[5.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[5.4645e-12, 1.0000e+00, 4.0772e-10, 1.0439e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7223]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72232836\n",
            "state: tensor([[5.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[3.4896e-12, 1.0000e+00, 2.8279e-10, 6.3398e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7440]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7440212\n",
            "state: tensor([[5.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[2.0462e-12, 1.0000e+00, 1.8266e-10, 3.5116e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7621]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7621184\n",
            "state: tensor([[5.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1349e-12, 1.0000e+00, 1.1263e-10, 1.8320e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7767]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77668047\n",
            "state: tensor([[5.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[5.4223e-13, 1.0000e+00, 6.1354e-11, 8.1287e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7911]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79114\n",
            "state: tensor([[5.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5620e-13, 1.0000e+00, 3.3118e-11, 3.5629e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8042]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80422574\n",
            "state: tensor([[5.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1130e-13, 1.0000e+00, 1.6679e-11, 1.4244e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8171]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8170913\n",
            "state: tensor([[5.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[4.4191e-14, 1.0000e+00, 7.7956e-12, 5.1676e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8276]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8276035\n",
            "state: tensor([[ 5.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.7545e-14, 1.0000e+00, 3.6437e-12, 1.8748e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8373]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83733606\n",
            "state: tensor([[ 5.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[6.9506e-15, 1.0000e+00, 1.7001e-12, 6.7824e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8428]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8427692\n",
            "state: tensor([[6.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5209e-12, 1.0000e+00, 1.3566e-10, 2.7447e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3636]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36356303\n",
            "state: tensor([[6.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.9981e-12, 1.0000e+00, 1.7021e-10, 3.7014e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3996]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39958653\n",
            "state: tensor([[6.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[2.5569e-12, 1.0000e+00, 2.0898e-10, 4.8486e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4353]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43528613\n",
            "state: tensor([[6.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[3.2253e-12, 1.0000e+00, 2.5360e-10, 6.2500e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4697]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4696572\n",
            "state: tensor([[6.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[4.0241e-12, 1.0000e+00, 3.0510e-10, 7.9531e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5059]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50586087\n",
            "state: tensor([[6.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[4.7619e-12, 1.0000e+00, 3.5122e-10, 9.5523e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5379]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5378607\n",
            "state: tensor([[6.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[5.3388e-12, 1.0000e+00, 3.8692e-10, 1.0791e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5665]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5665322\n",
            "state: tensor([[6.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[5.1380e-12, 1.0000e+00, 3.7630e-10, 1.0279e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6014]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.601364\n",
            "state: tensor([[6.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[4.5076e-12, 1.0000e+00, 3.3960e-10, 8.8219e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6347]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6346826\n",
            "state: tensor([[6.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[3.8271e-12, 1.0000e+00, 2.9839e-10, 7.3008e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6598]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6598367\n",
            "state: tensor([[6.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[3.2494e-12, 1.0000e+00, 2.6219e-10, 6.0419e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6818]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.68180686\n",
            "state: tensor([[6.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5232e-12, 1.0000e+00, 2.1390e-10, 4.5389e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6997]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6997277\n",
            "state: tensor([[6.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.7739e-12, 1.0000e+00, 1.6065e-10, 3.0610e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7225]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.722488\n",
            "state: tensor([[6.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1833e-12, 1.0000e+00, 1.1552e-10, 1.9491e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7445]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74451053\n",
            "state: tensor([[6.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[7.5907e-13, 1.0000e+00, 8.0424e-11, 1.1897e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7648]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7647929\n",
            "state: tensor([[6.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[4.4344e-13, 1.0000e+00, 5.1780e-11, 6.5643e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7824]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78239185\n",
            "state: tensor([[6.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[2.4728e-13, 1.0000e+00, 3.2073e-11, 3.4432e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7970]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7970343\n",
            "state: tensor([[6.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.1835e-13, 1.0000e+00, 1.7498e-11, 1.5305e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8106]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81059754\n",
            "state: tensor([[6.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[5.6438e-14, 1.0000e+00, 9.5179e-12, 6.7771e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8225]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.82254684\n",
            "state: tensor([[6.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.5111e-14, 1.0000e+00, 4.8888e-12, 2.7807e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8339]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83393204\n",
            "state: tensor([[ 6.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[9.9532e-15, 1.0000e+00, 2.2820e-12, 1.0065e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8427]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8426553\n",
            "state: tensor([[ 6.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[3.9474e-15, 1.0000e+00, 1.0657e-12, 3.6463e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8517]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8517453\n",
            "state: tensor([[6.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[3.4696e-13, 1.0000e+00, 3.9993e-11, 5.4552e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3638]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.363791\n",
            "state: tensor([[6.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[4.5459e-13, 1.0000e+00, 5.0064e-11, 7.3361e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4001]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40011802\n",
            "state: tensor([[6.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[5.8229e-13, 1.0000e+00, 6.1518e-11, 9.6206e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4358]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4358454\n",
            "state: tensor([[6.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[7.3453e-13, 1.0000e+00, 7.4653e-11, 1.2401e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4701]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47006443\n",
            "state: tensor([[6.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[9.1550e-13, 1.0000e+00, 8.9731e-11, 1.5764e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5048]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5048133\n",
            "state: tensor([[6.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0870e-12, 1.0000e+00, 1.0360e-10, 1.9001e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5372]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.537196\n",
            "state: tensor([[6.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2509e-12, 1.0000e+00, 1.1653e-10, 2.2128e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5667]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56665057\n",
            "state: tensor([[6.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2052e-12, 1.0000e+00, 1.1338e-10, 2.1118e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6010]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6010269\n",
            "state: tensor([[6.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0996e-12, 1.0000e+00, 1.0559e-10, 1.8954e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6351]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6351201\n",
            "state: tensor([[6.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[9.3359e-13, 1.0000e+00, 9.2776e-11, 1.5686e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6652]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66515625\n",
            "state: tensor([[6.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[7.9265e-13, 1.0000e+00, 8.1518e-11, 1.2981e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6886]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6886117\n",
            "state: tensor([[6.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[6.7299e-13, 1.0000e+00, 7.1626e-11, 1.0743e-11]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7062]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7061743\n",
            "state: tensor([[6.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[5.2035e-13, 1.0000e+00, 5.8227e-11, 8.0328e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7231]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72313815\n",
            "state: tensor([[6.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[3.6570e-13, 1.0000e+00, 4.3717e-11, 5.4153e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7449]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74488455\n",
            "state: tensor([[6.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[2.5585e-13, 1.0000e+00, 3.2696e-11, 3.6333e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7655]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7655327\n",
            "state: tensor([[6.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6373e-13, 1.0000e+00, 2.2716e-11, 2.2119e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7839]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.78394073\n",
            "state: tensor([[6.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[9.4860e-14, 1.0000e+00, 1.4524e-11, 1.2095e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8001]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.800135\n",
            "state: tensor([[6.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[5.2854e-14, 1.0000e+00, 8.9909e-12, 6.3378e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8148]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8147653\n",
            "state: tensor([[6.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[2.5716e-14, 1.0000e+00, 4.9725e-12, 2.8676e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8281]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8280649\n",
            "state: tensor([[6.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2318e-14, 1.0000e+00, 2.7145e-12, 1.2760e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8391]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8391155\n",
            "state: tensor([[ 6.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[5.6692e-15, 1.0000e+00, 1.4338e-12, 5.4336e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8495]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8495079\n",
            "state: tensor([[ 6.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[2.2471e-15, 1.0000e+00, 6.6926e-13, 1.9667e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8574]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85737634\n",
            "state: tensor([[7.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[7.9149e-14, 1.0000e+00, 1.1790e-11, 1.0843e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3637]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3636618\n",
            "state: tensor([[7.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0342e-13, 1.0000e+00, 1.4725e-11, 1.4540e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4001]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40008807\n",
            "state: tensor([[7.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.3261e-13, 1.0000e+00, 1.8109e-11, 1.9089e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4363]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4362686\n",
            "state: tensor([[7.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6728e-13, 1.0000e+00, 2.1976e-11, 2.4607e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4705]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.47047156\n",
            "state: tensor([[7.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[2.0814e-13, 1.0000e+00, 2.6376e-11, 3.1225e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5033]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50327146\n",
            "state: tensor([[7.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[2.4815e-13, 1.0000e+00, 3.0558e-11, 3.7798e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5364]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5363983\n",
            "state: tensor([[7.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[2.8147e-13, 1.0000e+00, 3.3966e-11, 4.3333e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5664]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5664078\n",
            "state: tensor([[7.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[2.8302e-13, 1.0000e+00, 3.4211e-11, 4.3417e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5991]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5991304\n",
            "state: tensor([[7.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[2.6805e-13, 1.0000e+00, 3.2826e-11, 4.0654e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6349]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6349254\n",
            "state: tensor([[7.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[2.2774e-13, 1.0000e+00, 2.8845e-11, 3.3701e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6672]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.667157\n",
            "state: tensor([[7.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9336e-13, 1.0000e+00, 2.5345e-11, 2.7890e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6936]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69356436\n",
            "state: tensor([[7.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[1.6417e-13, 1.0000e+00, 2.2270e-11, 2.3081e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7154]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71537\n",
            "state: tensor([[7.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.3938e-13, 1.0000e+00, 1.9567e-11, 1.9101e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7290]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7289764\n",
            "state: tensor([[7.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0731e-13, 1.0000e+00, 1.5850e-11, 1.4216e-12]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7450]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74499726\n",
            "state: tensor([[7.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[7.5390e-14, 1.0000e+00, 1.1897e-11, 9.5801e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7657]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76571953\n",
            "state: tensor([[7.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[5.3004e-14, 1.0000e+00, 8.9338e-12, 6.4615e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7849]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7849398\n",
            "state: tensor([[7.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[3.5442e-14, 1.0000e+00, 6.4349e-12, 4.1280e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8020]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80195177\n",
            "state: tensor([[7.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[2.0292e-14, 1.0000e+00, 4.0741e-12, 2.2287e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8160]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8160072\n",
            "state: tensor([[7.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1249e-14, 1.0000e+00, 2.5116e-12, 1.1610e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8297]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8296924\n",
            "state: tensor([[7.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[5.5389e-15, 1.0000e+00, 1.4029e-12, 5.3206e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8435]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84351826\n",
            "state: tensor([[ 7.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6535e-15, 1.0000e+00, 7.6594e-13, 2.3679e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8540]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8539702\n",
            "state: tensor([[ 7.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.2669e-15, 1.0000e+00, 4.1700e-13, 1.0498e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8634]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86343694\n",
            "state: tensor([[7.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[1.8017e-14, 1.0000e+00, 3.4695e-12, 2.1502e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3635]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36354864\n",
            "state: tensor([[7.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[2.3530e-14, 1.0000e+00, 4.3312e-12, 2.8818e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4001]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40005806\n",
            "state: tensor([[7.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[3.0201e-14, 1.0000e+00, 5.3308e-12, 3.7877e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4363]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43629926\n",
            "state: tensor([[7.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[3.8097e-14, 1.0000e+00, 6.4690e-12, 4.8824e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4698]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46976006\n",
            "state: tensor([[7.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[4.7323e-14, 1.0000e+00, 7.7530e-12, 6.1849e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5017]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50172645\n",
            "state: tensor([[7.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[5.6647e-14, 1.0000e+00, 9.0136e-12, 7.5187e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5356]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5355995\n",
            "state: tensor([[7.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[6.3396e-14, 1.0000e+00, 9.9090e-12, 8.4928e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5660]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56595075\n",
            "state: tensor([[7.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[6.6521e-14, 1.0000e+00, 1.0331e-11, 8.9333e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5973]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5973093\n",
            "state: tensor([[7.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[6.3888e-14, 1.0000e+00, 1.0025e-11, 8.4965e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6333]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6332825\n",
            "state: tensor([[7.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[5.5511e-14, 1.0000e+00, 8.9650e-12, 7.2312e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6670]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66696584\n",
            "state: tensor([[7.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[4.7167e-14, 1.0000e+00, 7.8802e-12, 5.9922e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6960]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6959568\n",
            "state: tensor([[7.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[4.0047e-14, 1.0000e+00, 6.9240e-12, 4.9590e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7200]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.71996766\n",
            "state: tensor([[7.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[3.4001e-14, 1.0000e+00, 6.0838e-12, 4.1039e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7387]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.73866254\n",
            "state: tensor([[7.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[2.8868e-14, 1.0000e+00, 5.3456e-12, 3.3963e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7503]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7502679\n",
            "state: tensor([[7.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[2.2127e-14, 1.0000e+00, 4.3142e-12, 2.5157e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7655]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76551366\n",
            "state: tensor([[7.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5542e-14, 1.0000e+00, 3.2374e-12, 1.6948e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7847]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7846766\n",
            "state: tensor([[7.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0917e-14, 1.0000e+00, 2.4294e-12, 1.1418e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8022]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8022079\n",
            "state: tensor([[7.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[7.6832e-15, 1.0000e+00, 1.8251e-12, 7.7163e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8185]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8184554\n",
            "state: tensor([[7.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[4.3611e-15, 1.0000e+00, 1.1472e-12, 4.1273e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8313]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8313295\n",
            "state: tensor([[7.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.3958e-15, 1.0000e+00, 7.0203e-13, 2.1287e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8436]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84360886\n",
            "state: tensor([[ 7.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1930e-15, 1.0000e+00, 3.9581e-13, 9.8717e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8565]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8565486\n",
            "state: tensor([[ 7.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[5.7152e-16, 1.0000e+00, 2.1610e-13, 4.3934e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8675]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8674927\n",
            "state: tensor([[8.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[4.0991e-15, 1.0000e+00, 1.0205e-12, 4.2617e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3635]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36351743\n",
            "state: tensor([[8.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[5.3534e-15, 1.0000e+00, 1.2739e-12, 5.7116e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40002793\n",
            "state: tensor([[8.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[6.8779e-15, 1.0000e+00, 1.5692e-12, 7.5155e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4362]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4362151\n",
            "state: tensor([[8.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[8.6761e-15, 1.0000e+00, 1.9043e-12, 9.6878e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4684]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46843788\n",
            "state: tensor([[8.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.0759e-14, 1.0000e+00, 2.2789e-12, 1.2251e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5002]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.50017816\n",
            "state: tensor([[8.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2850e-14, 1.0000e+00, 2.6450e-12, 1.4853e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5349]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.53490627\n",
            "state: tensor([[8.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4279e-14, 1.0000e+00, 2.8907e-12, 1.6645e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5654]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5653929\n",
            "state: tensor([[8.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.5729e-14, 1.0000e+00, 3.1368e-12, 1.8475e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5947]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5947094\n",
            "state: tensor([[8.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5003e-14, 1.0000e+00, 3.0249e-12, 1.7469e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6317]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.63171214\n",
            "state: tensor([[8.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.3537e-14, 1.0000e+00, 2.7895e-12, 1.5502e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6657]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6657406\n",
            "state: tensor([[8.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.1504e-14, 1.0000e+00, 2.4498e-12, 1.2870e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6965]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.69646925\n",
            "state: tensor([[8.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[9.7690e-15, 1.0000e+00, 2.1528e-12, 1.0654e-13]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7224]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.722437\n",
            "state: tensor([[8.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[8.2942e-15, 1.0000e+00, 1.8916e-12, 8.8173e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7444]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7444393\n",
            "state: tensor([[8.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[7.0421e-15, 1.0000e+00, 1.6620e-12, 7.2969e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7593]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7592991\n",
            "state: tensor([[8.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[5.9790e-15, 1.0000e+00, 1.4603e-12, 6.0387e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7700]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7700099\n",
            "state: tensor([[8.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[4.5617e-15, 1.0000e+00, 1.1740e-12, 4.4504e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7841]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7841473\n",
            "state: tensor([[8.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[3.2041e-15, 1.0000e+00, 8.8100e-13, 2.9983e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8018]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8017928\n",
            "state: tensor([[8.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[2.2506e-15, 1.0000e+00, 6.6111e-13, 2.0199e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8181]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8180727\n",
            "state: tensor([[8.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5927e-15, 1.0000e+00, 4.9896e-13, 1.3732e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8332]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83320343\n",
            "state: tensor([[8.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[9.3761e-16, 1.0000e+00, 3.2313e-13, 7.6461e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8455]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.845526\n",
            "state: tensor([[ 8.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1247e-16, 1.0000e+00, 1.9693e-13, 3.9207e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8569]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85694367\n",
            "state: tensor([[ 8.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[2.5742e-16, 1.0000e+00, 1.1184e-13, 1.8353e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8688]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8687578\n",
            "state: tensor([[8.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[9.3260e-16, 1.0000e+00, 3.0016e-13, 8.4466e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3658]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3657883\n",
            "state: tensor([[8.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2180e-15, 1.0000e+00, 3.7470e-13, 1.1320e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39999786\n",
            "state: tensor([[8.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[1.5664e-15, 1.0000e+00, 4.6193e-13, 1.4912e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4351]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43508703\n",
            "state: tensor([[8.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.9759e-15, 1.0000e+00, 5.6057e-13, 1.9222e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4671]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4671087\n",
            "state: tensor([[8.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[2.4461e-15, 1.0000e+00, 6.6987e-13, 2.4265e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4986]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4986072\n",
            "state: tensor([[8.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[2.8837e-15, 1.0000e+00, 7.6921e-13, 2.8998e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5337]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5336879\n",
            "state: tensor([[8.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[3.2161e-15, 1.0000e+00, 8.4332e-13, 3.2622e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5648]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56480527\n",
            "state: tensor([[8.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[3.5737e-15, 1.0000e+00, 9.2167e-13, 3.6558e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5939]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5938505\n",
            "state: tensor([[8.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[3.5328e-15, 1.0000e+00, 9.1497e-13, 3.5999e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6298]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62977487\n",
            "state: tensor([[8.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[3.3048e-15, 1.0000e+00, 8.6882e-13, 3.3271e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6640]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66396284\n",
            "state: tensor([[8.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[2.8028e-15, 1.0000e+00, 7.6171e-13, 2.7568e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6957]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6956874\n",
            "state: tensor([[8.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[2.3830e-15, 1.0000e+00, 6.6934e-13, 2.2891e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7240]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7239545\n",
            "state: tensor([[8.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[2.0233e-15, 1.0000e+00, 5.8812e-13, 1.8944e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7470]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74695516\n",
            "state: tensor([[8.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.7178e-15, 1.0000e+00, 5.1675e-13, 1.5677e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7671]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.76706207\n",
            "state: tensor([[8.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[1.4585e-15, 1.0000e+00, 4.5405e-13, 1.2974e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7785]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7785137\n",
            "state: tensor([[8.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[1.2383e-15, 1.0000e+00, 3.9895e-13, 1.0737e-14]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7875]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7875062\n",
            "state: tensor([[8.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[9.4042e-16, 1.0000e+00, 3.1948e-13, 7.8732e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8014]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80137694\n",
            "state: tensor([[8.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[6.6055e-16, 1.0000e+00, 2.3974e-13, 5.3042e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8177]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8176875\n",
            "state: tensor([[8.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[4.6398e-16, 1.0000e+00, 1.7991e-13, 3.5735e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8328]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8327829\n",
            "state: tensor([[8.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[3.2715e-16, 1.0000e+00, 1.3538e-13, 2.4194e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8468]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84676343\n",
            "state: tensor([[ 8.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[2.0158e-16, 1.0000e+00, 9.1016e-14, 1.4165e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8586]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8586198\n",
            "state: tensor([[ 8.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.0962e-16, 1.0000e+00, 5.5241e-14, 7.2213e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8692]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.86922204\n",
            "state: tensor([[9.0000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[2.1218e-16, 1.0000e+00, 8.8285e-14, 1.6741e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3687]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.36867246\n",
            "state: tensor([[9.0000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[2.7710e-16, 1.0000e+00, 1.1021e-13, 2.2437e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39996773\n",
            "state: tensor([[9.0000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[3.5672e-16, 1.0000e+00, 1.3598e-13, 2.9589e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4332]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4331858\n",
            "state: tensor([[9.0000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[4.4998e-16, 1.0000e+00, 1.6501e-13, 3.8141e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4655]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4655097\n",
            "state: tensor([[9.0000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[5.5614e-16, 1.0000e+00, 1.9690e-13, 4.8064e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4970]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4970244\n",
            "state: tensor([[9.0000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[6.4689e-16, 1.0000e+00, 2.2362e-13, 5.6590e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5324]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.532403\n",
            "state: tensor([[9.0000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[7.2438e-16, 1.0000e+00, 2.4602e-13, 6.3935e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5642]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56421715\n",
            "state: tensor([[9.0000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[8.0492e-16, 1.0000e+00, 2.6888e-13, 7.1650e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5933]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.59329134\n",
            "state: tensor([[9.0000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[8.3536e-16, 1.0000e+00, 2.7782e-13, 7.4450e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6272]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6272275\n",
            "state: tensor([[9.0000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[7.9533e-16, 1.0000e+00, 2.6746e-13, 7.0284e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6622]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6622053\n",
            "state: tensor([[9.0000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[6.8368e-16, 1.0000e+00, 2.3707e-13, 5.9121e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6942]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6942352\n",
            "state: tensor([[9.0000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[5.8070e-16, 1.0000e+00, 2.0811e-13, 4.9053e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7233]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7232622\n",
            "state: tensor([[9.0000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[4.9356e-16, 1.0000e+00, 1.8285e-13, 4.0701e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7485]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.74847746\n",
            "state: tensor([[9.0000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[4.1904e-16, 1.0000e+00, 1.6067e-13, 3.3683e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7696]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7695972\n",
            "state: tensor([[9.0000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[3.5579e-16, 1.0000e+00, 1.4117e-13, 2.7875e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7866]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7866479\n",
            "state: tensor([[9.0000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[3.0208e-16, 1.0000e+00, 1.2404e-13, 2.3069e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7957]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.795662\n",
            "state: tensor([[9.0000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[2.5647e-16, 1.0000e+00, 1.0899e-13, 1.9091e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8038]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8038192\n",
            "state: tensor([[9.0000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[1.9387e-16, 1.0000e+00, 8.6941e-14, 1.3928e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8173]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8173016\n",
            "state: tensor([[9.0000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[1.3618e-16, 1.0000e+00, 6.5241e-14, 9.3836e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8324]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83242595\n",
            "state: tensor([[9.0000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[9.5652e-17, 1.0000e+00, 4.8958e-14, 6.3218e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8464]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8464042\n",
            "state: tensor([[ 9.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[6.6808e-17, 1.0000e+00, 3.6553e-14, 4.2359e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8593]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8592615\n",
            "state: tensor([[ 9.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[4.1410e-17, 1.0000e+00, 2.4697e-14, 2.4958e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8703]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.87027764\n",
            "state: tensor([[9.5000, 0.0000, 1.8000]]) dist_dsc.probs: tensor([[4.8180e-17, 1.0000e+00, 2.5925e-14, 3.3111e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3719]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3718784\n",
            "state: tensor([[9.5000, 0.5000, 1.8000]]) dist_dsc.probs: tensor([[6.3043e-17, 1.0000e+00, 3.2416e-14, 4.4469e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3989]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3989169\n",
            "state: tensor([[9.5000, 1.0000, 1.8000]]) dist_dsc.probs: tensor([[8.1239e-17, 1.0000e+00, 4.0028e-14, 5.8710e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4313]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.43127024\n",
            "state: tensor([[9.5000, 1.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0248e-16, 1.0000e+00, 4.8575e-14, 7.5680e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4637]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46366233\n",
            "state: tensor([[9.5000, 2.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2644e-16, 1.0000e+00, 5.7878e-14, 9.5202e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4954]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49543828\n",
            "state: tensor([[9.5000, 2.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4511e-16, 1.0000e+00, 6.5010e-14, 1.1044e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5311]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5311157\n",
            "state: tensor([[9.5000, 3.0000, 1.8000]]) dist_dsc.probs: tensor([[1.6316e-16, 1.0000e+00, 7.1772e-14, 1.2531e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5636]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5636282\n",
            "state: tensor([[9.5000, 3.5000, 1.8000]]) dist_dsc.probs: tensor([[1.8129e-16, 1.0000e+00, 7.8440e-14, 1.4043e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5927]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5927316\n",
            "state: tensor([[9.5000, 4.0000, 1.8000]]) dist_dsc.probs: tensor([[1.9753e-16, 1.0000e+00, 8.4356e-14, 1.5397e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6244]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6244286\n",
            "state: tensor([[9.5000, 4.5000, 1.8000]]) dist_dsc.probs: tensor([[1.8762e-16, 1.0000e+00, 8.1036e-14, 1.4507e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6602]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.66016877\n",
            "state: tensor([[9.5000, 5.0000, 1.8000]]) dist_dsc.probs: tensor([[1.6691e-16, 1.0000e+00, 7.3839e-14, 1.2689e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6926]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.692581\n",
            "state: tensor([[9.5000, 5.5000, 1.8000]]) dist_dsc.probs: tensor([[1.4156e-16, 1.0000e+00, 6.4738e-14, 1.0514e-15]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7220]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72204727\n",
            "state: tensor([[9.5000, 6.0000, 1.8000]]) dist_dsc.probs: tensor([[1.2031e-16, 1.0000e+00, 5.6859e-14, 8.7282e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7487]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7487087\n",
            "state: tensor([[9.5000, 6.5000, 1.8000]]) dist_dsc.probs: tensor([[1.0222e-16, 1.0000e+00, 4.9954e-14, 7.2369e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7710]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7710011\n",
            "state: tensor([[9.5000, 7.0000, 1.8000]]) dist_dsc.probs: tensor([[8.6790e-17, 1.0000e+00, 4.3892e-14, 5.9890e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7905]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7904563\n",
            "state: tensor([[9.5000, 7.5000, 1.8000]]) dist_dsc.probs: tensor([[7.3688e-17, 1.0000e+00, 3.8566e-14, 4.9563e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8035]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80353916\n",
            "state: tensor([[9.5000, 8.0000, 1.8000]]) dist_dsc.probs: tensor([[6.2564e-17, 1.0000e+00, 3.3886e-14, 4.1017e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8114]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81141484\n",
            "state: tensor([[9.5000, 8.5000, 1.8000]]) dist_dsc.probs: tensor([[5.3119e-17, 1.0000e+00, 2.9774e-14, 3.3945e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8190]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81900656\n",
            "state: tensor([[9.5000, 9.0000, 1.8000]]) dist_dsc.probs: tensor([[3.9968e-17, 1.0000e+00, 2.3659e-14, 2.4641e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8321]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8320684\n",
            "state: tensor([[9.5000, 9.5000, 1.8000]]) dist_dsc.probs: tensor([[2.8074e-17, 1.0000e+00, 1.7754e-14, 1.6601e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8461]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84607387\n",
            "state: tensor([[ 9.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.9719e-17, 1.0000e+00, 1.3323e-14, 1.1184e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8590]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85900134\n",
            "state: tensor([[ 9.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.3643e-17, 1.0000e+00, 9.8696e-15, 7.4165e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8708]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.87081146\n",
            "state: tensor([[10.0000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0893e-17, 1.0000e+00, 7.5856e-15, 6.5185e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3740]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.3739772\n",
            "state: tensor([[10.0000,  0.5000,  1.8000]]) dist_dsc.probs: tensor([[1.4343e-17, 1.0000e+00, 9.5346e-15, 8.8136e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3996]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.39956483\n",
            "state: tensor([[10.0000,  1.0000,  1.8000]]) dist_dsc.probs: tensor([[1.8501e-17, 1.0000e+00, 1.1783e-14, 1.1649e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4294]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4293505\n",
            "state: tensor([[10.0000,  1.5000,  1.8000]]) dist_dsc.probs: tensor([[2.3339e-17, 1.0000e+00, 1.4299e-14, 1.5016e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4618]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.46181086\n",
            "state: tensor([[10.0000,  2.0000,  1.8000]]) dist_dsc.probs: tensor([[2.8663e-17, 1.0000e+00, 1.6972e-14, 1.8796e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4939]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4939447\n",
            "state: tensor([[10.0000,  2.5000,  1.8000]]) dist_dsc.probs: tensor([[3.2552e-17, 1.0000e+00, 1.8899e-14, 2.1553e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5298]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52982616\n",
            "state: tensor([[10.0000,  3.0000,  1.8000]]) dist_dsc.probs: tensor([[3.6748e-17, 1.0000e+00, 2.0938e-14, 2.4559e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5630]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5630389\n",
            "state: tensor([[10.0000,  3.5000,  1.8000]]) dist_dsc.probs: tensor([[4.0834e-17, 1.0000e+00, 2.2883e-14, 2.7522e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5922]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5921713\n",
            "state: tensor([[10.0000,  4.0000,  1.8000]]) dist_dsc.probs: tensor([[4.5351e-17, 1.0000e+00, 2.4999e-14, 3.0825e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6218]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62182695\n",
            "state: tensor([[10.0000,  4.5000,  1.8000]]) dist_dsc.probs: tensor([[4.4364e-17, 1.0000e+00, 2.4605e-14, 3.0002e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6577977\n",
            "state: tensor([[10.0000,  5.0000,  1.8000]]) dist_dsc.probs: tensor([[4.0748e-17, 1.0000e+00, 2.2998e-14, 2.7232e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6909]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6909195\n",
            "state: tensor([[10.0000,  5.5000,  1.8000]]) dist_dsc.probs: tensor([[3.4530e-17, 1.0000e+00, 2.0148e-14, 2.2547e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7207]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.72070795\n",
            "state: tensor([[10.0000,  6.0000,  1.8000]]) dist_dsc.probs: tensor([[2.9330e-17, 1.0000e+00, 1.7687e-14, 1.8708e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7476]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7475892\n",
            "state: tensor([[10.0000,  6.5000,  1.8000]]) dist_dsc.probs: tensor([[2.4927e-17, 1.0000e+00, 1.5535e-14, 1.5531e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7721]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77210134\n",
            "state: tensor([[10.0000,  7.0000,  1.8000]]) dist_dsc.probs: tensor([[2.1171e-17, 1.0000e+00, 1.3647e-14, 1.2867e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7917]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.79174805\n",
            "state: tensor([[10.0000,  7.5000,  1.8000]]) dist_dsc.probs: tensor([[1.7975e-17, 1.0000e+00, 1.1991e-14, 1.0649e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8096]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.80955124\n",
            "state: tensor([[10.0000,  8.0000,  1.8000]]) dist_dsc.probs: tensor([[1.5262e-17, 1.0000e+00, 1.0536e-14, 8.8126e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8187]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81874603\n",
            "state: tensor([[10.0000,  8.5000,  1.8000]]) dist_dsc.probs: tensor([[1.2958e-17, 1.0000e+00, 9.2573e-15, 7.2930e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8261]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.82607096\n",
            "state: tensor([[10.0000,  9.0000,  1.8000]]) dist_dsc.probs: tensor([[1.1002e-17, 1.0000e+00, 8.1340e-15, 6.0355e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8331]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.833127\n",
            "state: tensor([[10.0000,  9.5000,  1.8000]]) dist_dsc.probs: tensor([[8.2397e-18, 1.0000e+00, 6.4383e-15, 4.3592e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8457]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84574306\n",
            "state: tensor([[10.0000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[5.7877e-18, 1.0000e+00, 4.8314e-15, 2.9368e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8587]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.85869604\n",
            "state: tensor([[10.0000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[4.0653e-18, 1.0000e+00, 3.6255e-15, 1.9785e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8706]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8706376\n",
            "state: tensor([[10.5000,  0.0000,  1.8000]]) dist_dsc.probs: tensor([[2.4628e-18, 1.0000e+00, 2.2195e-15, 1.2833e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3752]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.37523484\n",
            "state: tensor([[10.5000,  0.5000,  1.8000]]) dist_dsc.probs: tensor([[3.2632e-18, 1.0000e+00, 2.8044e-15, 1.7468e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4002]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.40020737\n",
            "state: tensor([[10.5000,  1.0000,  1.8000]]) dist_dsc.probs: tensor([[4.2135e-18, 1.0000e+00, 3.4686e-15, 2.3114e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4274]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.42742708\n",
            "state: tensor([[10.5000,  1.5000,  1.8000]]) dist_dsc.probs: tensor([[5.3151e-18, 1.0000e+00, 4.2092e-15, 2.9795e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4600]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.4599555\n",
            "state: tensor([[10.5000,  2.0000,  1.8000]]) dist_dsc.probs: tensor([[6.4209e-18, 1.0000e+00, 4.9287e-15, 3.6622e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4930]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.49295387\n",
            "state: tensor([[10.5000,  2.5000,  1.8000]]) dist_dsc.probs: tensor([[7.3021e-18, 1.0000e+00, 5.4943e-15, 4.2061e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5285]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.52853394\n",
            "state: tensor([[10.5000,  3.0000,  1.8000]]) dist_dsc.probs: tensor([[8.2769e-18, 1.0000e+00, 6.1082e-15, 4.8132e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5624]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.56244886\n",
            "state: tensor([[10.5000,  3.5000,  1.8000]]) dist_dsc.probs: tensor([[9.1972e-18, 1.0000e+00, 6.6757e-15, 5.3940e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5916]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.5916106\n",
            "state: tensor([[10.5000,  4.0000,  1.8000]]) dist_dsc.probs: tensor([[1.0220e-17, 1.0000e+00, 7.2959e-15, 6.0449e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6205]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.62045515\n",
            "state: tensor([[10.5000,  4.5000,  1.8000]]) dist_dsc.probs: tensor([[1.0490e-17, 1.0000e+00, 7.4711e-15, 6.2047e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6552]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6552228\n",
            "state: tensor([[10.5000,  5.0000,  1.8000]]) dist_dsc.probs: tensor([[9.9642e-18, 1.0000e+00, 7.1771e-15, 5.8458e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6885]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.6885431\n",
            "state: tensor([[10.5000,  5.5000,  1.8000]]) dist_dsc.probs: tensor([[8.4298e-18, 1.0000e+00, 6.2754e-15, 4.8391e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7192]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7191734\n",
            "state: tensor([[10.5000,  6.0000,  1.8000]]) dist_dsc.probs: tensor([[7.1500e-18, 1.0000e+00, 5.5020e-15, 4.0099e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7465]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7464655\n",
            "state: tensor([[10.5000,  6.5000,  1.8000]]) dist_dsc.probs: tensor([[6.0767e-18, 1.0000e+00, 4.8324e-15, 3.3288e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7711]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.77109605\n",
            "state: tensor([[10.5000,  7.0000,  1.8000]]) dist_dsc.probs: tensor([[5.1646e-18, 1.0000e+00, 4.2443e-15, 2.7634e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7927]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.7927178\n",
            "state: tensor([[10.5000,  7.5000,  1.8000]]) dist_dsc.probs: tensor([[4.3849e-18, 1.0000e+00, 3.7281e-15, 2.2879e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8108]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.81081617\n",
            "state: tensor([[10.5000,  8.0000,  1.8000]]) dist_dsc.probs: tensor([[3.7229e-18, 1.0000e+00, 3.2757e-15, 1.8934e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8253]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8252689\n",
            "state: tensor([[10.5000,  8.5000,  1.8000]]) dist_dsc.probs: tensor([[3.1609e-18, 1.0000e+00, 2.8782e-15, 1.5669e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8329]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8328849\n",
            "state: tensor([[10.5000,  9.0000,  1.8000]]) dist_dsc.probs: tensor([[2.6837e-18, 1.0000e+00, 2.5290e-15, 1.2967e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8397]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.83968884\n",
            "state: tensor([[10.5000,  9.5000,  1.8000]]) dist_dsc.probs: tensor([[2.2783e-18, 1.0000e+00, 2.2219e-15, 1.0730e-17]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8464]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.84638065\n",
            "state: tensor([[10.5000, 10.0000,  1.8000]]) dist_dsc.probs: tensor([[1.6987e-18, 1.0000e+00, 1.7521e-15, 7.7118e-18]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8584]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.8583903\n",
            "state: tensor([[10.5000, 10.5000,  1.8000]]) dist_dsc.probs: tensor([[1.1932e-18, 1.0000e+00, 1.3148e-15, 5.1955e-18]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8704]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "1 -0.87035584\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state: tensor([[0., 0., 1.]]) dist_dsc.probs: tensor([[2.1380e-05, 9.9810e-01, 1.0964e-04, 1.7722e-03]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3954]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 10.,  1.]]) dist_dsc.probs: tensor([[5.3392e-12, 1.0000e+00, 4.0442e-10, 1.0002e-10]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6354]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 20.,  1.]]) dist_dsc.probs: tensor([[2.5520e-21, 1.0000e+00, 8.4545e-18, 5.8336e-21]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8338]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 30.,  1.]]) dist_dsc.probs: tensor([[1.2198e-30, 1.0000e+00, 1.7674e-25, 3.4024e-31]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9285]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 40.,  1.]]) dist_dsc.probs: tensor([[5.6387e-40, 1.0000e+00, 3.5941e-33, 1.9129e-41]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9706]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 50.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 6.8449e-41, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9881]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9952]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9981]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9992]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 0., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9997]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[  0., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[  0., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10.,  0.,  1.]]) dist_dsc.probs: tensor([[8.3220e-17, 1.0000e+00, 4.0531e-14, 6.0552e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3553]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 10.,  1.]]) dist_dsc.probs: tensor([[3.6046e-17, 1.0000e+00, 2.1826e-14, 2.1812e-16]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 20.,  1.]]) dist_dsc.probs: tensor([[3.3362e-24, 1.0000e+00, 3.5876e-20, 3.9528e-24]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9473]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 30.,  1.]]) dist_dsc.probs: tensor([[6.2561e-33, 1.0000e+00, 2.3341e-27, 1.0218e-33]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9789]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 40.,  1.]]) dist_dsc.probs: tensor([[4.3777e-42, 1.0000e+00, 6.7192e-35, 8.9683e-44]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9918]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 50.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 1.9030e-42, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9968]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9987]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9995]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9998]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[10., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 10., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 10., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20.,  0.,  1.]]) dist_dsc.probs: tensor([[8.7803e-30, 1.0000e+00, 7.6164e-25, 3.9501e-30]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.3884]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 10.,  1.]]) dist_dsc.probs: tensor([[2.8981e-29, 1.0000e+00, 2.1472e-24, 1.3876e-29]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8816]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 20.,  1.]]) dist_dsc.probs: tensor([[6.2954e-31, 1.0000e+00, 1.0046e-25, 1.7446e-31]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9774]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 30.,  1.]]) dist_dsc.probs: tensor([[3.3433e-37, 1.0000e+00, 6.9674e-31, 2.1433e-38]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9942]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 40.,  1.]]) dist_dsc.probs: tensor([[1.4013e-45, 1.0000e+00, 1.1257e-37, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9970]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 50.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 9.8091e-45, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9987]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9995]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9998]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[20., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 20., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 20., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30.,  0.,  1.]]) dist_dsc.probs: tensor([[8.0855e-43, 1.0000e+00, 1.2798e-35, 2.2421e-44]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4276]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 10.,  1.]]) dist_dsc.probs: tensor([[1.3967e-41, 1.0000e+00, 1.4065e-34, 4.8345e-43]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8719]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 20.,  1.]]) dist_dsc.probs: tensor([[5.7874e-43, 1.0000e+00, 1.0946e-35, 1.2612e-44]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9864]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 30.,  1.]]) dist_dsc.probs: tensor([[9.8091e-45, 1.0000e+00, 4.0537e-37, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9965]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 40.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 6.7515e-42, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9992]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9997]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9998]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[30., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 30., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 30., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.4722]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 10.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 2.8026e-45, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8708]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 20.,  1.]]) dist_dsc.probs: tensor([[0.0000e+00, 1.0000e+00, 1.4013e-45, 0.0000e+00]],\n",
            "       grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9871]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9981]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9995]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[40., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 40., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 40., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5214]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 10.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8759]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 20.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9863]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9987]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9997]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[50., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 50., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 50., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.5707]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 10.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8829]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 20.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9852]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9987]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9998]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[60., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 60., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 60., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6161]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 10.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8898]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 20.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9847]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9986]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[70., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 70., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 70., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6578]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 10.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.8975]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 20.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9845]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9985]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[80., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 80., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 80., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90.,  0.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.6959]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 10.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9046]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 20.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9852]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 30.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9984]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 40.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 50.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 60.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 70.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 80.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[90., 90.,  1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 90., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[ 90., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,   0.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7303]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  10.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9104]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  20.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9859]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  30.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9984]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  40.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9999]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  50.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  60.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  70.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  80.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100.,  90.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[100., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,   0.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.7615]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  10.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9173]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  20.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9868]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  30.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9983]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  40.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-0.9998]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  50.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  60.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  70.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  80.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.0000]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110.,  90.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110., 100.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n",
            "state: tensor([[110., 110.,   1.]]) dist_dsc.probs: tensor([[0., 1., 0., 0.]], grad_fn=<DivBackward0>) dist_cnt.mean: tensor([[-1.]], grad_fn=<TanhBackward0>) dist_cnt.scale tensor([[7.3891]], grad_fn=<ExpBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
            "[562468, 486108]\n"
          ]
        }
      ],
      "source": [
        "def one_hot_encoding(levels, n_units=2,n_states=5, MAX_maintenance_time=0):\n",
        "    level_ohe = []\n",
        "    #mstatus_ohe = []\n",
        "    for unit_idx in range(n_units):\n",
        "        l = [0] * n_states\n",
        "        #m = [0] * (MAX_maintenance_time + 1)\n",
        "        l[levels[unit_idx]] = 1\n",
        "        #m[maintenance_status[unit_idx]] = 1\n",
        "        level_ohe = level_ohe + l\n",
        "        #mstatus_ohe = mstatus_ohe + m\n",
        "    return level_ohe #, mstatus_ohe\n",
        "\n",
        "\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        print(action)\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"lightblue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, act_dsc, act_cnt, load_total):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "    if act_dsc==0:\n",
        "        ax.text(center_x,center_y,f'M12',ha='center', va='center')\n",
        "    elif act_dsc==1:\n",
        "        ax.text(center_x,center_y,f'M1',ha='center', va='center')\n",
        "    elif act_dsc==2:\n",
        "        ax.text(center_x,center_y,f'M2',ha='center', va='center')\n",
        "    else: #稼働継続\n",
        "        print(act_cnt, type(act_cnt))\n",
        "        act_cnt = act_cnt * 0.5 + 0.5\n",
        "        #ax.text(center_x, center_y,f'{(round(act_cnt[0],2),round(1-act_cnt[0],2))}',ha='center', va='center',fontsize=5)\n",
        "        ax.text(center_x, center_y, f'{(round(act_cnt, 2))}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "def plot_state_value(ax, center_x, center_y, val):\n",
        "    size = 1\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        1,\n",
        "        1,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(act_dsc)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    #print(act_dsc)\n",
        "\n",
        "    ax.text(center_x, center_y, f'{round(float(val), 0)}', ha='center', va='center', fontsize=5)\n",
        "\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 20\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a/10\n",
        "            x2 =s2*a/10\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] + list([load_total])\n",
        "            #print(state)\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "    # 軸のラベルを設定\n",
        "    ax.set_xticks(range(x+2))  # 目盛りの位置を設定\n",
        "    ax.set_xticklabels([f\"{int(label * a)}\" for label in ax.get_xticks()])  # X倍の値でラベルを整数で更新\n",
        "    ax.set_yticks(range(x+2))\n",
        "    ax.set_yticklabels([f\"{int(label * a)}\" for label in ax.get_yticks()])\n",
        "    plt.show()\n",
        "\n",
        "def state_value(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 10\n",
        "    for s1 in range(x+2):\n",
        "        for s2 in range(x+2):\n",
        "            a =env.L/(x)\n",
        "            x1 =s1*a\n",
        "            x2 =s2*a\n",
        "            #level_ohe = one_hot_encoding(levels=[s2,s3])\n",
        "            state = [x1,x2] + list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            #print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_state_value(ax, s1, s2, val)\n",
        "    # 軸の範囲と目盛りを設定\n",
        "    ax.set_xlim(-0.5,x+1.5)\n",
        "    ax.set_ylim(-0.5,x+1.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s1\") #変更\n",
        "    ax.set_ylabel(\"s2\") #変更\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"load_total=\"+str(load_total))\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1)\n",
        "optimal_policy(load_total=1.8)\n",
        "\n",
        "state_value(load_total=1)\n",
        "\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=0,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=1,m2=1,m3=0,b=1,d=0,t=1)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=1,d=0,t=0.5)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=0,t=1)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=0,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=1,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=2,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=3,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "#optimal_policy(s1=4,m1=0,m2=0,m3=0,b=-1,d=-1,t=0.3)\n",
        "s1 = 4\n",
        "s2 = 4\n",
        "s3 = 4\n",
        "m1 = 0\n",
        "m2 = 0\n",
        "m3 = 0\n",
        "inventory = 0\n",
        "#demand = 0\n",
        "remain_interval = 1\n",
        "#level_ohe, mstatus_ohe = one_hot_encoding(levels=[s1,s2,s3],maintenance_status=[m1,m2,m3])\n",
        "#state = level_ohe + mstatus_ohe + list([inventory,demand,remain_interval])\n",
        "#act_dsc, act_cnt, log_prob, val = agent.choose_action_max_prob(state)\n",
        "#act_dsc = act_dsc.item()\n",
        "#act_dsc\n",
        "print(env.Visit)\n",
        "print(env.cntCount)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def get_color(action):\n",
        "    if action < 0:\n",
        "        return \"white\"\n",
        "    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"blue\"]\n",
        "    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\n",
        "    return cmap[action]\n",
        "\n",
        "def plot_action(ax, center_x, center_y, size, action):\n",
        "    opt_action = patches.Rectangle(\n",
        "        (center_x-size/2, center_y-size/2),\n",
        "        size,\n",
        "        size,\n",
        "        linewidth = 0,\n",
        "        facecolor = get_color(action)\n",
        "    )\n",
        "    ax.add_patch(opt_action)\n",
        "    if action >= 0:\n",
        "        r = list(map(int, bin(action)[2:].zfill(env.num_targets)))\n",
        "        #ax.text(center_x,center_y,f'({r[0]},{r[1]},{r[2]})',ha='center', va='center')\n",
        "        ax.text(center_x,center_y,f'{action}',ha='center', va='center')\n",
        "\n",
        "def optimal_policy(load_total):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = 26\n",
        "    for s1 in range(x):\n",
        "        for s2 in range(x):\n",
        "            state = [s1,s2] + list([load_total])\n",
        "            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\n",
        "            act_dsc = act_dsc.item()\n",
        "            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\n",
        "            #print(act_dsc,act_cnt)\n",
        "            print(\"val:\",val)\n",
        "            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\n",
        "            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\n",
        "            plot_action(ax,cs2,cs3,size=1,action=action)\n",
        "    ax.set_xlim(-0.5,x-1+0.5)\n",
        "    ax.set_ylim(-0.5,x-1+0.5)\n",
        "    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\n",
        "    #cbar.ax.set_yticklabels([f'{i:b}' for i in range(8)])\n",
        "    ax.set_aspect('equal', adjustable='box')  # アスペクト比を保持\n",
        "    ax.set_xlabel(\"s2\")\n",
        "    ax.set_ylabel(\"s3\")\n",
        "    # グラフを表示\n",
        "    ax.set_title(\"(s1=\"+str(cs1)+\", s2, s3, z1=\"+str(cz1)+\", z2=\"+str(cz2)+\", z3=\"+str(cz3)+\")\")\n",
        "    plt.show()\n",
        "\n",
        "optimal_policy(load_total=0.2)\n",
        "optimal_policy(load_total=1)\n",
        "optimal_policy(load_total=1.8)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qBPaE5n89aZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "346b443b-9df1-4dd0-f65f-198e6b953a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef get_color(action):\\n    if action < 0:\\n        return \"white\"\\n    #cmap = [\"red\",\"sandybrown\",\"orange\",\"lawngreen\",\"darkorange\",\"lightgreen\",\"green\",\"blue\"]\\n    cmap = [\"red\",\"orange\",\"orange\",\"lightgreen\",\"orange\",\"lightgreen\",\"lightgreen\",\"lightblue\"]\\n    return cmap[action]\\n\\ndef plot_action(ax, center_x, center_y, size, action):\\n    opt_action = patches.Rectangle(\\n        (center_x-size/2, center_y-size/2),\\n        size,\\n        size,\\n        linewidth = 0,\\n        facecolor = get_color(action)\\n    )\\n    ax.add_patch(opt_action)\\n    if action >= 0:\\n        r = list(map(int, bin(action)[2:].zfill(env.num_targets)))\\n        #ax.text(center_x,center_y,f\\'({r[0]},{r[1]},{r[2]})\\',ha=\\'center\\', va=\\'center\\')\\n        ax.text(center_x,center_y,f\\'{action}\\',ha=\\'center\\', va=\\'center\\')\\n\\ndef optimal_policy(load_total):\\n    fig, ax = plt.subplots()\\n    x = 26\\n    for s1 in range(x):\\n        for s2 in range(x):\\n            state = [s1,s2] + list([load_total])\\n            act_dsc, act_cnt, log_prob_dsc, log_prob_cnt, val = agent.choose_action_max_prob(state)\\n            act_dsc = act_dsc.item()\\n            act_cnt = act_cnt.squeeze().cpu().detach().numpy().copy()\\n            #print(act_dsc,act_cnt)\\n            print(\"val:\",val)\\n            #status = np.array([(cs1<env.failures[0])*1,(cs2<env.failures[1])*1,(cs3<env.failures[2])*1])\\n            plot_action(ax, s1, s2, act_dsc, act_cnt, load_total)\\n            plot_action(ax,cs2,cs3,size=1,action=action)\\n    ax.set_xlim(-0.5,x-1+0.5)\\n    ax.set_ylim(-0.5,x-1+0.5)\\n    #cbar = plt.colorbar(scatter, cax=cax, ticks=np.arange(0.5, 8.5, 1))\\n    #cbar.ax.set_yticklabels([f\\'{i:b}\\' for i in range(8)])\\n    ax.set_aspect(\\'equal\\', adjustable=\\'box\\')  # アスペクト比を保持\\n    ax.set_xlabel(\"s2\")\\n    ax.set_ylabel(\"s3\")\\n    # グラフを表示\\n    ax.set_title(\"(s1=\"+str(cs1)+\", s2, s3, z1=\"+str(cz1)+\", z2=\"+str(cz2)+\", z3=\"+str(cz3)+\")\")\\n    plt.show()\\n\\noptimal_policy(load_total=0.2)\\noptimal_policy(load_total=1)\\noptimal_policy(load_total=1.8)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "DRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}